{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from our_dataset import ProofDataset\n",
    "from our_dataset_with_labels import ProofDatasetWithLabels\n",
    "from statement_embedding import emb_to_stmt, num_to_label\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING!!!!\n",
    "# running ProofDataset(root=\"data/\", file_limit=None) currently has a high initial cost\n",
    "# it requires ~63 GB of space, and it took my computer about 2 hours to run\n",
    "# however, after an initial run, the dataset doesn't need to be created again\n",
    "# secondary runs merely verify that all files are in place; this takes my computer about 15s\n",
    "# WARNING!!!!\n",
    "\n",
    "# to avoid previous warning, use file_limit=8000 to only load and verify the first 8000 graphs (<4GB)\n",
    "# if entire graph dataset is desired, use file_limit=None\n",
    "file_limit = 100    # desired number of graphs to work with\n",
    "vocab_size = 1598   # number of characters in our vocabulary\n",
    "label_size = file_limit  # number of labels in dataset; all labels is 45332; defaults to file_limit in pf_data\n",
    "\n",
    "pf_data = ProofDatasetWithLabels(root=\"data/\",file_limit=file_limit, label_size=file_limit )  # normalize with transform = NormalizeFeatures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is used to get the label of a statement from tag_proof.csv\n",
    "\n",
    "def get_thm_lbl(num):\n",
    "    with open(\"../Assets/tag_proof.csv\",\"r\") as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for idx, row in enumerate(reader):\n",
    "            if num == idx:\n",
    "                return row['tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's explore graph 2, i.e. mp2, as our current graph at proof step 3\n",
    "graph_num = 2\n",
    "step_num = 0\n",
    "graph_lbl = get_thm_lbl(graph_num)\n",
    "cur_graph = pf_data.get(graph_num)\n",
    "cur_step = cur_graph.x[step_num]\n",
    "\n",
    "# let's get the normalized statement feature vector at step 3 in our proof\n",
    "cur_step_vec_norm = cur_step\n",
    "cur_step_vec_norm = cur_step_vec_norm[cur_step_vec_norm.nonzero()]  # remove trailing zeros\n",
    "cur_step_vec_norm = cur_step_vec_norm.reshape(-1)\n",
    "cur_step_vec_norm = cur_step_vec_norm.tolist()\n",
    "\n",
    "# unnormalized version of above\n",
    "cur_step_vec = cur_step*vocab_size\n",
    "cur_step_vec = cur_step_vec[cur_step_vec.nonzero()] # remove trailing zeros\n",
    "cur_step_vec = cur_step_vec.reshape(-1)\n",
    "cur_step_vec = cur_step_vec.to(int).tolist()\n",
    "\n",
    "# convert feature vector back to statement\n",
    "stmt_list = cur_step_vec.copy()\n",
    "stmt_list.insert(0,0)\n",
    "\n",
    "# let's get the label of our current step\n",
    "cur_label = cur_graph.y[step_num]*label_size\n",
    "cur_label = cur_label.to(int).item()\n",
    "\n",
    "# get statement dependencies of current step \n",
    "req_stmt_num = []\n",
    "req_stmt = []\n",
    "for idx, x in enumerate(cur_graph.edge_index[1]):\n",
    "    if step_num == x:\n",
    "        req_stmt_num.append(cur_graph.edge_index[0][idx].to(int))\n",
    "for num in req_stmt_num:\n",
    "    dependency = cur_graph.x[num]*vocab_size\n",
    "    dependency = dependency.to(int).tolist()\n",
    "    dependency.insert(0,0)\n",
    "    dependency = emb_to_stmt(dependency)\n",
    "    req_stmt.append(dependency)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our current graph is the proof of mp2, which has 5 node(s) and 4 edge(s).\n",
      "Each node has 11547 feature(s) and 1 label(s)\n",
      "All of the above info. is captured in Data(x=[5, 11547], edge_index=[2, 4], y=[5])\n",
      "\n",
      "For step 0 in the proof, the truncated normalized feature vector is\n",
      "[0.8035044074058533],\n",
      "and the truncated unnormalized feature vector is \n",
      "[1284].\n",
      "\n",
      "The corresponding proof statement is ps\n",
      "The corresponding label is $e\n",
      "\n",
      "Step 0 depends on 0 statements:\n",
      "No dependencies\n"
     ]
    }
   ],
   "source": [
    "print(f\"Our current graph is the proof of {graph_lbl}, which has {cur_graph.num_nodes} node(s) and {cur_graph.num_edges} edge(s).\")\n",
    "print(f\"Each node has {cur_graph.num_features} feature(s) and {torch.numel(cur_graph.y[0])} label(s)\")\n",
    "print(f\"All of the above info. is captured in {cur_graph}\") # edge_index always begins with 2, in case you were curious\n",
    "print()\n",
    "print(f\"For step {step_num} in the proof, the truncated normalized feature vector is\")\n",
    "print(f\"{cur_step_vec_norm},\")\n",
    "print(f\"and the truncated unnormalized feature vector is \")\n",
    "print(f\"{cur_step_vec}.\")\n",
    "print()\n",
    "print(f\"The corresponding proof statement is {emb_to_stmt(stmt_list)}\")\n",
    "print(f\"The corresponding label is {num_to_label(cur_label)}\")\n",
    "print()\n",
    "print(f\"Step {step_num} depends on {len(req_stmt)} statements:\")\n",
    "if len(req_stmt) == 0:\n",
    "    print(\"No dependencies\")\n",
    "else:\n",
    "    for stmt in req_stmt:\n",
    "        print(stmt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(11)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set   = 80 graphs\n",
      "Validation set = 10 graphs\n",
      "Test set       = 10 graphs\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Create training, validation, and test sets\n",
    "train_dataset = pf_data[:int(len(pf_data)*0.8)]\n",
    "val_dataset   = pf_data[int(len(pf_data)*0.8):int(len(pf_data)*0.9)]\n",
    "test_dataset  = pf_data[int(len(pf_data)*0.9):]\n",
    "\n",
    "print(f'Training set   = {len(train_dataset)} graphs')\n",
    "print(f'Validation set = {len(val_dataset)} graphs')\n",
    "print(f'Test set       = {len(test_dataset)} graphs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mini-batches\n",
    "# Shuffling for now; probably will remove shuffling later\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loader:\n",
      " - Batch 0: DataBatch(x=[31, 11547], edge_index=[2, 23], y=[31], batch=[31], ptr=[9])\n",
      " - Batch 1: DataBatch(x=[31, 11547], edge_index=[2, 23], y=[31], batch=[31], ptr=[9])\n",
      " - Batch 2: DataBatch(x=[32, 11547], edge_index=[2, 24], y=[32], batch=[32], ptr=[9])\n",
      " - Batch 3: DataBatch(x=[29, 11547], edge_index=[2, 21], y=[29], batch=[29], ptr=[9])\n",
      " - Batch 4: DataBatch(x=[33, 11547], edge_index=[2, 25], y=[33], batch=[33], ptr=[9])\n",
      " - Batch 5: DataBatch(x=[31, 11547], edge_index=[2, 23], y=[31], batch=[31], ptr=[9])\n",
      " - Batch 6: DataBatch(x=[30, 11547], edge_index=[2, 22], y=[30], batch=[30], ptr=[9])\n",
      " - Batch 7: DataBatch(x=[29, 11547], edge_index=[2, 21], y=[29], batch=[29], ptr=[9])\n",
      " - Batch 8: DataBatch(x=[27, 11547], edge_index=[2, 19], y=[27], batch=[27], ptr=[9])\n",
      " - Batch 9: DataBatch(x=[26, 11547], edge_index=[2, 18], y=[26], batch=[26], ptr=[9])\n",
      "\n",
      "Validation loader:\n",
      " - Batch 0: DataBatch(x=[23, 11547], edge_index=[2, 15], y=[23], batch=[23], ptr=[9])\n",
      " - Batch 1: DataBatch(x=[6, 11547], edge_index=[2, 4], y=[6], batch=[6], ptr=[3])\n",
      "\n",
      "Test loader:\n",
      " - Batch 0: DataBatch(x=[26, 11547], edge_index=[2, 18], y=[26], batch=[26], ptr=[9])\n",
      " - Batch 1: DataBatch(x=[5, 11547], edge_index=[2, 3], y=[5], batch=[5], ptr=[3])\n"
     ]
    }
   ],
   "source": [
    "print('\\nTrain loader:')\n",
    "for i, batch in enumerate(train_loader):\n",
    "    print(f' - Batch {i}: {batch}')\n",
    "\n",
    "print('\\nValidation loader:')\n",
    "for i, batch in enumerate(val_loader):\n",
    "    print(f' - Batch {i}: {batch}')\n",
    "\n",
    "print('\\nTest loader:')\n",
    "for i, batch in enumerate(test_loader):\n",
    "    print(f' - Batch {i}: {batch}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
