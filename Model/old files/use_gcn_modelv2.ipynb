{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from use_dataset import ProofDataset\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.loader import DataLoader, NeighborLoader\n",
    "import random\n",
    "from sklearn.metrics import f1_score as f1\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, ReLU, Dropout\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing graph 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jared\\Documents\\GitHub\\proof-truth\\Model\\use_dataset.py:79: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:278.)\n",
      "  x_emb = torch.tensor([x_emb]).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing graph 1000\n",
      "processing graph 2000\n",
      "processing graph 3000\n",
      "processing graph 4000\n",
      "class_corr dict is of form (old_label, new_label): {0: 0, 5: 1, 6: 2, 7: 3, 9: 4, 11: 5, 14: 6, 15: 7, 16: 8, 17: 9, 18: 10, 20: 11, 22: 12, 24: 13, 25: 14, 29: 15, 31: 16, 32: 17, 34: 18, 35: 19, 36: 20, 42: 21, 47: 22, 52: 23, 57: 24, 63: 25, 65: 26, 77: 27, 82: 28, 83: 29, 86: 30, 115: 31, 119: 32, 121: 33, 123: 34, 136: 35, 141: 36, 144: 37, 147: 38, 149: 39, 155: 40, 157: 41, 185: 42, 189: 43, 200: 44, 212: 45, 215: 46, 218: 47, 219: 48, 220: 49, 221: 50, 223: 51, 225: 52, 226: 53, 227: 54, 228: 55, 229: 56, 231: 57, 232: 58, 233: 59, 234: 60, 235: 61, 236: 62, 237: 63, 238: 64, 245: 65, 246: 66, 247: 67, 248: 68, 251: 69, 252: 70, 253: 71, 254: 72, 255: 73, 256: 74, 260: 75, 261: 76, 264: 77, 265: 78, 268: 79, 270: 80, 274: 81, 275: 82, 276: 83, 278: 84, 279: 85, 280: 86, 281: 87, 282: 88, 284: 89, 285: 90, 286: 91, 288: 92, 290: 93, 292: 94, 293: 95, 295: 96, 299: 97, 300: 98, 301: 99, 302: 100, 304: 101, 306: 102, 307: 103, 316: 104, 317: 105, 318: 106, 320: 107, 321: 108, 322: 109, 323: 110, 324: 111, 326: 112, 337: 113, 338: 114, 339: 115, 341: 116, 342: 117, 343: 118, 344: 119, 345: 120, 346: 121, 347: 122, 348: 123, 349: 124, 353: 125, 354: 126, 360: 127, 361: 128, 364: 129, 366: 130, 383: 131, 400: 132, 403: 133, 405: 134, 410: 135, 411: 136, 414: 137, 416: 138, 417: 139, 419: 140, 423: 141, 454: 142, 462: 143, 464: 144, 466: 145, 471: 146, 472: 147, 473: 148, 475: 149, 478: 150, 480: 151, 484: 152, 485: 153, 486: 154, 488: 155, 494: 156, 499: 157, 500: 158, 501: 159, 513: 160, 515: 161, 532: 162, 533: 163, 534: 164, 554: 165, 568: 166, 570: 167, 578: 168, 579: 169, 583: 170, 586: 171, 587: 172, 595: 173, 596: 174, 598: 175, 611: 176, 613: 177, 615: 178, 619: 179, 625: 180, 626: 181, 629: 182, 631: 183, 632: 184, 633: 185, 644: 186, 655: 187, 689: 188, 690: 189, 691: 190, 708: 191, 713: 192, 714: 193, 725: 194, 768: 195, 770: 196, 772: 197, 803: 198, 812: 199, 845: 200, 848: 201, 850: 202, 854: 203, 856: 204, 859: 205, 860: 206, 861: 207, 864: 208, 865: 209, 867: 210, 870: 211, 871: 212, 910: 213, 911: 214, 912: 215, 916: 216, 919: 217, 934: 218, 979: 219, 981: 220, 1005: 221, 1085: 222, 1086: 223, 1087: 224, 1092: 225, 1107: 226, 1114: 227, 1115: 228, 1116: 229, 1125: 230, 1129: 231, 1130: 232, 1131: 233, 1132: 234, 1133: 235, 1134: 236, 1135: 237, 1368: 238, 1483: 239, 1503: 240, 1522: 241, 1542: 242, 1545: 243, 1552: 244, 1623: 245, 1629: 246, 1643: 247, 1657: 248, 1660: 249, 1671: 250, 1673: 251, 1677: 252, 1696: 253, 1702: 254, 1706: 255, 1715: 256, 1738: 257, 1752: 258, 1753: 259, 1755: 260, 1756: 261, 1757: 262, 1758: 263, 1782: 264, 1783: 265, 1786: 266, 1797: 267, 1799: 268, 1801: 269, 1806: 270, 1813: 271, 1817: 272, 1818: 273, 1821: 274, 1825: 275, 1828: 276, 1833: 277, 1835: 278, 1836: 279, 1838: 280, 1849: 281, 1850: 282, 1854: 283, 1855: 284, 1858: 285, 1859: 286, 1862: 287, 1871: 288, 1878: 289, 1883: 290, 1895: 291, 1897: 292, 1898: 293, 1900: 294, 1904: 295, 1911: 296, 1913: 297, 1915: 298, 1916: 299, 1917: 300, 1918: 301, 1921: 302, 1922: 303, 1928: 304, 1931: 305, 1940: 306, 1943: 307, 1950: 308, 1954: 309, 1956: 310, 1972: 311, 2019: 312, 2023: 313, 2024: 314, 2025: 315, 2027: 316, 2030: 317, 2032: 318, 2033: 319, 2043: 320, 2044: 321, 2070: 322, 2073: 323, 2079: 324, 2081: 325, 2085: 326, 2089: 327, 2091: 328, 2101: 329, 2119: 330, 2127: 331, 2148: 332, 2152: 333, 2153: 334, 2158: 335, 2161: 336, 2167: 337, 2179: 338, 2181: 339, 2183: 340, 2194: 341, 2195: 342, 2198: 343, 2206: 344, 2210: 345, 2212: 346, 2215: 347, 2216: 348, 2223: 349, 2252: 350, 2253: 351, 2270: 352, 2276: 353, 2285: 354, 2324: 355, 2334: 356, 2335: 357, 2341: 358, 2353: 359, 2354: 360, 2393: 361, 2447: 362, 2448: 363, 2491: 364, 2524: 365, 2545: 366, 2601: 367, 2606: 368, 2609: 369, 2619: 370, 2632: 371, 2640: 372, 2641: 373, 2646: 374, 2648: 375, 2650: 376, 2780: 377, 2783: 378, 2788: 379, 2795: 380, 2798: 381, 2799: 382, 2801: 383, 2802: 384, 2803: 385, 2805: 386, 2806: 387, 2807: 388, 2808: 389, 2809: 390, 2810: 391, 2812: 392, 2813: 393, 2814: 394, 2817: 395, 2823: 396, 2824: 397, 2826: 398, 2827: 399, 2828: 400, 2832: 401, 2834: 402, 2836: 403, 2839: 404, 2848: 405, 2852: 406, 2856: 407, 2861: 408, 2865: 409, 2866: 410, 2869: 411, 2874: 412, 2875: 413, 2877: 414, 2878: 415, 2880: 416, 2881: 417, 2884: 418, 2887: 419, 2920: 420, 2932: 421, 2935: 422, 2941: 423, 2946: 424, 2956: 425, 2958: 426, 2959: 427, 2960: 428, 2961: 429, 2964: 430, 2968: 431, 2969: 432, 2971: 433, 2974: 434, 2991: 435, 2994: 436, 2995: 437, 2997: 438, 3001: 439, 3095: 440, 3114: 441, 3115: 442, 3116: 443, 3117: 444, 3118: 445, 3119: 446, 3131: 447, 3136: 448, 3140: 449, 3151: 450, 3152: 451, 3165: 452, 3173: 453, 3183: 454, 3186: 455, 3192: 456, 3202: 457, 3204: 458, 3205: 459, 3209: 460, 3213: 461, 3215: 462, 3259: 463, 3303: 464, 3306: 465, 3310: 466, 3378: 467, 3447: 468, 3449: 469, 3455: 470, 3462: 471, 3518: 472, 3520: 473, 3569: 474, 3574: 475, 3708: 476, 3724: 477, 3725: 478, 3726: 479, 3727: 480, 3733: 481, 3734: 482, 3743: 483, 3779: 484, 3832: 485, 3834: 486, 3845: 487, 3850: 488, 3855: 489, 3859: 490, 3894: 491, 3900: 492, 3901: 493, 3903: 494, 3904: 495, 3906: 496, 3911: 497, 3922: 498, 3924: 499, 3925: 500, 3933: 501, 3940: 502, 3943: 503, 3944: 504, 3946: 505, 3947: 506, 3955: 507, 3974: 508, 4049: 509, 4050: 510, 4079: 511, 4083: 512, 4090: 513, 4091: 514, 4096: 515, 4102: 516, 4114: 517, 4131: 518, 4138: 519, 4139: 520, 4172: 521, 4198: 522, 4204: 523, 4250: 524, 4253: 525, 4262: 526, 4263: 527, 4301: 528, 4307: 529, 4316: 530, 4326: 531, 4414: 532, 4434: 533, 4437: 534, 4505: 535, 4529: 536, 4531: 537, 4533: 538, 4538: 539, 4541: 540, 4544: 541, 4549: 542, 4551: 543, 4595: 544, 4631: 545, 4632: 546, 4633: 547, 4634: 548, 4766: 549, 4806: 550, 4886: 551, 4887: 552, 4888: 553, 4889: 554, 5034: 555, 5096: 556, 45332: 557}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# use file_limit=5000 to only load and verify the first 5000 graphs (~60 MB)\n",
    "\n",
    "file_limit = 5000    # desired number of graphs to work with\n",
    "vocab_size = 1598   # number of characters in our vocabulary\n",
    "\n",
    "pf_data = ProofDataset(root=\"data/\",read_name=\"5000_relabeled_data.json\" , write_name=\"5000_relabeled_data.pt\" ,file_limit=file_limit)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set   = 4000 graphs\n",
      "Validation set = 500 graphs\n",
      "Test set       = 500 graphs\n"
     ]
    }
   ],
   "source": [
    "# make train/val/test for GCN\n",
    "# set seed for random # generation\n",
    "random.seed(10)\n",
    "length = file_limit\n",
    "total_indices = [i for i in range(file_limit)]\n",
    "\n",
    "# create index vectors to filter dataset\n",
    "train_indices = random.sample(total_indices, int(length*.8))\n",
    "train_indices.sort()\n",
    "\n",
    "val_index_options = [x for x in total_indices if x not in train_indices]\n",
    "val_indices = random.sample(val_index_options, int(length*.1))\n",
    "val_indices.sort()\n",
    "\n",
    "test_index_options = [x for x in total_indices if x not in train_indices if x not in val_indices]\n",
    "test_indices = random.sample(test_index_options, int(length*.1))\n",
    "test_indices.sort()\n",
    "\n",
    "# Create training, validation, and test sets\n",
    "train_dataset = pf_data[train_indices]\n",
    "val_dataset = pf_data[val_indices]\n",
    "test_dataset = pf_data[test_indices]\n",
    "\n",
    "print(f'Training set   = {len(train_dataset)} graphs')\n",
    "print(f'Validation set = {len(val_dataset)} graphs')\n",
    "print(f'Test set       = {len(test_dataset)} graphs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mini-batches\n",
    "# Shuffling for now; probably will remove shuffling later\n",
    "train_loader = DataLoader(train_dataset, batch_size=1000, shuffle=True,num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=1000, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loader:\n",
      " - Batch 0: DataBatch(x=[3344, 512], edge_index=[2, 2844], y=[3344], batch=[3344], ptr=[501])\n",
      " - Batch 1: DataBatch(x=[2824, 512], edge_index=[2, 2324], y=[2824], batch=[2824], ptr=[501])\n",
      " - Batch 2: DataBatch(x=[3009, 512], edge_index=[2, 2509], y=[3009], batch=[3009], ptr=[501])\n",
      " - Batch 3: DataBatch(x=[2992, 512], edge_index=[2, 2492], y=[2992], batch=[2992], ptr=[501])\n",
      " - Batch 4: DataBatch(x=[3145, 512], edge_index=[2, 2645], y=[3145], batch=[3145], ptr=[501])\n",
      " - Batch 5: DataBatch(x=[3002, 512], edge_index=[2, 2502], y=[3002], batch=[3002], ptr=[501])\n",
      " - Batch 6: DataBatch(x=[3255, 512], edge_index=[2, 2755], y=[3255], batch=[3255], ptr=[501])\n",
      " - Batch 7: DataBatch(x=[2979, 512], edge_index=[2, 2479], y=[2979], batch=[2979], ptr=[501])\n",
      "\n",
      "Validation loader:\n",
      " - Batch 0: DataBatch(x=[3207, 512], edge_index=[2, 2707], y=[3207], batch=[3207], ptr=[501])\n",
      "\n",
      "Test loader:\n",
      " - Batch 0: DataBatch(x=[3037, 512], edge_index=[2, 2537], y=[3037], batch=[3037], ptr=[501])\n"
     ]
    }
   ],
   "source": [
    "print('\\nTrain loader:')\n",
    "for i, batch in enumerate(train_loader):\n",
    "    print(f' - Batch {i}: {batch}')\n",
    "\n",
    "print('\\nValidation loader:')\n",
    "for i, batch in enumerate(val_loader):\n",
    "    print(f' - Batch {i}: {batch}')\n",
    "\n",
    "print('\\nTest loader:')\n",
    "for i, batch in enumerate(test_loader):\n",
    "    print(f' - Batch {i}: {batch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary to record label frequency\n",
    "\n",
    "# get max label used in pf_data\n",
    "max_label = 0\n",
    "\n",
    "for i in range(file_limit):\n",
    "    for j in pf_data.get(i).y:\n",
    "        if j > max_label:\n",
    "            max_label = j.to(int).item()\n",
    "\n",
    "# initialize histogram for labels used in pf_data            \n",
    "label_count = {}\n",
    "\n",
    "for i in range(file_limit):\n",
    "    for j in range(max_label+1):        \n",
    "        label_count[j] = 0\n",
    "\n",
    "for i in range(file_limit):\n",
    "    for j in pf_data.get(i).y:\n",
    "        label_count[j.to(int).item()] += 1\n",
    "\n",
    "step_count = 0\n",
    "max = 0\n",
    "max_freq_index = None   #find the most frequently used index\n",
    "labels_never_used = 0\n",
    "labels_used_once = 0\n",
    "labels_used_twice = 0\n",
    "\n",
    "\n",
    "for k,v in label_count.items():\n",
    "    step_count += v\n",
    "    if v > max:     \n",
    "        max = v\n",
    "        max_freq_index = k\n",
    "\n",
    "    if v == 0:\n",
    "        labels_never_used += 1\n",
    "    if v == 1:\n",
    "        labels_used_once += 1\n",
    "    if v ==2:\n",
    "        labels_used_twice += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of steps is: 30794\n",
      "highest frequency label is 557 and occurs 7631 times\n",
      "final label used is 557\n",
      "{0: 5140, 1: 545, 2: 52, 3: 10, 4: 23, 5: 269, 6: 13, 7: 39, 8: 41, 9: 389, 10: 25, 11: 27, 12: 244, 13: 22, 14: 56, 15: 11, 16: 20, 17: 107, 18: 46, 19: 74, 20: 13, 21: 17, 22: 25, 23: 16, 24: 12, 25: 17, 26: 16, 27: 11, 28: 14, 29: 49, 30: 17, 31: 12, 32: 13, 33: 11, 34: 41, 35: 11, 36: 19, 37: 12, 38: 13, 39: 17, 40: 21, 41: 28, 42: 82, 43: 24, 44: 11, 45: 126, 46: 59, 47: 26, 48: 82, 49: 129, 50: 99, 51: 23, 52: 23, 53: 53, 54: 114, 55: 28, 56: 24, 57: 37, 58: 58, 59: 77, 60: 58, 61: 22, 62: 28, 63: 124, 64: 40, 65: 43, 66: 10, 67: 16, 68: 11, 69: 31, 70: 14, 71: 18, 72: 20, 73: 24, 74: 18, 75: 19, 76: 32, 77: 51, 78: 24, 79: 12, 80: 12, 81: 25, 82: 15, 83: 16, 84: 473, 85: 38, 86: 73, 87: 171, 88: 72, 89: 19, 90: 19, 91: 66, 92: 20, 93: 60, 94: 23, 95: 20, 96: 27, 97: 44, 98: 106, 99: 14, 100: 15, 101: 38, 102: 241, 103: 36, 104: 11, 105: 112, 106: 23, 107: 10, 108: 59, 109: 13, 110: 40, 111: 10, 112: 12, 113: 31, 114: 21, 115: 48, 116: 16, 117: 10, 118: 12, 119: 50, 120: 26, 121: 12, 122: 15, 123: 68, 124: 24, 125: 48, 126: 13, 127: 10, 128: 26, 129: 12, 130: 11, 131: 20, 132: 16, 133: 21, 134: 17, 135: 143, 136: 35, 137: 28, 138: 192, 139: 38, 140: 25, 141: 11, 142: 44, 143: 37, 144: 61, 145: 10, 146: 13, 147: 31, 148: 15, 149: 10, 150: 21, 151: 21, 152: 109, 153: 121, 154: 94, 155: 71, 156: 11, 157: 12, 158: 14, 159: 11, 160: 19, 161: 77, 162: 25, 163: 16, 164: 25, 165: 12, 166: 10, 167: 12, 168: 29, 169: 13, 170: 27, 171: 18, 172: 42, 173: 23, 174: 19, 175: 19, 176: 16, 177: 10, 178: 25, 179: 17, 180: 81, 181: 62, 182: 65, 183: 43, 184: 47, 185: 91, 186: 12, 187: 16, 188: 11, 189: 15, 190: 29, 191: 15, 192: 13, 193: 23, 194: 12, 195: 12, 196: 11, 197: 12, 198: 13, 199: 17, 200: 43, 201: 10, 202: 17, 203: 58, 204: 16, 205: 21, 206: 16, 207: 11, 208: 38, 209: 27, 210: 41, 211: 13, 212: 11, 213: 25, 214: 10, 215: 40, 216: 36, 217: 12, 218: 24, 219: 21, 220: 17, 221: 10, 222: 25, 223: 57, 224: 10, 225: 23, 226: 13, 227: 11, 228: 17, 229: 13, 230: 17, 231: 18, 232: 43, 233: 43, 234: 46, 235: 16, 236: 15, 237: 21, 238: 16, 239: 21, 240: 24, 241: 23, 242: 25, 243: 43, 244: 10, 245: 32, 246: 34, 247: 24, 248: 12, 249: 14, 250: 10, 251: 26, 252: 11, 253: 10, 254: 18, 255: 10, 256: 45, 257: 42, 258: 10, 259: 29, 260: 20, 261: 18, 262: 17, 263: 37, 264: 34, 265: 38, 266: 20, 267: 26, 268: 33, 269: 17, 270: 26, 271: 60, 272: 23, 273: 11, 274: 156, 275: 17, 276: 10, 277: 24, 278: 17, 279: 29, 280: 21, 281: 128, 282: 23, 283: 34, 284: 28, 285: 39, 286: 18, 287: 18, 288: 18, 289: 15, 290: 10, 291: 14, 292: 54, 293: 17, 294: 84, 295: 15, 296: 49, 297: 13, 298: 321, 299: 11, 300: 18, 301: 17, 302: 49, 303: 32, 304: 36, 305: 25, 306: 34, 307: 15, 308: 13, 309: 18, 310: 13, 311: 26, 312: 20, 313: 12, 314: 17, 315: 12, 316: 23, 317: 10, 318: 20, 319: 23, 320: 16, 321: 12, 322: 24, 323: 11, 324: 14, 325: 33, 326: 11, 327: 31, 328: 21, 329: 16, 330: 12, 331: 11, 332: 41, 333: 29, 334: 49, 335: 42, 336: 17, 337: 21, 338: 31, 339: 47, 340: 38, 341: 30, 342: 14, 343: 14, 344: 14, 345: 10, 346: 32, 347: 12, 348: 19, 349: 15, 350: 40, 351: 14, 352: 18, 353: 10, 354: 10, 355: 17, 356: 33, 357: 20, 358: 11, 359: 23, 360: 18, 361: 15, 362: 11, 363: 24, 364: 15, 365: 16, 366: 12, 367: 25, 368: 16, 369: 19, 370: 12, 371: 33, 372: 11, 373: 11, 374: 13, 375: 27, 376: 12, 377: 51, 378: 25, 379: 10, 380: 24, 381: 34, 382: 20, 383: 22, 384: 16, 385: 36, 386: 54, 387: 21, 388: 75, 389: 26, 390: 12, 391: 26, 392: 33, 393: 37, 394: 20, 395: 13, 396: 19, 397: 77, 398: 26, 399: 56, 400: 17, 401: 10, 402: 53, 403: 32, 404: 26, 405: 33, 406: 33, 407: 14, 408: 53, 409: 27, 410: 48, 411: 16, 412: 27, 413: 70, 414: 12, 415: 44, 416: 95, 417: 48, 418: 24, 419: 17, 420: 15, 421: 10, 422: 10, 423: 20, 424: 65, 425: 22, 426: 136, 427: 20, 428: 17, 429: 10, 430: 17, 431: 11, 432: 18, 433: 10, 434: 15, 435: 43, 436: 27, 437: 10, 438: 10, 439: 16, 440: 21, 441: 125, 442: 91, 443: 44, 444: 43, 445: 80, 446: 16, 447: 21, 448: 61, 449: 20, 450: 11, 451: 14, 452: 37, 453: 12, 454: 12, 455: 17, 456: 14, 457: 14, 458: 12, 459: 17, 460: 16, 461: 41, 462: 13, 463: 15, 464: 10, 465: 13, 466: 10, 467: 13, 468: 85, 469: 21, 470: 29, 471: 59, 472: 14, 473: 27, 474: 12, 475: 10, 476: 12, 477: 19, 478: 25, 479: 40, 480: 22, 481: 31, 482: 13, 483: 16, 484: 25, 485: 41, 486: 28, 487: 34, 488: 15, 489: 37, 490: 12, 491: 58, 492: 52, 493: 10, 494: 11, 495: 44, 496: 11, 497: 28, 498: 12, 499: 22, 500: 10, 501: 29, 502: 12, 503: 21, 504: 14, 505: 10, 506: 15, 507: 10, 508: 18, 509: 11, 510: 21, 511: 44, 512: 44, 513: 12, 514: 14, 515: 11, 516: 13, 517: 11, 518: 37, 519: 11, 520: 16, 521: 11, 522: 20, 523: 10, 524: 18, 525: 10, 526: 10, 527: 14, 528: 19, 529: 14, 530: 22, 531: 11, 532: 13, 533: 42, 534: 43, 535: 14, 536: 10, 537: 28, 538: 27, 539: 11, 540: 11, 541: 18, 542: 16, 543: 10, 544: 11, 545: 23, 546: 19, 547: 18, 548: 11, 549: 10, 550: 12, 551: 15, 552: 19, 553: 49, 554: 20, 555: 24, 556: 21, 557: 7631}\n",
      "558 unique labels are used\n",
      "0 unique labels never used\n",
      "0 unique labels used once\n",
      "0 unique labels used twice\n"
     ]
    }
   ],
   "source": [
    "print(f\"total number of steps is:\", step_count)\n",
    "print(f\"highest frequency label is {max_freq_index} and occurs {max} times\")\n",
    "print(f\"final label used is {len(label_count)-1}\")\n",
    "print(label_count)\n",
    "print(len(label_count),\"unique labels are used\")\n",
    "print(labels_never_used,\"unique labels never used\")\n",
    "print(labels_used_once, \"unique labels used once\")\n",
    "print(labels_used_twice, \"unique labels used twice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make array of label frequencies for sklearn compute_class_weight using entire dataset\n",
    "# really should be doing this for train set (otherwise, data leakage...)\n",
    "# however, train set may not include certain labels, which leads to error in compute_class_weight\n",
    "\n",
    "# make array of unique classes\n",
    "class_num_arr = [i for i in range(len(label_count))]\n",
    "class_num_arr = np.array(class_num_arr)\n",
    "\n",
    "# make array of all data points with labels\n",
    "lbl_arr = np.array([])\n",
    "for i in range(file_limit):\n",
    "    for y in pf_data.get(i).y:\n",
    "        lbl_arr = np.append(lbl_arr,[y.numpy()],axis=0).astype(int)\n",
    "\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\",classes = class_num_arr, y=lbl_arr)\n",
    "class_weights = torch.from_numpy(class_weights).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make class for GCN model\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    \"\"\"GCN\"\"\"\n",
    "    def __init__(self, dim_h):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(pf_data.num_features, dim_h)\n",
    "        self.conv2 = GCNConv(dim_h, dim_h)\n",
    "        #self.conv3 = GCNConv(dim_h, dim_h)\n",
    "        #self.conv4 = GCNConv(dim_h, dim_h)\n",
    "        self.lin = Linear(dim_h, len(class_num_arr))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = self.conv1(x, edge_index)\n",
    "        h = h.relu()\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        h = self.conv2(h, edge_index)        \n",
    "        h = h.relu()\n",
    "        h = F.dropout(h, p=0.8, training=self.training)\n",
    "        h = self.lin(h)\n",
    "        \n",
    "        return F.log_softmax(h, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCN model training\n",
    "\n",
    "def train(model, loader, lr):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    # commented out code is to use class weights to account for imbalanced dataset\n",
    "    #criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "    epochs = 4000\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs+1):\n",
    "        total_loss = 0\n",
    "        acc = 0\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "        cur_graph = 0   # used to keep track of current statement to enforce preds of only PREVIOUS labels in training\n",
    "\n",
    "        # Train on batches\n",
    "        for data in loader:\n",
    "            cur_graph_batch = data.batch + cur_graph            \n",
    "            cur_graph += torch.max(data.batch) + 1\n",
    "            data = data.to(device, non_blocking=True)\n",
    "            data.y = data.y.to(torch.float).to(device, non_blocking=True)\n",
    "            optimizer.zero_grad()\n",
    "            length = len(loader)\n",
    "            out = model(data.x, data.edge_index.long())\n",
    "            data.y = data.y.type(torch.LongTensor).to(device, non_blocking=True)\n",
    "            out = out.type(torch.float32).to(device, non_blocking=True)\n",
    "            loss = criterion(out, data.y)\n",
    "            total_loss += loss / length\n",
    "\n",
    "            # commented out code below is meant to enforce predictions to only come from previous theorems\n",
    "            # for dict to be properly created, you must delete and recreate test.pt by rerunning \n",
    "            # pf_data = ProofDataset(root=\"data/\",file_limit=file_limit)\n",
    "            \n",
    "            #dict = pf_data.class_corr\n",
    "            #dict_keys = [k for k in dict.keys()]            \n",
    "\n",
    "            #def return_next_lowest_idx(num):\n",
    "                #if num in dict_keys:\n",
    "                    #return dict[num]\n",
    "                #while num not in dict_keys:\n",
    "                    #try:\n",
    "                        #num -= 1\n",
    "                        #return dict[num]\n",
    "                    #except:\n",
    "                        #pass\n",
    "\n",
    "            #with torch.no_grad():\n",
    "                #cur_graph_batch.apply_(return_next_lowest_idx)\n",
    "                #masked_lbls = (torch.arange(out.size(1)) < (cur_graph_batch[..., None]+1))*(out.cpu())\n",
    "                #try:\n",
    "                    #masked_lbls = (torch.arange(out.size(1)) < (cur_graph_batch[..., None]+1))*(out.cpu())\n",
    "                    #masked_lbls = torch.where(masked_lbls==0,np.nan,masked_lbls)\n",
    "                    #masked_lbls = masked_lbls.detach().numpy()                \n",
    "                    #pred = np.nanargmax(masked_lbls,axis=1)\n",
    "                    #pred = torch.from_numpy(pred)\n",
    "                    #acc += accuracy(pred, data.y.cpu()) / length\n",
    "                #except Exception as e:\n",
    "                    #print(\"a lil error\")\n",
    "                    #out = out + .00000001\n",
    "                    #masked_lbls = (torch.arange(out.size(1)) < (cur_graph_batch[..., None]+1))*(out.cpu())\n",
    "                    #masked_lbls = torch.where(masked_lbls==0,np.nan,masked_lbls)\n",
    "                    #masked_lbls = masked_lbls.detach().numpy()\n",
    "                    #pred = np.nanargmax(masked_lbls,axis=1)\n",
    "                    #pred = torch.from_numpy(pred)\n",
    "                    #acc += accuracy(pred, data.y.cpu()) / length\n",
    "                    #out = out - .00000001\n",
    "\n",
    "            #comment out the follow pred and acc lines if enforcing predictions as described above\n",
    "            pred = out.argmax(dim=1)\n",
    "            acc += accuracy(pred, data.y) / length\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # run model on validation set\n",
    "            val_loss, val_acc, val_f1 = test(model, val_loader)\n",
    "\n",
    "        # Print metrics every epoch\n",
    "        if(epoch % 10 == 0):\n",
    "            print(f'Epoch {epoch:>3} | Train Loss: {total_loss:.2f} | Train Acc: {acc*100:>5.2f}% | Val Loss: {val_loss:.2f} | Val Acc: {val_acc*100:.2f}% | F Score: {val_f1:.2f}')\n",
    "            \n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, loader):\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    fscore = 0\n",
    "    \n",
    "    for data in loader:\n",
    "        data = data.to(device, non_blocking=True)\n",
    "        length = len(loader)\n",
    "        out = model(data.x, data.edge_index.long())\n",
    "        data.y = data.y.type(torch.LongTensor).to(device, non_blocking=True)\n",
    "        loss += criterion(out, data.y) / length\n",
    "        pred = out.argmax(dim=1)\n",
    "        acc += accuracy(pred, data.y) / length\n",
    "        fscore += f1(pred.cpu(), data.y.cpu(), average='macro')    # micro looks better, but macro prob more accurate\n",
    "\n",
    "    return loss, acc, fscore\n",
    "\n",
    "def accuracy(pred_y, y):\n",
    "    \"\"\"Calculate accuracy.\"\"\"\n",
    "    return ((pred_y == y).sum() / len(y)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (conv1): GCNConv(512, 3200)\n",
       "  (conv2): GCNConv(3200, 3200)\n",
       "  (lin): Linear(in_features=3200, out_features=558, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize (and reset weights of) model\n",
    "# dim_h is hyperparameter of number of hidden layers\n",
    "\n",
    "gcn_trained = None\n",
    "gcn = None\n",
    "gcn = GCN(dim_h=3200).to(device)\n",
    "gcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | Train Loss: 5.67 | Train Acc: 18.61% | Val Loss: 4.64 | Val Acc: 25.60% | F Score: 0.00\n",
      "Epoch  10 | Train Loss: 4.11 | Train Acc: 32.63% | Val Loss: 4.22 | Val Acc: 32.93% | F Score: 0.00\n",
      "Epoch  20 | Train Loss: 3.74 | Train Acc: 34.46% | Val Loss: 3.90 | Val Acc: 34.21% | F Score: 0.00\n",
      "Epoch  30 | Train Loss: 3.49 | Train Acc: 34.33% | Val Loss: 3.69 | Val Acc: 33.30% | F Score: 0.01\n",
      "Epoch  40 | Train Loss: 3.30 | Train Acc: 35.47% | Val Loss: 3.53 | Val Acc: 34.21% | F Score: 0.01\n",
      "Epoch  50 | Train Loss: 3.17 | Train Acc: 35.93% | Val Loss: 3.43 | Val Acc: 35.02% | F Score: 0.02\n",
      "Epoch  60 | Train Loss: 3.06 | Train Acc: 36.50% | Val Loss: 3.34 | Val Acc: 35.42% | F Score: 0.03\n",
      "Epoch  70 | Train Loss: 3.00 | Train Acc: 36.24% | Val Loss: 3.28 | Val Acc: 35.36% | F Score: 0.04\n",
      "Epoch  80 | Train Loss: 2.91 | Train Acc: 37.03% | Val Loss: 3.24 | Val Acc: 35.27% | F Score: 0.05\n",
      "Epoch  90 | Train Loss: 2.82 | Train Acc: 38.89% | Val Loss: 3.15 | Val Acc: 37.54% | F Score: 0.05\n",
      "Epoch 100 | Train Loss: 2.76 | Train Acc: 39.34% | Val Loss: 3.14 | Val Acc: 37.89% | F Score: 0.05\n",
      "Epoch 110 | Train Loss: 2.69 | Train Acc: 40.17% | Val Loss: 3.07 | Val Acc: 38.35% | F Score: 0.07\n",
      "Epoch 120 | Train Loss: 2.64 | Train Acc: 40.49% | Val Loss: 3.04 | Val Acc: 38.04% | F Score: 0.08\n",
      "Epoch 130 | Train Loss: 2.62 | Train Acc: 41.14% | Val Loss: 3.01 | Val Acc: 38.63% | F Score: 0.08\n",
      "Epoch 140 | Train Loss: 2.56 | Train Acc: 41.62% | Val Loss: 2.99 | Val Acc: 38.98% | F Score: 0.07\n",
      "Epoch 150 | Train Loss: 2.50 | Train Acc: 42.43% | Val Loss: 2.93 | Val Acc: 40.10% | F Score: 0.09\n",
      "Epoch 160 | Train Loss: 2.46 | Train Acc: 42.95% | Val Loss: 2.92 | Val Acc: 39.48% | F Score: 0.09\n",
      "Epoch 170 | Train Loss: 2.46 | Train Acc: 42.89% | Val Loss: 2.91 | Val Acc: 39.79% | F Score: 0.12\n",
      "Epoch 180 | Train Loss: 2.38 | Train Acc: 43.76% | Val Loss: 2.86 | Val Acc: 41.32% | F Score: 0.11\n",
      "Epoch 190 | Train Loss: 2.35 | Train Acc: 44.32% | Val Loss: 2.85 | Val Acc: 41.44% | F Score: 0.13\n",
      "Epoch 200 | Train Loss: 2.29 | Train Acc: 45.11% | Val Loss: 2.82 | Val Acc: 41.60% | F Score: 0.11\n",
      "Epoch 210 | Train Loss: 2.28 | Train Acc: 45.02% | Val Loss: 2.80 | Val Acc: 40.69% | F Score: 0.11\n",
      "Epoch 220 | Train Loss: 2.23 | Train Acc: 46.04% | Val Loss: 2.76 | Val Acc: 42.91% | F Score: 0.14\n",
      "Epoch 230 | Train Loss: 2.23 | Train Acc: 45.76% | Val Loss: 2.78 | Val Acc: 42.75% | F Score: 0.14\n",
      "Epoch 240 | Train Loss: 2.18 | Train Acc: 46.46% | Val Loss: 2.74 | Val Acc: 43.69% | F Score: 0.14\n",
      "Epoch 250 | Train Loss: 2.14 | Train Acc: 47.42% | Val Loss: 2.71 | Val Acc: 44.22% | F Score: 0.15\n",
      "Epoch 260 | Train Loss: 2.09 | Train Acc: 48.24% | Val Loss: 2.69 | Val Acc: 44.81% | F Score: 0.17\n",
      "Epoch 270 | Train Loss: 2.08 | Train Acc: 48.36% | Val Loss: 2.68 | Val Acc: 45.09% | F Score: 0.17\n",
      "Epoch 280 | Train Loss: 2.04 | Train Acc: 49.01% | Val Loss: 2.67 | Val Acc: 45.00% | F Score: 0.15\n",
      "Epoch 290 | Train Loss: 2.02 | Train Acc: 49.62% | Val Loss: 2.69 | Val Acc: 43.09% | F Score: 0.14\n",
      "Epoch 300 | Train Loss: 2.00 | Train Acc: 49.79% | Val Loss: 2.67 | Val Acc: 43.47% | F Score: 0.15\n",
      "Epoch 310 | Train Loss: 1.99 | Train Acc: 49.37% | Val Loss: 2.66 | Val Acc: 43.25% | F Score: 0.19\n",
      "Epoch 320 | Train Loss: 1.95 | Train Acc: 50.81% | Val Loss: 2.67 | Val Acc: 43.90% | F Score: 0.16\n",
      "Epoch 330 | Train Loss: 1.97 | Train Acc: 50.69% | Val Loss: 2.68 | Val Acc: 43.41% | F Score: 0.15\n",
      "Epoch 340 | Train Loss: 1.91 | Train Acc: 51.59% | Val Loss: 2.63 | Val Acc: 44.15% | F Score: 0.18\n",
      "Epoch 350 | Train Loss: 1.88 | Train Acc: 52.33% | Val Loss: 2.61 | Val Acc: 44.68% | F Score: 0.18\n",
      "Epoch 360 | Train Loss: 1.86 | Train Acc: 52.42% | Val Loss: 2.63 | Val Acc: 44.65% | F Score: 0.17\n",
      "Epoch 370 | Train Loss: 1.85 | Train Acc: 52.80% | Val Loss: 2.62 | Val Acc: 45.31% | F Score: 0.17\n",
      "Epoch 380 | Train Loss: 1.85 | Train Acc: 51.75% | Val Loss: 2.62 | Val Acc: 44.40% | F Score: 0.19\n",
      "Epoch 390 | Train Loss: 1.80 | Train Acc: 53.66% | Val Loss: 2.61 | Val Acc: 44.22% | F Score: 0.17\n",
      "Epoch 400 | Train Loss: 1.76 | Train Acc: 54.28% | Val Loss: 2.57 | Val Acc: 45.77% | F Score: 0.20\n",
      "Epoch 410 | Train Loss: 1.74 | Train Acc: 54.90% | Val Loss: 2.58 | Val Acc: 46.02% | F Score: 0.18\n",
      "Epoch 420 | Train Loss: 1.74 | Train Acc: 54.85% | Val Loss: 2.58 | Val Acc: 46.02% | F Score: 0.19\n",
      "Epoch 430 | Train Loss: 1.72 | Train Acc: 55.08% | Val Loss: 2.57 | Val Acc: 46.06% | F Score: 0.19\n",
      "Epoch 440 | Train Loss: 1.68 | Train Acc: 56.09% | Val Loss: 2.57 | Val Acc: 46.27% | F Score: 0.19\n",
      "Epoch 450 | Train Loss: 1.69 | Train Acc: 55.32% | Val Loss: 2.56 | Val Acc: 46.65% | F Score: 0.20\n",
      "Epoch 460 | Train Loss: 1.67 | Train Acc: 55.98% | Val Loss: 2.56 | Val Acc: 47.05% | F Score: 0.20\n",
      "Epoch 470 | Train Loss: 1.64 | Train Acc: 56.10% | Val Loss: 2.55 | Val Acc: 46.65% | F Score: 0.21\n",
      "Epoch 480 | Train Loss: 1.60 | Train Acc: 57.61% | Val Loss: 2.53 | Val Acc: 46.93% | F Score: 0.21\n",
      "Epoch 490 | Train Loss: 1.60 | Train Acc: 57.59% | Val Loss: 2.55 | Val Acc: 47.24% | F Score: 0.20\n",
      "Epoch 500 | Train Loss: 1.55 | Train Acc: 58.40% | Val Loss: 2.49 | Val Acc: 48.43% | F Score: 0.24\n",
      "Epoch 510 | Train Loss: 1.55 | Train Acc: 58.45% | Val Loss: 2.49 | Val Acc: 50.17% | F Score: 0.25\n",
      "Epoch 520 | Train Loss: 1.54 | Train Acc: 58.59% | Val Loss: 2.50 | Val Acc: 50.23% | F Score: 0.26\n",
      "Epoch 530 | Train Loss: 1.51 | Train Acc: 59.23% | Val Loss: 2.48 | Val Acc: 50.33% | F Score: 0.26\n",
      "Epoch 540 | Train Loss: 1.50 | Train Acc: 59.44% | Val Loss: 2.48 | Val Acc: 50.70% | F Score: 0.25\n",
      "Epoch 550 | Train Loss: 1.52 | Train Acc: 59.18% | Val Loss: 2.49 | Val Acc: 50.83% | F Score: 0.27\n",
      "Epoch 560 | Train Loss: 1.50 | Train Acc: 59.28% | Val Loss: 2.48 | Val Acc: 51.08% | F Score: 0.27\n",
      "Epoch 570 | Train Loss: 1.46 | Train Acc: 60.18% | Val Loss: 2.47 | Val Acc: 50.86% | F Score: 0.27\n",
      "Epoch 580 | Train Loss: 1.47 | Train Acc: 59.99% | Val Loss: 2.50 | Val Acc: 49.89% | F Score: 0.28\n",
      "Epoch 590 | Train Loss: 1.45 | Train Acc: 60.74% | Val Loss: 2.48 | Val Acc: 51.23% | F Score: 0.28\n",
      "Epoch 600 | Train Loss: 1.46 | Train Acc: 60.12% | Val Loss: 2.49 | Val Acc: 51.14% | F Score: 0.28\n",
      "Epoch 610 | Train Loss: 1.43 | Train Acc: 60.85% | Val Loss: 2.49 | Val Acc: 51.33% | F Score: 0.27\n",
      "Epoch 620 | Train Loss: 1.41 | Train Acc: 61.20% | Val Loss: 2.48 | Val Acc: 51.42% | F Score: 0.28\n",
      "Epoch 630 | Train Loss: 1.40 | Train Acc: 61.28% | Val Loss: 2.49 | Val Acc: 50.64% | F Score: 0.28\n",
      "Epoch 640 | Train Loss: 1.35 | Train Acc: 62.86% | Val Loss: 2.46 | Val Acc: 52.07% | F Score: 0.29\n",
      "Epoch 650 | Train Loss: 1.37 | Train Acc: 62.06% | Val Loss: 2.49 | Val Acc: 51.08% | F Score: 0.29\n",
      "Epoch 660 | Train Loss: 1.36 | Train Acc: 62.50% | Val Loss: 2.46 | Val Acc: 52.39% | F Score: 0.28\n",
      "Epoch 670 | Train Loss: 1.33 | Train Acc: 63.08% | Val Loss: 2.46 | Val Acc: 52.23% | F Score: 0.29\n",
      "Epoch 680 | Train Loss: 1.32 | Train Acc: 63.36% | Val Loss: 2.45 | Val Acc: 52.14% | F Score: 0.29\n",
      "Epoch 690 | Train Loss: 1.32 | Train Acc: 63.13% | Val Loss: 2.46 | Val Acc: 51.76% | F Score: 0.29\n",
      "Epoch 700 | Train Loss: 1.49 | Train Acc: 57.85% | Val Loss: 2.64 | Val Acc: 46.74% | F Score: 0.29\n",
      "Epoch 710 | Train Loss: 1.37 | Train Acc: 61.88% | Val Loss: 2.52 | Val Acc: 51.42% | F Score: 0.30\n",
      "Epoch 720 | Train Loss: 1.30 | Train Acc: 63.77% | Val Loss: 2.47 | Val Acc: 53.23% | F Score: 0.29\n",
      "Epoch 730 | Train Loss: 1.32 | Train Acc: 62.60% | Val Loss: 2.56 | Val Acc: 50.45% | F Score: 0.29\n",
      "Epoch 740 | Train Loss: 1.33 | Train Acc: 62.69% | Val Loss: 2.55 | Val Acc: 51.14% | F Score: 0.31\n",
      "Epoch 750 | Train Loss: 1.24 | Train Acc: 65.26% | Val Loss: 2.47 | Val Acc: 52.67% | F Score: 0.30\n",
      "Epoch 760 | Train Loss: 1.25 | Train Acc: 65.14% | Val Loss: 2.48 | Val Acc: 53.20% | F Score: 0.30\n",
      "Epoch 770 | Train Loss: 1.27 | Train Acc: 64.03% | Val Loss: 2.51 | Val Acc: 52.23% | F Score: 0.31\n",
      "Epoch 780 | Train Loss: 1.21 | Train Acc: 65.64% | Val Loss: 2.49 | Val Acc: 53.63% | F Score: 0.31\n",
      "Epoch 790 | Train Loss: 1.25 | Train Acc: 64.74% | Val Loss: 2.54 | Val Acc: 51.64% | F Score: 0.30\n",
      "Epoch 800 | Train Loss: 1.21 | Train Acc: 65.46% | Val Loss: 2.54 | Val Acc: 52.48% | F Score: 0.31\n",
      "Epoch 810 | Train Loss: 1.18 | Train Acc: 66.47% | Val Loss: 2.48 | Val Acc: 52.48% | F Score: 0.31\n",
      "Epoch 820 | Train Loss: 1.14 | Train Acc: 67.95% | Val Loss: 2.48 | Val Acc: 54.13% | F Score: 0.29\n",
      "Epoch 830 | Train Loss: 1.12 | Train Acc: 68.11% | Val Loss: 2.47 | Val Acc: 52.95% | F Score: 0.31\n",
      "Epoch 840 | Train Loss: 1.10 | Train Acc: 68.67% | Val Loss: 2.47 | Val Acc: 53.23% | F Score: 0.31\n",
      "Epoch 850 | Train Loss: 1.10 | Train Acc: 68.99% | Val Loss: 2.47 | Val Acc: 53.70% | F Score: 0.31\n",
      "Epoch 860 | Train Loss: 1.11 | Train Acc: 68.65% | Val Loss: 2.48 | Val Acc: 53.23% | F Score: 0.31\n",
      "Epoch 870 | Train Loss: 1.12 | Train Acc: 68.01% | Val Loss: 2.52 | Val Acc: 53.32% | F Score: 0.32\n",
      "Epoch 880 | Train Loss: 1.14 | Train Acc: 67.10% | Val Loss: 2.55 | Val Acc: 52.98% | F Score: 0.32\n",
      "Epoch 890 | Train Loss: 1.11 | Train Acc: 67.92% | Val Loss: 2.55 | Val Acc: 52.39% | F Score: 0.33\n",
      "Epoch 900 | Train Loss: 1.11 | Train Acc: 68.12% | Val Loss: 2.56 | Val Acc: 52.29% | F Score: 0.33\n",
      "Epoch 910 | Train Loss: 1.07 | Train Acc: 69.35% | Val Loss: 2.51 | Val Acc: 53.10% | F Score: 0.32\n",
      "Epoch 920 | Train Loss: 1.06 | Train Acc: 69.60% | Val Loss: 2.51 | Val Acc: 54.51% | F Score: 0.31\n",
      "Epoch 930 | Train Loss: 1.13 | Train Acc: 66.98% | Val Loss: 2.56 | Val Acc: 53.88% | F Score: 0.31\n",
      "Epoch 940 | Train Loss: 1.06 | Train Acc: 69.49% | Val Loss: 2.52 | Val Acc: 54.29% | F Score: 0.33\n",
      "Epoch 950 | Train Loss: 1.05 | Train Acc: 69.43% | Val Loss: 2.55 | Val Acc: 53.73% | F Score: 0.32\n",
      "Epoch 960 | Train Loss: 1.01 | Train Acc: 70.97% | Val Loss: 2.51 | Val Acc: 54.41% | F Score: 0.33\n",
      "Epoch 970 | Train Loss: 0.99 | Train Acc: 71.47% | Val Loss: 2.51 | Val Acc: 54.19% | F Score: 0.33\n",
      "Epoch 980 | Train Loss: 0.99 | Train Acc: 71.18% | Val Loss: 2.52 | Val Acc: 54.23% | F Score: 0.33\n",
      "Epoch 990 | Train Loss: 0.98 | Train Acc: 71.88% | Val Loss: 2.52 | Val Acc: 54.85% | F Score: 0.33\n",
      "Epoch 1000 | Train Loss: 1.02 | Train Acc: 70.64% | Val Loss: 2.53 | Val Acc: 54.76% | F Score: 0.32\n",
      "Epoch 1010 | Train Loss: 1.07 | Train Acc: 68.62% | Val Loss: 2.60 | Val Acc: 53.73% | F Score: 0.33\n",
      "Epoch 1020 | Train Loss: 1.04 | Train Acc: 69.59% | Val Loss: 2.60 | Val Acc: 53.70% | F Score: 0.32\n",
      "Epoch 1030 | Train Loss: 0.98 | Train Acc: 71.26% | Val Loss: 2.59 | Val Acc: 54.10% | F Score: 0.34\n",
      "Epoch 1040 | Train Loss: 0.94 | Train Acc: 72.61% | Val Loss: 2.58 | Val Acc: 53.94% | F Score: 0.34\n",
      "Epoch 1050 | Train Loss: 0.96 | Train Acc: 71.71% | Val Loss: 2.58 | Val Acc: 54.16% | F Score: 0.34\n",
      "Epoch 1060 | Train Loss: 0.95 | Train Acc: 72.26% | Val Loss: 2.58 | Val Acc: 55.16% | F Score: 0.33\n",
      "Epoch 1070 | Train Loss: 0.98 | Train Acc: 71.17% | Val Loss: 2.58 | Val Acc: 54.57% | F Score: 0.34\n",
      "Epoch 1080 | Train Loss: 1.11 | Train Acc: 67.25% | Val Loss: 2.83 | Val Acc: 47.43% | F Score: 0.32\n",
      "Epoch 1090 | Train Loss: 0.93 | Train Acc: 72.56% | Val Loss: 2.61 | Val Acc: 54.29% | F Score: 0.35\n",
      "Epoch 1100 | Train Loss: 0.89 | Train Acc: 74.10% | Val Loss: 2.58 | Val Acc: 55.35% | F Score: 0.34\n",
      "Epoch 1110 | Train Loss: 0.87 | Train Acc: 74.92% | Val Loss: 2.58 | Val Acc: 56.03% | F Score: 0.34\n",
      "Epoch 1120 | Train Loss: 0.86 | Train Acc: 75.25% | Val Loss: 2.58 | Val Acc: 56.22% | F Score: 0.34\n",
      "Epoch 1130 | Train Loss: 0.98 | Train Acc: 71.05% | Val Loss: 2.67 | Val Acc: 53.91% | F Score: 0.34\n",
      "Epoch 1140 | Train Loss: 0.94 | Train Acc: 72.39% | Val Loss: 2.68 | Val Acc: 53.98% | F Score: 0.34\n",
      "Epoch 1150 | Train Loss: 0.85 | Train Acc: 74.94% | Val Loss: 2.62 | Val Acc: 55.60% | F Score: 0.34\n",
      "Epoch 1160 | Train Loss: 0.83 | Train Acc: 75.60% | Val Loss: 2.61 | Val Acc: 56.81% | F Score: 0.35\n",
      "Epoch 1170 | Train Loss: 0.84 | Train Acc: 75.35% | Val Loss: 2.62 | Val Acc: 55.60% | F Score: 0.35\n",
      "Epoch 1180 | Train Loss: 0.83 | Train Acc: 75.72% | Val Loss: 2.62 | Val Acc: 56.38% | F Score: 0.35\n",
      "Epoch 1190 | Train Loss: 0.87 | Train Acc: 74.60% | Val Loss: 2.63 | Val Acc: 56.56% | F Score: 0.35\n",
      "Epoch 1200 | Train Loss: 0.83 | Train Acc: 75.64% | Val Loss: 2.75 | Val Acc: 55.16% | F Score: 0.31\n",
      "Epoch 1210 | Train Loss: 0.80 | Train Acc: 76.70% | Val Loss: 2.66 | Val Acc: 56.25% | F Score: 0.33\n",
      "Epoch 1220 | Train Loss: 0.81 | Train Acc: 76.12% | Val Loss: 2.70 | Val Acc: 56.06% | F Score: 0.33\n",
      "Epoch 1230 | Train Loss: 0.78 | Train Acc: 77.17% | Val Loss: 2.66 | Val Acc: 56.94% | F Score: 0.35\n",
      "Epoch 1240 | Train Loss: 0.93 | Train Acc: 71.92% | Val Loss: 2.86 | Val Acc: 50.61% | F Score: 0.33\n",
      "Epoch 1250 | Train Loss: 0.77 | Train Acc: 77.55% | Val Loss: 2.67 | Val Acc: 57.19% | F Score: 0.35\n",
      "Epoch 1260 | Train Loss: 0.78 | Train Acc: 76.83% | Val Loss: 2.70 | Val Acc: 56.53% | F Score: 0.34\n",
      "Epoch 1270 | Train Loss: 0.85 | Train Acc: 74.29% | Val Loss: 2.78 | Val Acc: 53.01% | F Score: 0.35\n",
      "Epoch 1280 | Train Loss: 0.78 | Train Acc: 77.01% | Val Loss: 2.69 | Val Acc: 56.38% | F Score: 0.36\n",
      "Epoch 1290 | Train Loss: 0.76 | Train Acc: 77.72% | Val Loss: 2.70 | Val Acc: 56.88% | F Score: 0.36\n",
      "Epoch 1300 | Train Loss: 0.80 | Train Acc: 76.43% | Val Loss: 2.73 | Val Acc: 56.53% | F Score: 0.36\n",
      "Epoch 1310 | Train Loss: 0.75 | Train Acc: 77.70% | Val Loss: 2.72 | Val Acc: 56.53% | F Score: 0.36\n",
      "Epoch 1320 | Train Loss: 0.73 | Train Acc: 78.54% | Val Loss: 2.72 | Val Acc: 57.25% | F Score: 0.35\n",
      "Epoch 1330 | Train Loss: 0.78 | Train Acc: 76.71% | Val Loss: 2.75 | Val Acc: 55.60% | F Score: 0.36\n",
      "Epoch 1340 | Train Loss: 0.80 | Train Acc: 75.93% | Val Loss: 2.77 | Val Acc: 56.06% | F Score: 0.35\n",
      "Epoch 1350 | Train Loss: 1.22 | Train Acc: 62.23% | Val Loss: 3.11 | Val Acc: 44.18% | F Score: 0.30\n",
      "Epoch 1360 | Train Loss: 0.73 | Train Acc: 78.40% | Val Loss: 2.76 | Val Acc: 55.50% | F Score: 0.36\n",
      "Epoch 1370 | Train Loss: 0.82 | Train Acc: 74.98% | Val Loss: 2.83 | Val Acc: 55.75% | F Score: 0.36\n",
      "Epoch 1380 | Train Loss: 0.95 | Train Acc: 70.72% | Val Loss: 3.06 | Val Acc: 46.49% | F Score: 0.31\n",
      "Epoch 1390 | Train Loss: 0.74 | Train Acc: 77.68% | Val Loss: 2.80 | Val Acc: 55.88% | F Score: 0.36\n",
      "Epoch 1400 | Train Loss: 0.69 | Train Acc: 79.67% | Val Loss: 2.77 | Val Acc: 57.22% | F Score: 0.36\n",
      "Epoch 1410 | Train Loss: 0.69 | Train Acc: 79.73% | Val Loss: 2.79 | Val Acc: 57.56% | F Score: 0.36\n",
      "Epoch 1420 | Train Loss: 0.71 | Train Acc: 78.47% | Val Loss: 2.83 | Val Acc: 56.16% | F Score: 0.37\n",
      "Epoch 1430 | Train Loss: 0.69 | Train Acc: 79.48% | Val Loss: 2.81 | Val Acc: 57.00% | F Score: 0.36\n",
      "Epoch 1440 | Train Loss: 0.67 | Train Acc: 80.13% | Val Loss: 2.81 | Val Acc: 57.37% | F Score: 0.36\n",
      "Epoch 1450 | Train Loss: 0.76 | Train Acc: 76.77% | Val Loss: 2.90 | Val Acc: 54.35% | F Score: 0.36\n",
      "Epoch 1460 | Train Loss: 0.68 | Train Acc: 79.91% | Val Loss: 2.83 | Val Acc: 57.22% | F Score: 0.36\n",
      "Epoch 1470 | Train Loss: 0.65 | Train Acc: 80.79% | Val Loss: 2.85 | Val Acc: 57.66% | F Score: 0.35\n",
      "Epoch 1480 | Train Loss: 0.82 | Train Acc: 74.40% | Val Loss: 3.14 | Val Acc: 53.13% | F Score: 0.30\n",
      "Epoch 1490 | Train Loss: 0.66 | Train Acc: 80.20% | Val Loss: 2.92 | Val Acc: 56.50% | F Score: 0.34\n",
      "Epoch 1500 | Train Loss: 0.64 | Train Acc: 81.16% | Val Loss: 2.87 | Val Acc: 57.84% | F Score: 0.36\n",
      "Epoch 1510 | Train Loss: 0.77 | Train Acc: 75.74% | Val Loss: 2.99 | Val Acc: 53.57% | F Score: 0.36\n",
      "Epoch 1520 | Train Loss: 0.63 | Train Acc: 81.36% | Val Loss: 2.89 | Val Acc: 57.62% | F Score: 0.36\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[106], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# reset weights and train model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m gcn_trained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m gcn_trained \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgcn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m.0001\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[96], line 79\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, loader, lr)\u001b[0m\n\u001b[0;32m     76\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;66;03m# run model on validation set\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m     val_loss, val_acc, val_f1 \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Print metrics every epoch\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Jared\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[96], line 96\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(model, loader)\u001b[0m\n\u001b[0;32m     93\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     94\u001b[0m fscore \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 96\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jared\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Jared\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Jared\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jared\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch_geometric\\loader\\dataloader.py:19\u001b[0m, in \u001b[0;36mCollater.__call__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     17\u001b[0m elem \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, BaseData):\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_data_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfollow_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexclude_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default_collate(batch)\n",
      "File \u001b[1;32mc:\\Users\\Jared\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch_geometric\\data\\batch.py:76\u001b[0m, in \u001b[0;36mBatch.from_data_list\u001b[1;34m(cls, data_list, follow_batch, exclude_keys)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_data_list\u001b[39m(\u001b[38;5;28mcls\u001b[39m, data_list: List[BaseData],\n\u001b[0;32m     66\u001b[0m                    follow_batch: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     67\u001b[0m                    exclude_keys: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     68\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Constructs a :class:`~torch_geometric.data.Batch` object from a\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;124;03m    Python list of :class:`~torch_geometric.data.Data` or\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :class:`~torch_geometric.data.HeteroData` objects.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03m    :obj:`follow_batch`.\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;124;03m    Will exclude any keys given in :obj:`exclude_keys`.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m     batch, slice_dict, inc_dict \u001b[38;5;241m=\u001b[39m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincrement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m     batch\u001b[38;5;241m.\u001b[39m_num_graphs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_list)\n\u001b[0;32m     86\u001b[0m     batch\u001b[38;5;241m.\u001b[39m_slice_dict \u001b[38;5;241m=\u001b[39m slice_dict\n",
      "File \u001b[1;32mc:\\Users\\Jared\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch_geometric\\data\\collate.py:84\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(cls, data_list, increment, add_batch, follow_batch, exclude_keys)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Collate attributes into a unified representation:\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m value, slices, incs \u001b[38;5;241m=\u001b[39m \u001b[43m_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, Tensor) \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mis_cuda:\n\u001b[0;32m     88\u001b[0m     device \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mdevice\n",
      "File \u001b[1;32mc:\\Users\\Jared\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch_geometric\\data\\collate.py:131\u001b[0m, in \u001b[0;36m_collate\u001b[1;34m(key, values, data_list, stores, increment)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cat_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m elem\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    130\u001b[0m     values \u001b[38;5;241m=\u001b[39m [value\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m values]\n\u001b[1;32m--> 131\u001b[0m slices \u001b[38;5;241m=\u001b[39m \u001b[43mcumsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcat_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m increment:\n\u001b[0;32m    133\u001b[0m     incs \u001b[38;5;241m=\u001b[39m get_incs(key, values, data_list, stores)\n",
      "File \u001b[1;32mc:\\Users\\Jared\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch_geometric\\data\\collate.py:256\u001b[0m, in \u001b[0;36mcumsum\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m    254\u001b[0m out \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mnew_empty((value\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, ) \u001b[38;5;241m+\u001b[39m value\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m1\u001b[39m:])\n\u001b[0;32m    255\u001b[0m out[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 256\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcumsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# reset weights and train model\n",
    "gcn_trained = None\n",
    "gcn_trained = train(gcn, train_loader,lr=.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[90], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;66;03m# reset weights and train model\u001b[39;00m\n\u001b[0;32m     13\u001b[0m         gcn_trained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m         \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgcn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m.0001\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(h)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(lr)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     17\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(cap\u001b[38;5;241m.\u001b[39mstdout)   \n",
      "Cell \u001b[1;32mIn[88], line 26\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, loader, lr)\u001b[0m\n\u001b[0;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     25\u001b[0m length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(loader)\n\u001b[1;32m---> 26\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m data\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mLongTensor)\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     28\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Jared\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jared\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[43], line 17\u001b[0m, in \u001b[0;36mGCN.forward\u001b[1;34m(self, x, edge_index)\u001b[0m\n\u001b[0;32m     15\u001b[0m h \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mrelu()\n\u001b[0;32m     16\u001b[0m h \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(h, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m---> 17\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m        \n\u001b[0;32m     18\u001b[0m h \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mrelu()\n\u001b[0;32m     19\u001b[0m h \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(h, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n",
      "File \u001b[1;32mc:\\Users\\Jared\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jared\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jared\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch_geometric\\nn\\conv\\gcn_conv.py:198\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[1;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[0;32m    195\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin(x)\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: Tensor, edge_weight: OptTensor)\u001b[39;00m\n\u001b[1;32m--> 198\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m                     \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     out \u001b[38;5;241m=\u001b[39m out \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n",
      "File \u001b[1;32mc:\\Users\\Jared\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py:454\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[1;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    452\u001b[0m         aggr_kwargs \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m res\n\u001b[1;32m--> 454\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggregate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43maggr_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aggregate_forward_hooks\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m    457\u001b[0m     res \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, (aggr_kwargs, ), out)\n",
      "File \u001b[1;32mc:\\Users\\Jared\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py:578\u001b[0m, in \u001b[0;36mMessagePassing.aggregate\u001b[1;34m(self, inputs, index, ptr, dim_size)\u001b[0m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maggregate\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Tensor, index: Tensor,\n\u001b[0;32m    566\u001b[0m               ptr: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    567\u001b[0m               dim_size: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    568\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Aggregates messages from neighbors as\u001b[39;00m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;124;03m    :math:`\\square_{j \\in \\mathcal{N}(i)}`.\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;124;03m    as specified in :meth:`__init__` by the :obj:`aggr` argument.\u001b[39;00m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggr_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mptr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jared\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch_geometric\\nn\\aggr\\base.py:126\u001b[0m, in \u001b[0;36mAggregation.__call__\u001b[1;34m(self, x, index, ptr, dim_size, dim, **kwargs)\u001b[0m\n\u001b[0;32m    124\u001b[0m         dim_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(index\u001b[38;5;241m.\u001b[39mmax()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m index\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate:\n\u001b[1;32m--> 126\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m index\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m dim_size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    127\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered invalid \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdim_size\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    128\u001b[0m                              \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdim_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but expected \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    129\u001b[0m                              \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>= \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(index\u001b[38;5;241m.\u001b[39mmax())\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(x, index, ptr, dim_size, dim, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%capture cap\n",
    "\n",
    "lr = [.1,.05,.01,.005,.001,.00005,.00001]\n",
    "h = [200,400,800,1600,3200]\n",
    "\n",
    "for rate in lr:\n",
    "    for hidden in h:\n",
    "        print(rate,hidden)\n",
    "        gcn_trained = None\n",
    "        gcn = None\n",
    "        gcn = GCN(dim_h=hidden).to(device)\n",
    "        gcn\n",
    "\n",
    "        # reset weights and train model\n",
    "        gcn_trained = None\n",
    "        train(gcn, train_loader,lr=.0001)\n",
    "\n",
    "with open('h_'+str(h)+'lr_'+str(lr)+'.txt', 'w') as f:\n",
    "        f.write(cap.stdout)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('h.txt', 'w') as f:\n",
    "        f.write(cap.stdout)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 14.20 | Test Acc: 45.04% | F Score: 0.22\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc, test_f1 = test(gcn_trained, test_loader)\n",
    "print(f'Test Loss: {test_loss:.2f} | Test Acc: {test_acc*100:.2f}% | F Score: {test_f1:.2f}')\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
