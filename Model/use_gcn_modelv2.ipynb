{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from use_dataset import ProofDataset\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.loader import DataLoader, NeighborLoader\n",
    "import random\n",
    "from sklearn.metrics import f1_score as f1\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, ReLU, Dropout\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use file_limit=5000 to only load and verify the first 5000 graphs (~60 MB)\n",
    "# if entire graph dataset is desired, use file_limit=None\n",
    "file_limit = 5000    # desired number of graphs to work with\n",
    "vocab_size = 1598   # number of characters in our vocabulary\n",
    "\n",
    "pf_data = ProofDataset(root=\"data/\",file_limit=file_limit)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set   = 4000 graphs\n",
      "Validation set = 500 graphs\n",
      "Test set       = 500 graphs\n"
     ]
    }
   ],
   "source": [
    "# make train/val/test for GCN\n",
    "# set seed for random # generation\n",
    "random.seed(10)\n",
    "length = file_limit\n",
    "total_indices = [i for i in range(file_limit)]\n",
    "\n",
    "# create index vectors to filter dataset\n",
    "train_indices = random.sample(total_indices, int(length*.8))\n",
    "train_indices.sort()\n",
    "\n",
    "val_index_options = [x for x in total_indices if x not in train_indices]\n",
    "val_indices = random.sample(val_index_options, int(length*.1))\n",
    "val_indices.sort()\n",
    "\n",
    "test_index_options = [x for x in total_indices if x not in train_indices if x not in val_indices]\n",
    "test_indices = random.sample(test_index_options, int(length*.1))\n",
    "test_indices.sort()\n",
    "\n",
    "# Create training, validation, and test sets\n",
    "train_dataset = pf_data[train_indices]\n",
    "val_dataset = pf_data[val_indices]\n",
    "test_dataset = pf_data[test_indices]\n",
    "\n",
    "print(f'Training set   = {len(train_dataset)} graphs')\n",
    "print(f'Validation set = {len(val_dataset)} graphs')\n",
    "print(f'Test set       = {len(test_dataset)} graphs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mini-batches\n",
    "# Shuffling for now; probably will remove shuffling later\n",
    "train_loader = DataLoader(train_dataset, batch_size=1000, shuffle=True,num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=1000, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loader:\n",
      " - Batch 0: DataBatch(x=[5770, 512], edge_index=[2, 4770], y=[5770], batch=[5770], ptr=[1001])\n",
      " - Batch 1: DataBatch(x=[6208, 512], edge_index=[2, 5208], y=[6208], batch=[6208], ptr=[1001])\n",
      " - Batch 2: DataBatch(x=[6196, 512], edge_index=[2, 5196], y=[6196], batch=[6196], ptr=[1001])\n",
      " - Batch 3: DataBatch(x=[6376, 512], edge_index=[2, 5376], y=[6376], batch=[6376], ptr=[1001])\n",
      "\n",
      "Validation loader:\n",
      " - Batch 0: DataBatch(x=[3207, 512], edge_index=[2, 2707], y=[3207], batch=[3207], ptr=[501])\n",
      "\n",
      "Test loader:\n",
      " - Batch 0: DataBatch(x=[3037, 512], edge_index=[2, 2537], y=[3037], batch=[3037], ptr=[501])\n"
     ]
    }
   ],
   "source": [
    "print('\\nTrain loader:')\n",
    "for i, batch in enumerate(train_loader):\n",
    "    print(f' - Batch {i}: {batch}')\n",
    "\n",
    "print('\\nValidation loader:')\n",
    "for i, batch in enumerate(val_loader):\n",
    "    print(f' - Batch {i}: {batch}')\n",
    "\n",
    "print('\\nTest loader:')\n",
    "for i, batch in enumerate(test_loader):\n",
    "    print(f' - Batch {i}: {batch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary to record label frequency\n",
    "\n",
    "# get max label used in pf_data\n",
    "max_label = 0\n",
    "\n",
    "for i in range(file_limit):\n",
    "    for j in pf_data.get(i).y:\n",
    "        if j > max_label:\n",
    "            max_label = j.to(int).item()\n",
    "\n",
    "# initialize histogram for labels used in pf_data            \n",
    "label_count = {}\n",
    "\n",
    "for i in range(file_limit):\n",
    "    for j in range(max_label+1):        \n",
    "        label_count[j] = 0\n",
    "\n",
    "for i in range(file_limit):\n",
    "    for j in pf_data.get(i).y:\n",
    "        label_count[j.to(int).item()] += 1\n",
    "\n",
    "step_count = 0\n",
    "max = 0\n",
    "max_freq_index = None   #find the most frequently used index\n",
    "labels_never_used = 0\n",
    "labels_used_once = 0\n",
    "labels_used_twice = 0\n",
    "\n",
    "\n",
    "for k,v in label_count.items():\n",
    "    step_count += v\n",
    "    if v > max:     \n",
    "        max = v\n",
    "        max_freq_index = k\n",
    "\n",
    "    if v == 0:\n",
    "        labels_never_used += 1\n",
    "    if v == 1:\n",
    "        labels_used_once += 1\n",
    "    if v ==2:\n",
    "        labels_used_twice += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of steps is: 30794\n",
      "highest frequency label is 0 and occurs 5140 times\n",
      "final label used is 3228\n",
      "{0: 5140, 1: 545, 2: 52, 3: 10, 4: 3, 5: 23, 6: 7, 7: 269, 8: 2, 9: 13, 10: 39, 11: 41, 12: 389, 13: 25, 14: 3, 15: 27, 16: 2, 17: 244, 18: 22, 19: 56, 20: 5, 21: 11, 22: 7, 23: 20, 24: 107, 25: 5, 26: 46, 27: 74, 28: 13, 29: 9, 30: 8, 31: 9, 32: 5, 33: 2, 34: 17, 35: 4, 36: 6, 37: 4, 38: 2, 39: 25, 40: 1, 41: 4, 42: 16, 43: 2, 44: 4, 45: 6, 46: 1, 47: 12, 48: 3, 49: 3, 50: 4, 51: 5, 52: 17, 53: 7, 54: 16, 55: 1, 56: 6, 57: 6, 58: 2, 59: 3, 60: 8, 61: 3, 62: 3, 63: 4, 64: 11, 65: 5, 66: 6, 67: 14, 68: 49, 69: 17, 70: 6, 71: 9, 72: 6, 73: 7, 74: 5, 75: 7, 76: 2, 77: 4, 78: 2, 79: 3, 80: 4, 81: 3, 82: 3, 83: 2, 84: 5, 85: 1, 86: 2, 87: 4, 88: 6, 89: 12, 90: 1, 91: 2, 92: 13, 93: 11, 94: 41, 95: 7, 96: 2, 97: 1, 98: 5, 99: 3, 100: 1, 101: 2, 102: 8, 103: 1, 104: 3, 105: 11, 106: 7, 107: 2, 108: 3, 109: 19, 110: 8, 111: 2, 112: 12, 113: 7, 114: 13, 115: 2, 116: 17, 117: 5, 118: 1, 119: 3, 120: 2, 121: 21, 122: 9, 123: 28, 124: 4, 125: 1, 126: 1, 127: 3, 128: 7, 129: 3, 130: 1, 131: 1, 132: 2, 133: 2, 134: 3, 135: 4, 136: 8, 137: 2, 138: 2, 139: 2, 140: 1, 141: 2, 142: 1, 143: 7, 144: 5, 145: 8, 146: 82, 147: 2, 148: 2, 149: 24, 150: 3, 151: 3, 152: 2, 153: 1, 154: 1, 155: 3, 156: 2, 157: 2, 158: 11, 159: 5, 160: 3, 161: 2, 162: 1, 163: 8, 164: 1, 165: 1, 166: 5, 167: 5, 168: 126, 169: 1, 170: 4, 171: 59, 172: 4, 173: 26, 174: 82, 175: 129, 176: 99, 177: 4, 178: 23, 179: 4, 180: 23, 181: 53, 182: 114, 183: 28, 184: 24, 185: 1, 186: 37, 187: 58, 188: 77, 189: 58, 190: 22, 191: 28, 192: 124, 193: 40, 194: 4, 195: 8, 196: 6, 197: 43, 198: 10, 199: 16, 200: 11, 201: 7, 202: 4, 203: 31, 204: 14, 205: 18, 206: 20, 207: 24, 208: 18, 209: 3, 210: 2, 211: 19, 212: 32, 213: 2, 214: 3, 215: 51, 216: 24, 217: 3, 218: 9, 219: 12, 220: 1, 221: 12, 222: 1, 223: 1, 224: 8, 225: 25, 226: 15, 227: 16, 228: 1, 229: 473, 230: 38, 231: 73, 232: 171, 233: 72, 234: 6, 235: 19, 236: 19, 237: 66, 238: 6, 239: 20, 240: 2, 241: 60, 242: 5, 243: 23, 244: 20, 245: 7, 246: 27, 247: 1, 248: 1, 249: 44, 250: 106, 251: 14, 252: 15, 253: 3, 254: 38, 255: 6, 256: 241, 257: 36, 258: 9, 259: 2, 260: 1, 261: 1, 262: 1, 263: 11, 264: 112, 265: 23, 266: 6, 267: 10, 268: 59, 269: 13, 270: 40, 271: 10, 272: 1, 273: 12, 274: 2, 275: 2, 276: 8, 277: 3, 278: 1, 279: 2, 280: 1, 281: 6, 282: 8, 283: 31, 284: 21, 285: 48, 286: 16, 287: 10, 288: 12, 289: 50, 290: 26, 291: 12, 292: 15, 293: 68, 294: 24, 295: 1, 296: 2, 297: 2, 298: 48, 299: 13, 300: 4, 301: 2, 302: 1, 303: 4, 304: 10, 305: 26, 306: 2, 307: 9, 308: 12, 309: 6, 310: 11, 311: 3, 312: 8, 313: 1, 314: 4, 315: 6, 316: 1, 317: 6, 318: 2, 319: 4, 320: 6, 321: 5, 322: 20, 323: 4, 324: 3, 325: 4, 326: 6, 327: 4, 328: 5, 329: 1, 330: 2, 331: 8, 332: 2, 333: 1, 334: 16, 335: 1, 336: 21, 337: 4, 338: 17, 339: 8, 340: 8, 341: 4, 342: 1, 343: 143, 344: 35, 345: 4, 346: 28, 347: 2, 348: 192, 349: 38, 350: 1, 351: 25, 352: 4, 353: 6, 354: 7, 355: 11, 356: 6, 357: 4, 358: 4, 359: 1, 360: 1, 361: 3, 362: 1, 363: 1, 364: 2, 365: 3, 366: 1, 367: 1, 368: 1, 369: 1, 370: 1, 371: 2, 372: 1, 373: 44, 374: 8, 375: 3, 376: 5, 377: 6, 378: 2, 379: 6, 380: 1, 381: 37, 382: 5, 383: 61, 384: 3, 385: 10, 386: 5, 387: 3, 388: 6, 389: 2, 390: 13, 391: 31, 392: 15, 393: 7, 394: 10, 395: 1, 396: 4, 397: 21, 398: 21, 399: 5, 400: 3, 401: 4, 402: 109, 403: 121, 404: 94, 405: 71, 406: 3, 407: 2, 408: 3, 409: 11, 410: 7, 411: 9, 412: 12, 413: 14, 414: 11, 415: 1, 416: 5, 417: 2, 418: 5, 419: 7, 420: 5, 421: 2, 422: 19, 423: 8, 424: 77, 425: 9, 426: 1, 427: 2, 428: 1, 429: 3, 430: 8, 431: 5, 432: 1, 433: 1, 434: 2, 435: 3, 436: 4, 437: 2, 438: 9, 439: 25, 440: 16, 441: 25, 442: 4, 443: 7, 444: 1, 445: 4, 446: 6, 447: 4, 448: 5, 449: 2, 450: 1, 451: 3, 452: 2, 453: 2, 454: 1, 455: 2, 456: 3, 457: 2, 458: 1, 459: 12, 460: 5, 461: 1, 462: 4, 463: 2, 464: 9, 465: 1, 466: 9, 467: 9, 468: 2, 469: 4, 470: 9, 471: 10, 472: 1, 473: 12, 474: 3, 475: 7, 476: 2, 477: 7, 478: 29, 479: 13, 480: 7, 481: 2, 482: 9, 483: 27, 484: 7, 485: 1, 486: 18, 487: 42, 488: 5, 489: 1, 490: 1, 491: 7, 492: 23, 493: 19, 494: 2, 495: 19, 496: 4, 497: 1, 498: 1, 499: 3, 500: 1, 501: 2, 502: 1, 503: 2, 504: 2, 505: 16, 506: 10, 507: 8, 508: 25, 509: 4, 510: 7, 511: 3, 512: 17, 513: 6, 514: 2, 515: 3, 516: 81, 517: 62, 518: 4, 519: 65, 520: 2, 521: 43, 522: 47, 523: 91, 524: 1, 525: 1, 526: 1, 527: 8, 528: 3, 529: 1, 530: 2, 531: 12, 532: 5, 533: 1, 534: 2, 535: 1, 536: 1, 537: 2, 538: 16, 539: 2, 540: 1, 541: 5, 542: 1, 543: 2, 544: 3, 545: 1, 546: 1, 547: 1, 548: 4, 549: 1, 550: 2, 551: 2, 552: 2, 553: 1, 554: 6, 555: 2, 556: 1, 557: 1, 558: 3, 559: 3, 560: 2, 561: 2, 562: 2, 563: 2, 564: 1, 565: 8, 566: 3, 567: 11, 568: 15, 569: 29, 570: 2, 571: 5, 572: 1, 573: 2, 574: 1, 575: 3, 576: 1, 577: 1, 578: 3, 579: 15, 580: 4, 581: 4, 582: 13, 583: 23, 584: 6, 585: 7, 586: 3, 587: 3, 588: 1, 589: 12, 590: 8, 591: 9, 592: 7, 593: 4, 594: 2, 595: 3, 596: 1, 597: 3, 598: 1, 599: 3, 600: 1, 601: 3, 602: 1, 603: 3, 604: 1, 605: 3, 606: 1, 607: 3, 608: 1, 609: 1, 610: 1, 611: 2, 612: 2, 613: 1, 614: 1, 615: 1, 616: 9, 617: 12, 618: 11, 619: 12, 620: 1, 621: 2, 622: 1, 623: 1, 624: 1, 625: 3, 626: 13, 627: 1, 628: 1, 629: 2, 630: 3, 631: 1, 632: 5, 633: 17, 634: 3, 635: 2, 636: 1, 637: 1, 638: 3, 639: 6, 640: 8, 641: 1, 642: 1, 643: 1, 644: 4, 645: 1, 646: 1, 647: 1, 648: 43, 649: 3, 650: 1, 651: 10, 652: 2, 653: 17, 654: 3, 655: 1, 656: 2, 657: 58, 658: 2, 659: 16, 660: 1, 661: 2, 662: 21, 663: 16, 664: 11, 665: 4, 666: 1, 667: 38, 668: 27, 669: 8, 670: 41, 671: 4, 672: 3, 673: 13, 674: 11, 675: 3, 676: 1, 677: 4, 678: 4, 679: 2, 680: 3, 681: 4, 682: 2, 683: 2, 684: 5, 685: 1, 686: 4, 687: 1, 688: 1, 689: 1, 690: 1, 691: 2, 692: 9, 693: 5, 694: 2, 695: 6, 696: 25, 697: 10, 698: 40, 699: 7, 700: 7, 701: 36, 702: 2, 703: 4, 704: 12, 705: 1, 706: 5, 707: 1, 708: 1, 709: 2, 710: 2, 711: 1, 712: 24, 713: 1, 714: 4, 715: 1, 716: 1, 717: 5, 718: 1, 719: 4, 720: 3, 721: 5, 722: 4, 723: 1, 724: 2, 725: 2, 726: 7, 727: 1, 728: 5, 729: 7, 730: 2, 731: 2, 732: 2, 733: 3, 734: 1, 735: 1, 736: 21, 737: 5, 738: 17, 739: 1, 740: 1, 741: 1, 742: 4, 743: 6, 744: 1, 745: 3, 746: 1, 747: 1, 748: 1, 749: 1, 750: 1, 751: 9, 752: 4, 753: 10, 754: 6, 755: 2, 756: 1, 757: 1, 758: 1, 759: 3, 760: 1, 761: 1, 762: 1, 763: 1, 764: 1, 765: 1, 766: 1, 767: 1, 768: 1, 769: 4, 770: 1, 771: 2, 772: 1, 773: 2, 774: 3, 775: 3, 776: 5, 777: 1, 778: 1, 779: 1, 780: 1, 781: 7, 782: 6, 783: 5, 784: 1, 785: 1, 786: 1, 787: 1, 788: 4, 789: 2, 790: 2, 791: 1, 792: 1, 793: 1, 794: 1, 795: 25, 796: 57, 797: 10, 798: 5, 799: 1, 800: 1, 801: 23, 802: 4, 803: 3, 804: 3, 805: 2, 806: 1, 807: 2, 808: 2, 809: 3, 810: 1, 811: 13, 812: 9, 813: 1, 814: 1, 815: 1, 816: 6, 817: 5, 818: 11, 819: 17, 820: 13, 821: 5, 822: 6, 823: 2, 824: 2, 825: 2, 826: 1, 827: 3, 828: 1, 829: 17, 830: 7, 831: 6, 832: 18, 833: 43, 834: 43, 835: 46, 836: 16, 837: 15, 838: 21, 839: 1, 840: 1, 841: 1, 842: 6, 843: 2, 844: 3, 845: 5, 846: 3, 847: 3, 848: 3, 849: 7, 850: 1, 851: 1, 852: 4, 853: 1, 854: 4, 855: 4, 856: 5, 857: 2, 858: 1, 859: 2, 860: 1, 861: 3, 862: 1, 863: 4, 864: 2, 865: 1, 866: 6, 867: 6, 868: 7, 869: 6, 870: 6, 871: 6, 872: 9, 873: 9, 874: 8, 875: 7, 876: 7, 877: 7, 878: 3, 879: 3, 880: 3, 881: 3, 882: 3, 883: 3, 884: 3, 885: 3, 886: 3, 887: 3, 888: 3, 889: 3, 890: 3, 891: 3, 892: 3, 893: 6, 894: 4, 895: 4, 896: 2, 897: 2, 898: 1, 899: 2, 900: 2, 901: 2, 902: 1, 903: 3, 904: 2, 905: 1, 906: 1, 907: 16, 908: 3, 909: 2, 910: 2, 911: 1, 912: 2, 913: 2, 914: 2, 915: 3, 916: 4, 917: 2, 918: 1, 919: 1, 920: 1, 921: 1, 922: 1, 923: 2, 924: 3, 925: 2, 926: 3, 927: 1, 928: 2, 929: 1, 930: 1, 931: 4, 932: 2, 933: 3, 934: 1, 935: 1, 936: 1, 937: 9, 938: 1, 939: 4, 940: 6, 941: 1, 942: 3, 943: 3, 944: 1, 945: 1, 946: 1, 947: 1, 948: 2, 949: 1, 950: 1, 951: 1, 952: 2, 953: 1, 954: 21, 955: 3, 956: 6, 957: 5, 958: 8, 959: 2, 960: 5, 961: 2, 962: 4, 963: 4, 964: 2, 965: 1, 966: 1, 967: 24, 968: 5, 969: 6, 970: 2, 971: 3, 972: 1, 973: 2, 974: 2, 975: 1, 976: 4, 977: 3, 978: 23, 979: 4, 980: 6, 981: 2, 982: 2, 983: 2, 984: 1, 985: 1, 986: 25, 987: 2, 988: 43, 989: 1, 990: 9, 991: 6, 992: 4, 993: 5, 994: 10, 995: 1, 996: 4, 997: 9, 998: 1, 999: 1, 1000: 3, 1001: 5, 1002: 1, 1003: 1, 1004: 1, 1005: 1, 1006: 1, 1007: 1, 1008: 1, 1009: 2, 1010: 1, 1011: 1, 1012: 1, 1013: 6, 1014: 1, 1015: 2, 1016: 5, 1017: 1, 1018: 1, 1019: 1, 1020: 1, 1021: 2, 1022: 1, 1023: 7, 1024: 2, 1025: 4, 1026: 1, 1027: 1, 1028: 1, 1029: 1, 1030: 1, 1031: 1, 1032: 32, 1033: 8, 1034: 1, 1035: 3, 1036: 3, 1037: 34, 1038: 3, 1039: 6, 1040: 3, 1041: 3, 1042: 1, 1043: 2, 1044: 2, 1045: 2, 1046: 3, 1047: 24, 1048: 3, 1049: 3, 1050: 2, 1051: 5, 1052: 3, 1053: 4, 1054: 1, 1055: 1, 1056: 1, 1057: 2, 1058: 5, 1059: 2, 1060: 2, 1061: 12, 1062: 3, 1063: 2, 1064: 14, 1065: 3, 1066: 2, 1067: 4, 1068: 3, 1069: 2, 1070: 2, 1071: 2, 1072: 10, 1073: 2, 1074: 26, 1075: 7, 1076: 11, 1077: 1, 1078: 2, 1079: 6, 1080: 4, 1081: 6, 1082: 4, 1083: 6, 1084: 1, 1085: 3, 1086: 6, 1087: 1, 1088: 6, 1089: 5, 1090: 10, 1091: 2, 1092: 3, 1093: 1, 1094: 2, 1095: 18, 1096: 4, 1097: 4, 1098: 3, 1099: 10, 1100: 8, 1101: 1, 1102: 1, 1103: 2, 1104: 1, 1105: 45, 1106: 4, 1107: 4, 1108: 7, 1109: 6, 1110: 6, 1111: 7, 1112: 3, 1113: 3, 1114: 3, 1115: 6, 1116: 2, 1117: 4, 1118: 2, 1119: 3, 1120: 3, 1121: 4, 1122: 2, 1123: 2, 1124: 2, 1125: 42, 1126: 9, 1127: 4, 1128: 5, 1129: 3, 1130: 3, 1131: 6, 1132: 2, 1133: 1, 1134: 3, 1135: 3, 1136: 3, 1137: 1, 1138: 10, 1139: 29, 1140: 6, 1141: 20, 1142: 18, 1143: 17, 1144: 37, 1145: 9, 1146: 4, 1147: 1, 1148: 1, 1149: 2, 1150: 4, 1151: 6, 1152: 1, 1153: 1, 1154: 1, 1155: 34, 1156: 38, 1157: 2, 1158: 20, 1159: 1, 1160: 5, 1161: 3, 1162: 1, 1163: 7, 1164: 8, 1165: 4, 1166: 1, 1167: 1, 1168: 26, 1169: 4, 1170: 33, 1171: 4, 1172: 17, 1173: 3, 1174: 5, 1175: 1, 1176: 2, 1177: 26, 1178: 1, 1179: 7, 1180: 60, 1181: 8, 1182: 7, 1183: 1, 1184: 23, 1185: 11, 1186: 4, 1187: 9, 1188: 156, 1189: 9, 1190: 2, 1191: 17, 1192: 9, 1193: 8, 1194: 10, 1195: 3, 1196: 7, 1197: 3, 1198: 24, 1199: 5, 1200: 17, 1201: 29, 1202: 9, 1203: 21, 1204: 1, 1205: 5, 1206: 1, 1207: 1, 1208: 3, 1209: 5, 1210: 1, 1211: 1, 1212: 4, 1213: 128, 1214: 23, 1215: 3, 1216: 1, 1217: 6, 1218: 34, 1219: 28, 1220: 1, 1221: 2, 1222: 39, 1223: 18, 1224: 5, 1225: 18, 1226: 1, 1227: 3, 1228: 2, 1229: 2, 1230: 2, 1231: 6, 1232: 4, 1233: 18, 1234: 1, 1235: 1, 1236: 4, 1237: 15, 1238: 8, 1239: 1, 1240: 1, 1241: 10, 1242: 1, 1243: 7, 1244: 6, 1245: 2, 1246: 8, 1247: 2, 1248: 14, 1249: 1, 1250: 54, 1251: 17, 1252: 84, 1253: 5, 1254: 2, 1255: 15, 1256: 9, 1257: 1, 1258: 1, 1259: 49, 1260: 1, 1261: 13, 1262: 1, 1263: 321, 1264: 11, 1265: 18, 1266: 17, 1267: 7, 1268: 49, 1269: 32, 1270: 4, 1271: 9, 1272: 3, 1273: 4, 1274: 36, 1275: 5, 1276: 2, 1277: 25, 1278: 9, 1279: 6, 1280: 9, 1281: 1, 1282: 1, 1283: 34, 1284: 1, 1285: 15, 1286: 2, 1287: 1, 1288: 2, 1289: 1, 1290: 13, 1291: 1, 1292: 1, 1293: 18, 1294: 4, 1295: 13, 1296: 2, 1297: 7, 1298: 1, 1299: 1, 1300: 1, 1301: 2, 1302: 1, 1303: 1, 1304: 1, 1305: 3, 1306: 26, 1307: 4, 1308: 2, 1309: 2, 1310: 2, 1311: 1, 1312: 4, 1313: 2, 1314: 2, 1315: 2, 1316: 1, 1317: 4, 1318: 1, 1319: 4, 1320: 7, 1321: 1, 1322: 5, 1323: 8, 1324: 3, 1325: 1, 1326: 3, 1327: 2, 1328: 3, 1329: 3, 1330: 1, 1331: 6, 1332: 5, 1333: 3, 1334: 3, 1335: 1, 1336: 2, 1337: 3, 1338: 3, 1339: 20, 1340: 1, 1341: 7, 1342: 12, 1343: 17, 1344: 12, 1345: 1, 1346: 23, 1347: 8, 1348: 5, 1349: 10, 1350: 4, 1351: 20, 1352: 23, 1353: 3, 1354: 3, 1355: 4, 1356: 1, 1357: 2, 1358: 1, 1359: 5, 1360: 2, 1361: 16, 1362: 12, 1363: 2, 1364: 1, 1365: 2, 1366: 4, 1367: 1, 1368: 4, 1369: 1, 1370: 3, 1371: 2, 1372: 3, 1373: 5, 1374: 5, 1375: 1, 1376: 2, 1377: 2, 1378: 1, 1379: 1, 1380: 24, 1381: 5, 1382: 11, 1383: 4, 1384: 5, 1385: 5, 1386: 14, 1387: 1, 1388: 33, 1389: 2, 1390: 2, 1391: 3, 1392: 11, 1393: 6, 1394: 31, 1395: 21, 1396: 4, 1397: 2, 1398: 2, 1399: 6, 1400: 2, 1401: 1, 1402: 16, 1403: 1, 1404: 2, 1405: 8, 1406: 7, 1407: 4, 1408: 2, 1409: 1, 1410: 1, 1411: 2, 1412: 1, 1413: 1, 1414: 4, 1415: 12, 1416: 1, 1417: 2, 1418: 1, 1419: 2, 1420: 1, 1421: 1, 1422: 2, 1423: 11, 1424: 1, 1425: 1, 1426: 1, 1427: 1, 1428: 7, 1429: 5, 1430: 3, 1431: 5, 1432: 41, 1433: 3, 1434: 29, 1435: 49, 1436: 7, 1437: 2, 1438: 8, 1439: 42, 1440: 8, 1441: 2, 1442: 17, 1443: 2, 1444: 1, 1445: 2, 1446: 3, 1447: 3, 1448: 21, 1449: 2, 1450: 3, 1451: 9, 1452: 2, 1453: 3, 1454: 4, 1455: 8, 1456: 2, 1457: 31, 1458: 3, 1459: 47, 1460: 7, 1461: 38, 1462: 4, 1463: 9, 1464: 8, 1465: 2, 1466: 5, 1467: 4, 1468: 30, 1469: 14, 1470: 1, 1471: 14, 1472: 6, 1473: 1, 1474: 5, 1475: 5, 1476: 5, 1477: 7, 1478: 7, 1479: 14, 1480: 7, 1481: 10, 1482: 5, 1483: 32, 1484: 1, 1485: 2, 1486: 12, 1487: 19, 1488: 8, 1489: 1, 1490: 1, 1491: 15, 1492: 6, 1493: 4, 1494: 1, 1495: 1, 1496: 3, 1497: 3, 1498: 4, 1499: 1, 1500: 9, 1501: 1, 1502: 1, 1503: 1, 1504: 3, 1505: 1, 1506: 2, 1507: 5, 1508: 5, 1509: 8, 1510: 40, 1511: 14, 1512: 3, 1513: 8, 1514: 3, 1515: 1, 1516: 6, 1517: 2, 1518: 2, 1519: 3, 1520: 1, 1521: 1, 1522: 3, 1523: 4, 1524: 5, 1525: 2, 1526: 18, 1527: 1, 1528: 6, 1529: 10, 1530: 2, 1531: 1, 1532: 10, 1533: 3, 1534: 5, 1535: 8, 1536: 1, 1537: 1, 1538: 1, 1539: 3, 1540: 2, 1541: 1, 1542: 2, 1543: 2, 1544: 2, 1545: 8, 1546: 6, 1547: 1, 1548: 1, 1549: 2, 1550: 4, 1551: 2, 1552: 2, 1553: 1, 1554: 3, 1555: 1, 1556: 1, 1557: 1, 1558: 17, 1559: 1, 1560: 2, 1561: 7, 1562: 33, 1563: 20, 1564: 1, 1565: 1, 1566: 6, 1567: 4, 1568: 11, 1569: 2, 1570: 4, 1571: 4, 1572: 2, 1573: 1, 1574: 23, 1575: 18, 1576: 1, 1577: 1, 1578: 2, 1579: 3, 1580: 4, 1581: 6, 1582: 2, 1583: 1, 1584: 1, 1585: 1, 1586: 1, 1587: 8, 1588: 3, 1589: 1, 1590: 1, 1591: 5, 1592: 3, 1593: 1, 1594: 5, 1595: 1, 1596: 4, 1597: 3, 1598: 4, 1599: 3, 1600: 15, 1601: 1, 1602: 3, 1603: 2, 1604: 1, 1605: 1, 1606: 2, 1607: 1, 1608: 6, 1609: 6, 1610: 3, 1611: 3, 1612: 1, 1613: 3, 1614: 3, 1615: 2, 1616: 2, 1617: 2, 1618: 2, 1619: 1, 1620: 1, 1621: 1, 1622: 2, 1623: 5, 1624: 5, 1625: 2, 1626: 4, 1627: 2, 1628: 3, 1629: 2, 1630: 1, 1631: 3, 1632: 3, 1633: 2, 1634: 3, 1635: 11, 1636: 24, 1637: 1, 1638: 4, 1639: 7, 1640: 3, 1641: 2, 1642: 3, 1643: 4, 1644: 1, 1645: 3, 1646: 3, 1647: 1, 1648: 4, 1649: 4, 1650: 2, 1651: 1, 1652: 2, 1653: 1, 1654: 2, 1655: 3, 1656: 1, 1657: 2, 1658: 2, 1659: 1, 1660: 15, 1661: 2, 1662: 2, 1663: 3, 1664: 9, 1665: 2, 1666: 2, 1667: 2, 1668: 1, 1669: 4, 1670: 1, 1671: 1, 1672: 2, 1673: 2, 1674: 1, 1675: 2, 1676: 1, 1677: 2, 1678: 1, 1679: 1, 1680: 2, 1681: 2, 1682: 16, 1683: 2, 1684: 1, 1685: 4, 1686: 2, 1687: 2, 1688: 5, 1689: 1, 1690: 1, 1691: 1, 1692: 1, 1693: 3, 1694: 1, 1695: 2, 1696: 12, 1697: 2, 1698: 1, 1699: 1, 1700: 6, 1701: 3, 1702: 4, 1703: 5, 1704: 3, 1705: 4, 1706: 1, 1707: 1, 1708: 1, 1709: 1, 1710: 7, 1711: 5, 1712: 1, 1713: 1, 1714: 1, 1715: 1, 1716: 1, 1717: 2, 1718: 3, 1719: 2, 1720: 2, 1721: 1, 1722: 1, 1723: 1, 1724: 1, 1725: 1, 1726: 1, 1727: 1, 1728: 1, 1729: 1, 1730: 1, 1731: 4, 1732: 1, 1733: 1, 1734: 1, 1735: 1, 1736: 1, 1737: 1, 1738: 1, 1739: 1, 1740: 3, 1741: 25, 1742: 6, 1743: 3, 1744: 9, 1745: 16, 1746: 1, 1747: 4, 1748: 19, 1749: 2, 1750: 3, 1751: 5, 1752: 6, 1753: 1, 1754: 12, 1755: 5, 1756: 4, 1757: 8, 1758: 1, 1759: 4, 1760: 3, 1761: 7, 1762: 4, 1763: 3, 1764: 33, 1765: 6, 1766: 1, 1767: 7, 1768: 1, 1769: 11, 1770: 11, 1771: 2, 1772: 3, 1773: 13, 1774: 3, 1775: 27, 1776: 12, 1777: 6, 1778: 3, 1779: 2, 1780: 2, 1781: 1, 1782: 2, 1783: 2, 1784: 4, 1785: 3, 1786: 1, 1787: 2, 1788: 1, 1789: 1, 1790: 3, 1791: 1, 1792: 2, 1793: 1, 1794: 3, 1795: 5, 1796: 4, 1797: 2, 1798: 4, 1799: 1, 1800: 4, 1801: 1, 1802: 3, 1803: 4, 1804: 3, 1805: 1, 1806: 1, 1807: 2, 1808: 5, 1809: 1, 1810: 1, 1811: 5, 1812: 1, 1813: 2, 1814: 2, 1815: 1, 1816: 4, 1817: 1, 1818: 3, 1819: 4, 1820: 2, 1821: 1, 1822: 4, 1823: 1, 1824: 1, 1825: 3, 1826: 1, 1827: 1, 1828: 1, 1829: 1, 1830: 3, 1831: 2, 1832: 3, 1833: 4, 1834: 2, 1835: 1, 1836: 2, 1837: 1, 1838: 2, 1839: 1, 1840: 1, 1841: 1, 1842: 1, 1843: 1, 1844: 3, 1845: 1, 1846: 2, 1847: 1, 1848: 51, 1849: 3, 1850: 25, 1851: 2, 1852: 4, 1853: 10, 1854: 1, 1855: 1, 1856: 1, 1857: 1, 1858: 1, 1859: 24, 1860: 34, 1861: 20, 1862: 22, 1863: 16, 1864: 36, 1865: 54, 1866: 21, 1867: 75, 1868: 26, 1869: 12, 1870: 26, 1871: 33, 1872: 37, 1873: 20, 1874: 2, 1875: 7, 1876: 13, 1877: 1, 1878: 8, 1879: 1, 1880: 19, 1881: 77, 1882: 9, 1883: 26, 1884: 56, 1885: 17, 1886: 1, 1887: 3, 1888: 10, 1889: 3, 1890: 53, 1891: 9, 1892: 32, 1893: 5, 1894: 6, 1895: 26, 1896: 3, 1897: 1, 1898: 1, 1899: 33, 1900: 1, 1901: 4, 1902: 33, 1903: 4, 1904: 3, 1905: 3, 1906: 14, 1907: 2, 1908: 4, 1909: 1, 1910: 53, 1911: 4, 1912: 5, 1913: 27, 1914: 48, 1915: 3, 1916: 3, 1917: 16, 1918: 6, 1919: 6, 1920: 1, 1921: 27, 1922: 70, 1923: 2, 1924: 12, 1925: 44, 1926: 95, 1927: 48, 1928: 2, 1929: 3, 1930: 24, 1931: 1, 1932: 17, 1933: 9, 1934: 3, 1935: 1, 1936: 3, 1937: 2, 1938: 7, 1939: 1, 1940: 5, 1941: 3, 1942: 2, 1943: 1, 1944: 1, 1945: 1, 1946: 1, 1947: 3, 1948: 1, 1949: 1, 1950: 1, 1951: 1, 1952: 2, 1953: 1, 1954: 2, 1955: 2, 1956: 15, 1957: 7, 1958: 2, 1959: 1, 1960: 8, 1961: 6, 1962: 7, 1963: 10, 1964: 2, 1965: 10, 1966: 1, 1967: 2, 1968: 4, 1969: 20, 1970: 5, 1971: 7, 1972: 65, 1973: 5, 1974: 7, 1975: 1, 1976: 2, 1977: 2, 1978: 22, 1979: 5, 1980: 136, 1981: 20, 1982: 17, 1983: 10, 1984: 1, 1985: 17, 1986: 3, 1987: 2, 1988: 11, 1989: 18, 1990: 10, 1991: 6, 1992: 5, 1993: 15, 1994: 8, 1995: 2, 1996: 1, 1997: 6, 1998: 3, 1999: 1, 2000: 1, 2001: 7, 2002: 7, 2003: 1, 2004: 43, 2005: 3, 2006: 1, 2007: 27, 2008: 10, 2009: 6, 2010: 10, 2011: 1, 2012: 1, 2013: 16, 2014: 3, 2015: 6, 2016: 2, 2017: 4, 2018: 1, 2019: 1, 2020: 1, 2021: 3, 2022: 1, 2023: 6, 2024: 1, 2025: 1, 2026: 1, 2027: 1, 2028: 2, 2029: 1, 2030: 1, 2031: 3, 2032: 3, 2033: 1, 2034: 2, 2035: 1, 2036: 5, 2037: 1, 2038: 4, 2039: 3, 2040: 1, 2041: 1, 2042: 4, 2043: 8, 2044: 2, 2045: 1, 2046: 2, 2047: 3, 2048: 2, 2049: 4, 2050: 3, 2051: 1, 2052: 1, 2053: 2, 2054: 2, 2055: 2, 2056: 2, 2057: 1, 2058: 1, 2059: 1, 2060: 1, 2061: 7, 2062: 1, 2063: 5, 2064: 1, 2065: 3, 2066: 1, 2067: 1, 2068: 2, 2069: 4, 2070: 1, 2071: 21, 2072: 2, 2073: 2, 2074: 1, 2075: 125, 2076: 91, 2077: 44, 2078: 43, 2079: 80, 2080: 16, 2081: 1, 2082: 7, 2083: 8, 2084: 3, 2085: 2, 2086: 5, 2087: 3, 2088: 4, 2089: 21, 2090: 2, 2091: 6, 2092: 1, 2093: 61, 2094: 3, 2095: 2, 2096: 20, 2097: 1, 2098: 6, 2099: 2, 2100: 3, 2101: 1, 2102: 2, 2103: 11, 2104: 14, 2105: 6, 2106: 1, 2107: 2, 2108: 5, 2109: 2, 2110: 1, 2111: 3, 2112: 4, 2113: 37, 2114: 2, 2115: 3, 2116: 2, 2117: 4, 2118: 3, 2119: 12, 2120: 4, 2121: 2, 2122: 3, 2123: 1, 2124: 3, 2125: 1, 2126: 4, 2127: 12, 2128: 17, 2129: 3, 2130: 2, 2131: 2, 2132: 14, 2133: 2, 2134: 1, 2135: 1, 2136: 3, 2137: 3, 2138: 7, 2139: 14, 2140: 1, 2141: 12, 2142: 17, 2143: 2, 2144: 7, 2145: 1, 2146: 16, 2147: 7, 2148: 4, 2149: 41, 2150: 3, 2151: 13, 2152: 1, 2153: 7, 2154: 3, 2155: 2, 2156: 2, 2157: 1, 2158: 6, 2159: 2, 2160: 2, 2161: 3, 2162: 2, 2163: 4, 2164: 1, 2165: 1, 2166: 8, 2167: 9, 2168: 1, 2169: 7, 2170: 6, 2171: 4, 2172: 1, 2173: 2, 2174: 3, 2175: 2, 2176: 1, 2177: 1, 2178: 3, 2179: 5, 2180: 15, 2181: 1, 2182: 2, 2183: 3, 2184: 5, 2185: 6, 2186: 2, 2187: 1, 2188: 1, 2189: 1, 2190: 1, 2191: 2, 2192: 2, 2193: 2, 2194: 2, 2195: 2, 2196: 5, 2197: 2, 2198: 1, 2199: 2, 2200: 1, 2201: 2, 2202: 6, 2203: 1, 2204: 1, 2205: 10, 2206: 13, 2207: 5, 2208: 10, 2209: 8, 2210: 1, 2211: 2, 2212: 2, 2213: 5, 2214: 1, 2215: 1, 2216: 1, 2217: 2, 2218: 3, 2219: 1, 2220: 3, 2221: 2, 2222: 2, 2223: 1, 2224: 3, 2225: 3, 2226: 1, 2227: 1, 2228: 2, 2229: 1, 2230: 1, 2231: 1, 2232: 2, 2233: 1, 2234: 1, 2235: 2, 2236: 1, 2237: 1, 2238: 1, 2239: 6, 2240: 3, 2241: 1, 2242: 2, 2243: 3, 2244: 3, 2245: 3, 2246: 1, 2247: 1, 2248: 1, 2249: 13, 2250: 9, 2251: 2, 2252: 5, 2253: 8, 2254: 2, 2255: 1, 2256: 2, 2257: 1, 2258: 5, 2259: 5, 2260: 3, 2261: 1, 2262: 4, 2263: 5, 2264: 2, 2265: 1, 2266: 6, 2267: 4, 2268: 1, 2269: 3, 2270: 2, 2271: 1, 2272: 1, 2273: 1, 2274: 1, 2275: 3, 2276: 1, 2277: 1, 2278: 3, 2279: 1, 2280: 3, 2281: 2, 2282: 3, 2283: 2, 2284: 1, 2285: 5, 2286: 3, 2287: 1, 2288: 1, 2289: 2, 2290: 2, 2291: 85, 2292: 21, 2293: 1, 2294: 1, 2295: 29, 2296: 9, 2297: 2, 2298: 8, 2299: 3, 2300: 59, 2301: 3, 2302: 3, 2303: 2, 2304: 2, 2305: 1, 2306: 3, 2307: 1, 2308: 2, 2309: 1, 2310: 3, 2311: 1, 2312: 1, 2313: 3, 2314: 7, 2315: 3, 2316: 2, 2317: 2, 2318: 2, 2319: 4, 2320: 1, 2321: 1, 2322: 2, 2323: 1, 2324: 4, 2325: 1, 2326: 5, 2327: 4, 2328: 14, 2329: 27, 2330: 3, 2331: 1, 2332: 4, 2333: 1, 2334: 4, 2335: 1, 2336: 1, 2337: 1, 2338: 2, 2339: 2, 2340: 2, 2341: 1, 2342: 4, 2343: 2, 2344: 3, 2345: 1, 2346: 1, 2347: 4, 2348: 9, 2349: 3, 2350: 1, 2351: 1, 2352: 1, 2353: 1, 2354: 1, 2355: 5, 2356: 1, 2357: 2, 2358: 1, 2359: 1, 2360: 3, 2361: 12, 2362: 1, 2363: 1, 2364: 2, 2365: 10, 2366: 7, 2367: 1, 2368: 1, 2369: 3, 2370: 1, 2371: 1, 2372: 1, 2373: 2, 2374: 1, 2375: 1, 2376: 2, 2377: 4, 2378: 2, 2379: 3, 2380: 2, 2381: 2, 2382: 1, 2383: 1, 2384: 4, 2385: 1, 2386: 1, 2387: 9, 2388: 3, 2389: 5, 2390: 5, 2391: 1, 2392: 1, 2393: 1, 2394: 3, 2395: 9, 2396: 1, 2397: 2, 2398: 2, 2399: 1, 2400: 3, 2401: 9, 2402: 1, 2403: 5, 2404: 2, 2405: 1, 2406: 3, 2407: 2, 2408: 1, 2409: 1, 2410: 3, 2411: 3, 2412: 3, 2413: 1, 2414: 2, 2415: 1, 2416: 1, 2417: 1, 2418: 1, 2419: 5, 2420: 2, 2421: 1, 2422: 1, 2423: 1, 2424: 1, 2425: 1, 2426: 2, 2427: 1, 2428: 1, 2429: 1, 2430: 1, 2431: 1, 2432: 1, 2433: 1, 2434: 1, 2435: 2, 2436: 2, 2437: 8, 2438: 12, 2439: 3, 2440: 1, 2441: 1, 2442: 19, 2443: 25, 2444: 40, 2445: 22, 2446: 5, 2447: 1, 2448: 31, 2449: 13, 2450: 6, 2451: 1, 2452: 2, 2453: 1, 2454: 16, 2455: 3, 2456: 8, 2457: 3, 2458: 1, 2459: 1, 2460: 9, 2461: 3, 2462: 1, 2463: 2, 2464: 2, 2465: 2, 2466: 3, 2467: 6, 2468: 3, 2469: 6, 2470: 5, 2471: 3, 2472: 5, 2473: 4, 2474: 9, 2475: 2, 2476: 1, 2477: 3, 2478: 2, 2479: 25, 2480: 1, 2481: 2, 2482: 2, 2483: 1, 2484: 3, 2485: 2, 2486: 5, 2487: 3, 2488: 1, 2489: 2, 2490: 1, 2491: 4, 2492: 2, 2493: 2, 2494: 1, 2495: 1, 2496: 1, 2497: 2, 2498: 3, 2499: 3, 2500: 1, 2501: 4, 2502: 1, 2503: 1, 2504: 1, 2505: 1, 2506: 41, 2507: 28, 2508: 3, 2509: 1, 2510: 2, 2511: 6, 2512: 1, 2513: 1, 2514: 34, 2515: 3, 2516: 4, 2517: 1, 2518: 15, 2519: 1, 2520: 1, 2521: 37, 2522: 1, 2523: 1, 2524: 3, 2525: 12, 2526: 3, 2527: 3, 2528: 1, 2529: 4, 2530: 5, 2531: 2, 2532: 1, 2533: 2, 2534: 1, 2535: 4, 2536: 4, 2537: 7, 2538: 7, 2539: 7, 2540: 58, 2541: 2, 2542: 52, 2543: 10, 2544: 3, 2545: 11, 2546: 44, 2547: 11, 2548: 4, 2549: 1, 2550: 3, 2551: 28, 2552: 4, 2553: 5, 2554: 2, 2555: 1, 2556: 12, 2557: 22, 2558: 10, 2559: 5, 2560: 3, 2561: 5, 2562: 1, 2563: 2, 2564: 5, 2565: 1, 2566: 29, 2567: 3, 2568: 8, 2569: 1, 2570: 12, 2571: 8, 2572: 21, 2573: 14, 2574: 1, 2575: 10, 2576: 15, 2577: 6, 2578: 4, 2579: 2, 2580: 4, 2581: 5, 2582: 1, 2583: 4, 2584: 10, 2585: 2, 2586: 2, 2587: 1, 2588: 2, 2589: 4, 2590: 3, 2591: 1, 2592: 4, 2593: 1, 2594: 1, 2595: 1, 2596: 18, 2597: 5, 2598: 1, 2599: 6, 2600: 4, 2601: 7, 2602: 5, 2603: 2, 2604: 2, 2605: 3, 2606: 1, 2607: 3, 2608: 2, 2609: 1, 2610: 1, 2611: 1, 2612: 1, 2613: 1, 2614: 4, 2615: 1, 2616: 7, 2617: 8, 2618: 4, 2619: 3, 2620: 1, 2621: 2, 2622: 1, 2623: 1, 2624: 5, 2625: 1, 2626: 4, 2627: 1, 2628: 1, 2629: 3, 2630: 1, 2631: 1, 2632: 2, 2633: 5, 2634: 7, 2635: 11, 2636: 21, 2637: 1, 2638: 1, 2639: 2, 2640: 5, 2641: 3, 2642: 2, 2643: 2, 2644: 2, 2645: 8, 2646: 1, 2647: 2, 2648: 1, 2649: 8, 2650: 2, 2651: 5, 2652: 1, 2653: 1, 2654: 2, 2655: 1, 2656: 2, 2657: 44, 2658: 8, 2659: 9, 2660: 44, 2661: 1, 2662: 2, 2663: 7, 2664: 6, 2665: 3, 2666: 9, 2667: 12, 2668: 14, 2669: 7, 2670: 3, 2671: 8, 2672: 3, 2673: 11, 2674: 2, 2675: 2, 2676: 2, 2677: 2, 2678: 13, 2679: 8, 2680: 2, 2681: 1, 2682: 3, 2683: 2, 2684: 2, 2685: 1, 2686: 2, 2687: 11, 2688: 1, 2689: 2, 2690: 3, 2691: 8, 2692: 2, 2693: 1, 2694: 1, 2695: 37, 2696: 6, 2697: 8, 2698: 5, 2699: 3, 2700: 11, 2701: 16, 2702: 9, 2703: 9, 2704: 9, 2705: 3, 2706: 2, 2707: 5, 2708: 6, 2709: 9, 2710: 3, 2711: 2, 2712: 1, 2713: 2, 2714: 2, 2715: 1, 2716: 8, 2717: 2, 2718: 5, 2719: 1, 2720: 5, 2721: 1, 2722: 1, 2723: 2, 2724: 11, 2725: 2, 2726: 1, 2727: 3, 2728: 5, 2729: 1, 2730: 2, 2731: 1, 2732: 4, 2733: 2, 2734: 6, 2735: 6, 2736: 2, 2737: 2, 2738: 1, 2739: 2, 2740: 20, 2741: 3, 2742: 6, 2743: 10, 2744: 4, 2745: 4, 2746: 1, 2747: 1, 2748: 3, 2749: 2, 2750: 1, 2751: 1, 2752: 4, 2753: 1, 2754: 1, 2755: 2, 2756: 4, 2757: 3, 2758: 3, 2759: 1, 2760: 1, 2761: 1, 2762: 6, 2763: 2, 2764: 1, 2765: 1, 2766: 2, 2767: 2, 2768: 1, 2769: 18, 2770: 4, 2771: 10, 2772: 2, 2773: 3, 2774: 1, 2775: 3, 2776: 2, 2777: 2, 2778: 8, 2779: 10, 2780: 14, 2781: 2, 2782: 1, 2783: 1, 2784: 3, 2785: 1, 2786: 1, 2787: 7, 2788: 1, 2789: 1, 2790: 1, 2791: 5, 2792: 4, 2793: 4, 2794: 1, 2795: 1, 2796: 2, 2797: 1, 2798: 1, 2799: 19, 2800: 2, 2801: 5, 2802: 14, 2803: 4, 2804: 7, 2805: 3, 2806: 1, 2807: 1, 2808: 22, 2809: 4, 2810: 5, 2811: 2, 2812: 3, 2813: 11, 2814: 1, 2815: 2, 2816: 2, 2817: 1, 2818: 2, 2819: 1, 2820: 1, 2821: 1, 2822: 1, 2823: 4, 2824: 2, 2825: 1, 2826: 4, 2827: 1, 2828: 5, 2829: 5, 2830: 2, 2831: 7, 2832: 2, 2833: 1, 2834: 2, 2835: 1, 2836: 3, 2837: 3, 2838: 2, 2839: 1, 2840: 3, 2841: 2, 2842: 2, 2843: 2, 2844: 3, 2845: 1, 2846: 5, 2847: 2, 2848: 4, 2849: 5, 2850: 3, 2851: 4, 2852: 4, 2853: 5, 2854: 3, 2855: 2, 2856: 13, 2857: 2, 2858: 2, 2859: 1, 2860: 1, 2861: 4, 2862: 2, 2863: 5, 2864: 3, 2865: 3, 2866: 42, 2867: 43, 2868: 3, 2869: 1, 2870: 1, 2871: 4, 2872: 5, 2873: 2, 2874: 2, 2875: 7, 2876: 2, 2877: 1, 2878: 3, 2879: 1, 2880: 2, 2881: 1, 2882: 1, 2883: 1, 2884: 3, 2885: 1, 2886: 2, 2887: 2, 2888: 1, 2889: 4, 2890: 1, 2891: 2, 2892: 1, 2893: 3, 2894: 4, 2895: 1, 2896: 1, 2897: 1, 2898: 9, 2899: 9, 2900: 1, 2901: 14, 2902: 3, 2903: 1, 2904: 2, 2905: 3, 2906: 2, 2907: 2, 2908: 2, 2909: 10, 2910: 28, 2911: 27, 2912: 5, 2913: 6, 2914: 11, 2915: 11, 2916: 6, 2917: 1, 2918: 18, 2919: 9, 2920: 2, 2921: 6, 2922: 16, 2923: 9, 2924: 10, 2925: 1, 2926: 1, 2927: 1, 2928: 5, 2929: 1, 2930: 2, 2931: 1, 2932: 1, 2933: 1, 2934: 1, 2935: 3, 2936: 6, 2937: 6, 2938: 2, 2939: 6, 2940: 3, 2941: 6, 2942: 3, 2943: 2, 2944: 3, 2945: 1, 2946: 2, 2947: 11, 2948: 3, 2949: 2, 2950: 1, 2951: 1, 2952: 1, 2953: 1, 2954: 3, 2955: 2, 2956: 4, 2957: 6, 2958: 2, 2959: 3, 2960: 1, 2961: 7, 2962: 1, 2963: 2, 2964: 4, 2965: 2, 2966: 1, 2967: 23, 2968: 19, 2969: 18, 2970: 11, 2971: 1, 2972: 2, 2973: 3, 2974: 1, 2975: 1, 2976: 2, 2977: 1, 2978: 1, 2979: 1, 2980: 6, 2981: 2, 2982: 1, 2983: 3, 2984: 1, 2985: 1, 2986: 9, 2987: 3, 2988: 3, 2989: 3, 2990: 2, 2991: 1, 2992: 1, 2993: 1, 2994: 2, 2995: 1, 2996: 1, 2997: 1, 2998: 5, 2999: 1, 3000: 8, 3001: 2, 3002: 1, 3003: 1, 3004: 1, 3005: 1, 3006: 2, 3007: 1, 3008: 9, 3009: 1, 3010: 1, 3011: 4, 3012: 2, 3013: 2, 3014: 1, 3015: 1, 3016: 3, 3017: 3, 3018: 2, 3019: 1, 3020: 1, 3021: 2, 3022: 2, 3023: 1, 3024: 2, 3025: 2, 3026: 3, 3027: 1, 3028: 2, 3029: 2, 3030: 5, 3031: 1, 3032: 1, 3033: 2, 3034: 1, 3035: 4, 3036: 1, 3037: 2, 3038: 1, 3039: 9, 3040: 2, 3041: 10, 3042: 8, 3043: 3, 3044: 2, 3045: 1, 3046: 1, 3047: 1, 3048: 1, 3049: 1, 3050: 1, 3051: 1, 3052: 1, 3053: 1, 3054: 1, 3055: 2, 3056: 3, 3057: 1, 3058: 5, 3059: 4, 3060: 12, 3061: 4, 3062: 1, 3063: 2, 3064: 4, 3065: 1, 3066: 2, 3067: 5, 3068: 2, 3069: 4, 3070: 3, 3071: 2, 3072: 1, 3073: 3, 3074: 1, 3075: 2, 3076: 2, 3077: 3, 3078: 3, 3079: 1, 3080: 4, 3081: 1, 3082: 1, 3083: 3, 3084: 7, 3085: 6, 3086: 4, 3087: 2, 3088: 7, 3089: 4, 3090: 2, 3091: 4, 3092: 2, 3093: 3, 3094: 4, 3095: 3, 3096: 3, 3097: 1, 3098: 1, 3099: 1, 3100: 3, 3101: 1, 3102: 1, 3103: 2, 3104: 1, 3105: 1, 3106: 2, 3107: 1, 3108: 1, 3109: 15, 3110: 19, 3111: 49, 3112: 20, 3113: 1, 3114: 3, 3115: 4, 3116: 3, 3117: 2, 3118: 3, 3119: 2, 3120: 5, 3121: 2, 3122: 1, 3123: 2, 3124: 1, 3125: 6, 3126: 1, 3127: 1, 3128: 3, 3129: 1, 3130: 2, 3131: 1, 3132: 1, 3133: 7, 3134: 1, 3135: 1, 3136: 1, 3137: 1, 3138: 2, 3139: 2, 3140: 6, 3141: 1, 3142: 2, 3143: 1, 3144: 1, 3145: 2, 3146: 2, 3147: 2, 3148: 3, 3149: 1, 3150: 2, 3151: 2, 3152: 1, 3153: 1, 3154: 1, 3155: 1, 3156: 1, 3157: 3, 3158: 1, 3159: 2, 3160: 5, 3161: 1, 3162: 1, 3163: 8, 3164: 9, 3165: 3, 3166: 1, 3167: 1, 3168: 5, 3169: 1, 3170: 2, 3171: 5, 3172: 2, 3173: 6, 3174: 7, 3175: 2, 3176: 2, 3177: 1, 3178: 1, 3179: 1, 3180: 1, 3181: 24, 3182: 2, 3183: 6, 3184: 6, 3185: 3, 3186: 1, 3187: 1, 3188: 2, 3189: 4, 3190: 6, 3191: 2, 3192: 4, 3193: 2, 3194: 1, 3195: 2, 3196: 1, 3197: 1, 3198: 2, 3199: 2, 3200: 1, 3201: 2, 3202: 1, 3203: 1, 3204: 1, 3205: 1, 3206: 1, 3207: 1, 3208: 1, 3209: 1, 3210: 3, 3211: 2, 3212: 2, 3213: 21, 3214: 3, 3215: 1, 3216: 1, 3217: 1, 3218: 1, 3219: 5, 3220: 1, 3221: 4, 3222: 1, 3223: 1, 3224: 1, 3225: 2, 3226: 1, 3227: 1, 3228: 1}\n",
      "3229 unique labels are used\n",
      "0 unique labels never used\n",
      "949 unique labels used once\n",
      "575 unique labels used twice\n"
     ]
    }
   ],
   "source": [
    "print(f\"total number of steps is:\", step_count)\n",
    "print(f\"highest frequency label is {max_freq_index} and occurs {max} times\")\n",
    "print(f\"final label used is {len(label_count)-1}\")\n",
    "print(label_count)\n",
    "print(len(label_count),\"unique labels are used\")\n",
    "print(labels_never_used,\"unique labels never used\")\n",
    "print(labels_used_once, \"unique labels used once\")\n",
    "print(labels_used_twice, \"unique labels used twice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make array of label frequencies for sklearn compute_class_weight using entire dataset\n",
    "# really should be doing this for train set (otherwise, data leakage...)\n",
    "# however, train set may not include certain labels, which leads to error in compute_class_weight\n",
    "\n",
    "# make array of unique classes\n",
    "class_num_arr = [i for i in range(len(label_count))]\n",
    "class_num_arr = np.array(class_num_arr)\n",
    "\n",
    "# make array of all data points with labels\n",
    "lbl_arr = np.array([])\n",
    "for i in range(file_limit):\n",
    "    for y in pf_data.get(i).y:\n",
    "        lbl_arr = np.append(lbl_arr,[y.numpy()],axis=0).astype(int)\n",
    "\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\",classes = class_num_arr, y=lbl_arr)\n",
    "class_weights = torch.from_numpy(class_weights).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make class for GCN model\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    \"\"\"GCN\"\"\"\n",
    "    def __init__(self, dim_h):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(pf_data.num_features, dim_h)\n",
    "        self.conv2 = GCNConv(dim_h, dim_h)\n",
    "        self.conv3 = GCNConv(dim_h, dim_h)\n",
    "        self.conv4 = GCNConv(dim_h, dim_h)\n",
    "        self.lin = Linear(dim_h, len(class_num_arr))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = self.conv1(x, edge_index)\n",
    "        h = h.relu()\n",
    "        h = F.dropout(h, p=0.1, training=self.training)\n",
    "        h = self.conv2(h, edge_index)        \n",
    "        h = h.relu()\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        h = self.lin(h)\n",
    "        \n",
    "        return F.log_softmax(h, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCN model training\n",
    "\n",
    "def train(model, loader):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    # commented out code is to use class weights to account for imbalanced dataset\n",
    "    #criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    epochs = 1000\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs+1):\n",
    "        total_loss = 0\n",
    "        acc = 0\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "        cur_graph = 0   # used to keep track of current statement to enforce preds of only PREVIOUS labels in training\n",
    "\n",
    "        # Train on batches\n",
    "        for data in loader:\n",
    "            cur_graph_batch = data.batch + cur_graph            \n",
    "            cur_graph += torch.max(data.batch) + 1\n",
    "            data = data.to(device, non_blocking=True)\n",
    "            data.y = data.y.to(torch.float).to(device, non_blocking=True)\n",
    "            optimizer.zero_grad()\n",
    "            length = len(loader)\n",
    "            out = model(data.x, data.edge_index.long())\n",
    "            data.y = data.y.type(torch.LongTensor).to(device, non_blocking=True)\n",
    "            out = out.type(torch.float32).to(device, non_blocking=True)\n",
    "            loss = criterion(out, data.y)\n",
    "            total_loss += loss / length\n",
    "\n",
    "            # commented out code below is meant to enforce predictions to only come from previous theorems\n",
    "            # for dict to be properly created, you must delete and recreate test.pt by rerunning \n",
    "            # pf_data = ProofDataset(root=\"data/\",file_limit=file_limit)\n",
    "            \n",
    "            #dict = pf_data.class_corr\n",
    "            #dict_keys = [k for k in dict.keys()]            \n",
    "\n",
    "            #def return_next_lowest_idx(num):\n",
    "                #if num in dict_keys:\n",
    "                    #return dict[num]\n",
    "                #while num not in dict_keys:\n",
    "                    #try:\n",
    "                        #num -= 1\n",
    "                        #return dict[num]\n",
    "                    #except:\n",
    "                        #pass\n",
    "\n",
    "            #with torch.no_grad():\n",
    "                #cur_graph_batch.apply_(return_next_lowest_idx)\n",
    "                #masked_lbls = (torch.arange(out.size(1)) < (cur_graph_batch[..., None]+1))*(out.cpu())\n",
    "                #try:\n",
    "                    #masked_lbls = (torch.arange(out.size(1)) < (cur_graph_batch[..., None]+1))*(out.cpu())\n",
    "                    #masked_lbls = torch.where(masked_lbls==0,np.nan,masked_lbls)\n",
    "                    #masked_lbls = masked_lbls.detach().numpy()                \n",
    "                    #pred = np.nanargmax(masked_lbls,axis=1)\n",
    "                    #pred = torch.from_numpy(pred)\n",
    "                    #acc += accuracy(pred, data.y.cpu()) / length\n",
    "                #except Exception as e:\n",
    "                    #print(\"a lil error\")\n",
    "                    #out = out + .00000001\n",
    "                    #masked_lbls = (torch.arange(out.size(1)) < (cur_graph_batch[..., None]+1))*(out.cpu())\n",
    "                    #masked_lbls = torch.where(masked_lbls==0,np.nan,masked_lbls)\n",
    "                    #masked_lbls = masked_lbls.detach().numpy()\n",
    "                    #pred = np.nanargmax(masked_lbls,axis=1)\n",
    "                    #pred = torch.from_numpy(pred)\n",
    "                    #acc += accuracy(pred, data.y.cpu()) / length\n",
    "                    #out = out - .00000001\n",
    "\n",
    "            #comment out the follow pred and acc lines if enforcing predictions as described above\n",
    "            pred = out.argmax(dim=1)\n",
    "            acc += accuracy(pred, data.y) / length\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # run model on validation set\n",
    "            val_loss, val_acc, val_f1 = test(model, val_loader)\n",
    "\n",
    "        # Print metrics every epoch\n",
    "        if(epoch % 10 == 0):\n",
    "            print(f'Epoch {epoch:>3} | Train Loss: {total_loss:.2f} | Train Acc: {acc*100:>5.2f}% | Val Loss: {val_loss:.2f} | Val Acc: {val_acc*100:.2f}% | F Score: {val_f1:.2f}')\n",
    "            \n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, loader):\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    fscore = 0\n",
    "    \n",
    "    for data in loader:\n",
    "        data = data.to(device, non_blocking=True)\n",
    "        length = len(loader)\n",
    "        out = model(data.x, data.edge_index.long())\n",
    "        data.y = data.y.type(torch.LongTensor).to(device, non_blocking=True)\n",
    "        loss += criterion(out, data.y) / length\n",
    "        pred = out.argmax(dim=1)\n",
    "        acc += accuracy(pred, data.y) / length\n",
    "        fscore += f1(pred.cpu(), data.y.cpu(), average='macro')    # micro looks better, but macro prob more accurate\n",
    "\n",
    "    return loss, acc, fscore\n",
    "\n",
    "def accuracy(pred_y, y):\n",
    "    \"\"\"Calculate accuracy.\"\"\"\n",
    "    return ((pred_y == y).sum() / len(y)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (conv1): GCNConv(512, 800)\n",
       "  (conv2): GCNConv(800, 800)\n",
       "  (conv3): GCNConv(800, 800)\n",
       "  (conv4): GCNConv(800, 800)\n",
       "  (lin): Linear(in_features=800, out_features=3229, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize (and reset weights of) model\n",
    "# dim_h is hyperparameter of number of hidden layers\n",
    "\n",
    "gcn_trained = None\n",
    "gcn = None\n",
    "gcn = GCN(dim_h=800).to(device)\n",
    "gcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset weights and train model\n",
    "gcn_trained = None\n",
    "gcn_trained = train(gcn, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 6.50 | Test Acc: 42.86% | F Score: 0.08\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc, test_f1 = test(gcn_trained, test_loader)\n",
    "print(f'Test Loss: {test_loss:.2f} | Test Acc: {test_acc*100:.2f}% | F Score: {test_f1:.2f}')\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
