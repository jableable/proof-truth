{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from use_dataset import ProofDataset, HiddenConcProofDataset\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Batch, Data, Dataset\n",
    "from torch_geometric.loader import DataLoader, NeighborLoader\n",
    "import random\n",
    "from sklearn.metrics import f1_score as f1\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, ReLU, Dropout, BatchNorm1d, Sequential\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, GINConv, global_add_pool, PairNorm\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use file_limit=5000 to only load and verify the first 5000 graphs (~60 MB)\n",
    "\n",
    "file_limit = 10000    # desired number of graphs to work with\n",
    "vocab_size = 1598   # number of characters in our vocabulary\n",
    "\n",
    "pf_data = ProofDataset(root=\"data/\",read_name=\"10000_relabeled_data_at_least_5.json\" , write_name=\"10000_relabeled_data_at_least_5_w_stmts.pt\" ,file_limit=file_limit)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make train/val/test for GIN\n",
    "\n",
    "# set seed for random # generation\n",
    "random.seed(10)\n",
    "length = file_limit\n",
    "total_indices = [i+file_limit for i in range(file_limit)]\n",
    "\n",
    "# create index vectors to filter dataset\n",
    "train_indices = [i for i in range(file_limit)]\n",
    "train_indices = train_indices + random.sample(total_indices, int(length*.8))\n",
    "train_indices.sort()\n",
    "\n",
    "val_index_options = [x for x in total_indices if x not in train_indices]\n",
    "val_indices = random.sample(val_index_options, int(length*.1))\n",
    "val_indices.sort()\n",
    "\n",
    "test_index_options = [x for x in total_indices if x not in train_indices if x not in val_indices]\n",
    "test_indices = random.sample(test_index_options, int(length*.1))\n",
    "test_indices.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our model is judged on how well it can predict the label for the final/conclusion node as seen in the val set created above\n",
    "# to not give our model too much info, we zero out the features on the final/conclusion node for each graph in the val set\n",
    "\n",
    "data_list = []\n",
    "\n",
    "for idx, graph in enumerate(pf_data):\n",
    "    # inherit features, labels, and edges\n",
    "    x = graph.x.clone()\n",
    "    y = graph.y.clone()\n",
    "    edge_index = graph.edge_index.clone()\n",
    "    x[-1] = torch.zeros(512) # zero out ALL conclusion nodes\n",
    "\n",
    "    #if idx in val_indices:\n",
    "        #x[-1] = torch.zeros(512)    # zero out features of final/conclusion node for each graph in val set\n",
    "\n",
    "# replace features of conlusion/final nodes from val set with average of neighboring node embeddings\n",
    "    #if idx in val_indices:\n",
    "\n",
    "        #connected_nodes = []\n",
    "        #sum = torch.zeros(512)\n",
    "\n",
    "        #for i, edge in enumerate(edge_index[0]): # case of outgoing edge coming from conc node\n",
    "            #if edge == (pf_data[idx].num_nodes-1):  \n",
    "                #connected_nodes.append(int(edge_index[1][i].item()))\n",
    "        #for i, edge in enumerate(edge_index[1]): # case of outgoing edge going to conc node\n",
    "            #if edge == (pf_data[idx].num_nodes-1):\n",
    "                #connected_nodes.append(int(edge_index[0][i].item()))\n",
    "\n",
    "        #for i in connected_nodes:\n",
    "            #if not torch.equal(pf_data[idx].x[i], pf_data[idx].x[-1]):\n",
    "            #sum += pf_data[idx].x[i]\n",
    "        #if len(connected_nodes) > 0:\n",
    "            #x[-1] = sum/(len(connected_nodes))\n",
    "        #else:\n",
    "            #x[-1] = sum\n",
    "\n",
    "\n",
    "    data_list.append(Data(x=x,y=y,edge_index=edge_index))\n",
    "\n",
    "hidden_conc_pf_data = HiddenConcProofDataset(root=\"data/\",read_name=\"5000_relabeled_data.json\",write_name=\"overwritten_labels\", data_list=data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set   = 9000 graphs\n",
      "Validation set = 500 graphs\n",
      "Test set       = 500 graphs\n"
     ]
    }
   ],
   "source": [
    "# Create training, validation, and test sets\n",
    "train_dataset = hidden_conc_pf_data[train_indices]\n",
    "val_dataset = hidden_conc_pf_data[val_indices]\n",
    "test_dataset = hidden_conc_pf_data[test_indices]\n",
    "\n",
    "print(f'Training set   = {len(train_dataset)} graphs')\n",
    "print(f'Validation set = {len(val_dataset)} graphs')\n",
    "print(f'Test set       = {len(test_dataset)} graphs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mini-batches\n",
    "# batch_size is number of graphs\n",
    "train_loader = DataLoader(train_dataset, batch_size=1000, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=500, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=500, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loader:\n",
      " - Batch 0: DataBatch(x=[3984, 512], edge_index=[2, 2984], y=[3984], batch=[3984], ptr=[1001])\n",
      " - Batch 1: DataBatch(x=[3876, 512], edge_index=[2, 2876], y=[3876], batch=[3876], ptr=[1001])\n",
      " - Batch 2: DataBatch(x=[4065, 512], edge_index=[2, 3065], y=[4065], batch=[4065], ptr=[1001])\n",
      " - Batch 3: DataBatch(x=[3737, 512], edge_index=[2, 2737], y=[3737], batch=[3737], ptr=[1001])\n",
      " - Batch 4: DataBatch(x=[3754, 512], edge_index=[2, 2754], y=[3754], batch=[3754], ptr=[1001])\n",
      " - Batch 5: DataBatch(x=[4068, 512], edge_index=[2, 3068], y=[4068], batch=[4068], ptr=[1001])\n",
      " - Batch 6: DataBatch(x=[3743, 512], edge_index=[2, 2743], y=[3743], batch=[3743], ptr=[1001])\n",
      " - Batch 7: DataBatch(x=[3860, 512], edge_index=[2, 2860], y=[3860], batch=[3860], ptr=[1001])\n",
      " - Batch 8: DataBatch(x=[3603, 512], edge_index=[2, 2603], y=[3603], batch=[3603], ptr=[1001])\n",
      "\n",
      "Validation loader:\n",
      " - Batch 0: DataBatch(x=[3207, 512], edge_index=[2, 2707], y=[3207], batch=[3207], ptr=[501])\n",
      "\n",
      "Test loader:\n",
      " - Batch 0: DataBatch(x=[3037, 512], edge_index=[2, 2537], y=[3037], batch=[3037], ptr=[501])\n"
     ]
    }
   ],
   "source": [
    "print('\\nTrain loader:')\n",
    "for i, batch in enumerate(train_loader):\n",
    "    print(f' - Batch {i}: {batch}')\n",
    "\n",
    "print('\\nValidation loader:')\n",
    "for i, batch in enumerate(val_loader):\n",
    "    print(f' - Batch {i}: {batch}')\n",
    "\n",
    "print('\\nTest loader:')\n",
    "for i, batch in enumerate(test_loader):\n",
    "    print(f' - Batch {i}: {batch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary to record label frequency\n",
    "\n",
    "# get max label used in hidden_conc_pf_data\n",
    "max_label = 0\n",
    "\n",
    "for i in range(file_limit*2):\n",
    "    for j in hidden_conc_pf_data.get(i).y:\n",
    "        if j > max_label:\n",
    "            max_label = j.to(int).item()\n",
    "\n",
    "# initialize histogram for labels used in hidden_conc_pf_data            \n",
    "label_count = {}\n",
    "\n",
    "for i in range(file_limit*2):\n",
    "    for j in range(max_label+1):        \n",
    "        label_count[j] = 0\n",
    "\n",
    "for i in range(file_limit*2):\n",
    "    for j in hidden_conc_pf_data.get(i).y:\n",
    "        label_count[j.to(int).item()] += 1\n",
    "\n",
    "step_count = 0\n",
    "max = 0\n",
    "max_freq_index = None   #find the most frequently used index\n",
    "labels_never_used = 0\n",
    "labels_used_once = 0\n",
    "labels_used_twice = 0\n",
    "\n",
    "\n",
    "for k,v in label_count.items():\n",
    "    step_count += v\n",
    "    if v > max:     \n",
    "        max = v\n",
    "        max_freq_index = k\n",
    "\n",
    "    if v == 0:\n",
    "        labels_never_used += 1\n",
    "    if v == 1:\n",
    "        labels_used_once += 1\n",
    "    if v ==2:\n",
    "        labels_used_twice += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of steps is: 40934\n",
      "highest frequency label is 554 and occurs 12143 times\n",
      "final label used is 554\n",
      "{0: 10280, 1: 545, 2: 52, 3: 24, 4: 270, 5: 14, 6: 40, 7: 42, 8: 390, 9: 26, 10: 28, 11: 245, 12: 23, 13: 57, 14: 12, 15: 21, 16: 108, 17: 47, 18: 75, 19: 14, 20: 18, 21: 26, 22: 17, 23: 13, 24: 18, 25: 17, 26: 12, 27: 15, 28: 50, 29: 18, 30: 13, 31: 14, 32: 12, 33: 42, 34: 12, 35: 20, 36: 13, 37: 14, 38: 18, 39: 22, 40: 29, 41: 83, 42: 25, 43: 12, 44: 127, 45: 60, 46: 27, 47: 83, 48: 130, 49: 100, 50: 24, 51: 24, 52: 54, 53: 115, 54: 29, 55: 25, 56: 38, 57: 59, 58: 78, 59: 59, 60: 23, 61: 29, 62: 125, 63: 41, 64: 44, 65: 11, 66: 17, 67: 12, 68: 32, 69: 15, 70: 19, 71: 21, 72: 25, 73: 19, 74: 20, 75: 33, 76: 52, 77: 25, 78: 13, 79: 13, 80: 26, 81: 16, 82: 17, 83: 474, 84: 39, 85: 74, 86: 172, 87: 73, 88: 20, 89: 20, 90: 67, 91: 21, 92: 61, 93: 24, 94: 21, 95: 28, 96: 45, 97: 107, 98: 15, 99: 16, 100: 39, 101: 242, 102: 37, 103: 12, 104: 113, 105: 24, 106: 11, 107: 60, 108: 14, 109: 41, 110: 11, 111: 13, 112: 32, 113: 22, 114: 49, 115: 17, 116: 11, 117: 13, 118: 51, 119: 27, 120: 13, 121: 16, 122: 69, 123: 25, 124: 49, 125: 14, 126: 11, 127: 27, 128: 13, 129: 12, 130: 21, 131: 16, 132: 22, 133: 18, 134: 144, 135: 36, 136: 29, 137: 193, 138: 39, 139: 26, 140: 12, 141: 45, 142: 38, 143: 62, 144: 11, 145: 14, 146: 32, 147: 16, 148: 11, 149: 22, 150: 22, 151: 110, 152: 122, 153: 95, 154: 72, 155: 12, 156: 13, 157: 15, 158: 12, 159: 20, 160: 78, 161: 26, 162: 17, 163: 26, 164: 13, 165: 11, 166: 13, 167: 30, 168: 14, 169: 28, 170: 19, 171: 43, 172: 24, 173: 20, 174: 20, 175: 17, 176: 11, 177: 26, 178: 18, 179: 82, 180: 63, 181: 66, 182: 44, 183: 48, 184: 92, 185: 13, 186: 17, 187: 12, 188: 16, 189: 30, 190: 16, 191: 14, 192: 24, 193: 13, 194: 13, 195: 12, 196: 13, 197: 14, 198: 18, 199: 43, 200: 11, 201: 18, 202: 59, 203: 17, 204: 22, 205: 17, 206: 12, 207: 39, 208: 28, 209: 42, 210: 14, 211: 12, 212: 26, 213: 11, 214: 41, 215: 37, 216: 13, 217: 25, 218: 22, 219: 18, 220: 11, 221: 25, 222: 57, 223: 11, 224: 24, 225: 14, 226: 12, 227: 18, 228: 14, 229: 18, 230: 19, 231: 44, 232: 44, 233: 47, 234: 17, 235: 16, 236: 22, 237: 17, 238: 21, 239: 24, 240: 23, 241: 26, 242: 44, 243: 11, 244: 33, 245: 35, 246: 25, 247: 13, 248: 15, 249: 11, 250: 27, 251: 12, 252: 11, 253: 19, 254: 11, 255: 46, 256: 43, 257: 11, 258: 30, 259: 21, 260: 19, 261: 18, 262: 38, 263: 34, 264: 39, 265: 20, 266: 26, 267: 34, 268: 18, 269: 27, 270: 61, 271: 24, 272: 12, 273: 157, 274: 18, 275: 11, 276: 25, 277: 18, 278: 30, 279: 22, 280: 129, 281: 24, 282: 35, 283: 29, 284: 40, 285: 19, 286: 19, 287: 19, 288: 16, 289: 11, 290: 15, 291: 55, 292: 18, 293: 85, 294: 16, 295: 49, 296: 14, 297: 322, 298: 12, 299: 19, 300: 18, 301: 50, 302: 33, 303: 37, 304: 26, 305: 35, 306: 16, 307: 14, 308: 19, 309: 14, 310: 27, 311: 21, 312: 13, 313: 18, 314: 13, 315: 24, 316: 11, 317: 21, 318: 24, 319: 17, 320: 13, 321: 24, 322: 12, 323: 15, 324: 34, 325: 12, 326: 32, 327: 22, 328: 17, 329: 13, 330: 12, 331: 42, 332: 30, 333: 50, 334: 43, 335: 18, 336: 22, 337: 32, 338: 48, 339: 39, 340: 31, 341: 15, 342: 15, 343: 15, 344: 11, 345: 33, 346: 13, 347: 20, 348: 16, 349: 41, 350: 15, 351: 19, 352: 11, 353: 11, 354: 18, 355: 34, 356: 21, 357: 12, 358: 24, 359: 19, 360: 16, 361: 12, 362: 25, 363: 16, 364: 17, 365: 13, 366: 25, 367: 17, 368: 20, 369: 13, 370: 33, 371: 12, 372: 12, 373: 14, 374: 28, 375: 13, 376: 51, 377: 26, 378: 11, 379: 25, 380: 35, 381: 21, 382: 23, 383: 17, 384: 37, 385: 55, 386: 22, 387: 76, 388: 27, 389: 13, 390: 27, 391: 34, 392: 38, 393: 21, 394: 14, 395: 20, 396: 78, 397: 27, 398: 57, 399: 18, 400: 11, 401: 54, 402: 33, 403: 27, 404: 34, 405: 34, 406: 15, 407: 54, 408: 28, 409: 49, 410: 17, 411: 28, 412: 71, 413: 13, 414: 45, 415: 96, 416: 49, 417: 25, 418: 18, 419: 16, 420: 11, 421: 11, 422: 20, 423: 66, 424: 23, 425: 137, 426: 21, 427: 18, 428: 11, 429: 18, 430: 12, 431: 19, 432: 11, 433: 16, 434: 43, 435: 28, 436: 11, 437: 11, 438: 17, 439: 21, 440: 125, 441: 91, 442: 44, 443: 43, 444: 80, 445: 17, 446: 22, 447: 62, 448: 21, 449: 12, 450: 15, 451: 38, 452: 13, 453: 13, 454: 18, 455: 15, 456: 15, 457: 13, 458: 18, 459: 17, 460: 42, 461: 14, 462: 16, 463: 11, 464: 14, 465: 11, 466: 14, 467: 86, 468: 22, 469: 30, 470: 60, 471: 15, 472: 28, 473: 13, 474: 11, 475: 13, 476: 19, 477: 26, 478: 41, 479: 23, 480: 32, 481: 14, 482: 17, 483: 26, 484: 41, 485: 29, 486: 35, 487: 16, 488: 38, 489: 13, 490: 59, 491: 53, 492: 11, 493: 45, 494: 12, 495: 29, 496: 13, 497: 23, 498: 11, 499: 30, 500: 13, 501: 22, 502: 15, 503: 11, 504: 16, 505: 11, 506: 19, 507: 12, 508: 22, 509: 45, 510: 45, 511: 13, 512: 15, 513: 12, 514: 14, 515: 12, 516: 38, 517: 12, 518: 17, 519: 11, 520: 21, 521: 11, 522: 19, 523: 11, 524: 11, 525: 15, 526: 20, 527: 15, 528: 23, 529: 12, 530: 14, 531: 43, 532: 44, 533: 15, 534: 28, 535: 27, 536: 12, 537: 12, 538: 19, 539: 17, 540: 11, 541: 12, 542: 24, 543: 20, 544: 19, 545: 12, 546: 11, 547: 13, 548: 15, 549: 19, 550: 50, 551: 21, 552: 24, 553: 21, 554: 12143}\n",
      "555 unique labels are used\n",
      "0 unique labels never used\n",
      "0 unique labels used once\n",
      "0 unique labels used twice\n"
     ]
    }
   ],
   "source": [
    "print(f\"total number of steps is:\", step_count)\n",
    "print(f\"highest frequency label is {max_freq_index} and occurs {max} times\")\n",
    "print(f\"final label used is {len(label_count)-1}\")\n",
    "print(label_count)\n",
    "print(len(label_count),\"unique labels are used\")\n",
    "print(labels_never_used,\"unique labels never used\")\n",
    "print(labels_used_once, \"unique labels used once\")\n",
    "print(labels_used_twice, \"unique labels used twice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0072, 0.1353, 1.4184, 3.0731, 0.2732, 5.2682, 1.8439, 1.7561, 0.1891,\n",
       "        2.8367], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make array of label frequencies for sklearn compute_class_weight using entire dataset\n",
    "# really should be doing this for train set (otherwise, data leakage...)\n",
    "# however, train set may not include certain labels, which leads to error in compute_class_weight\n",
    "\n",
    "# make array of unique classes\n",
    "class_num_arr = [i for i in range(len(label_count))]\n",
    "class_num_arr = np.array(class_num_arr)\n",
    "\n",
    "# make array of all data points with labels\n",
    "lbl_arr = np.array([])\n",
    "for i in range(file_limit*2):\n",
    "    for y in hidden_conc_pf_data.get(i).y:\n",
    "        lbl_arr = np.append(lbl_arr,[y.numpy()],axis=0).astype(int)\n",
    "\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\",classes = class_num_arr, y=lbl_arr)\n",
    "class_weights = torch.from_numpy(class_weights).float().to(device)\n",
    "class_weights[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIN(torch.nn.Module):\n",
    "    \"\"\"GIN\"\"\"\n",
    "    def __init__(self, dim_h, norm_mode='None', norm_scale=10):\n",
    "        super(GIN, self).__init__()\n",
    "        self.conv1 = GINConv(\n",
    "            Sequential(Linear(hidden_conc_pf_data.num_node_features, dim_h),\n",
    "                       BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()))\n",
    "        \n",
    "        self.conv2 = GINConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()))\n",
    "        \n",
    "        self.conv3 = GINConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()))\n",
    "        \n",
    "        self.lin1 = Linear(dim_h*3, dim_h*3)\n",
    "        self.lin2 = Linear(dim_h*3, hidden_conc_pf_data.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h1 = self.conv1(x, edge_index)\n",
    "        h2 = self.conv2(h1, edge_index)\n",
    "        h3 = self.conv3(h2, edge_index)\n",
    "\n",
    "        # Concatenate graph embeddings\n",
    "        h = torch.cat((h1, h2, h3), dim=1)\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "\n",
    "        h = self.lin1(h)\n",
    "        h = h.relu()\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "\n",
    "        h = self.lin2(h)        \n",
    "        return F.log_softmax(h, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GIN model training\n",
    "\n",
    "def train(model, loader, lr):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    # commented out code is to use class weights to account for imbalanced dataset\n",
    "    #criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "    epochs = 1000\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs+1):\n",
    "        total_loss = 0\n",
    "        acc = 0\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "        #cur_graph = 0   # used to keep track of current statement to enforce preds of only PREVIOUS labels in training\n",
    "\n",
    "        # Train on batches\n",
    "        for data in loader:       \n",
    "            #cur_graph += torch.max(data.batch) + 1\n",
    "            data = data.to(device, non_blocking=True)\n",
    "            data.y = data.y.to(torch.float).to(device, non_blocking=True)\n",
    "            optimizer.zero_grad()\n",
    "            length = len(loader)\n",
    "            out = model(data.x, data.edge_index.long())\n",
    "            data.y = data.y.type(torch.LongTensor).to(device, non_blocking=True)\n",
    "            out = out.type(torch.float32).to(device, non_blocking=True)\n",
    "            loss = criterion(out, data.y)\n",
    "            total_loss += loss / length\n",
    "\n",
    "            # commented out code below is meant to enforce predictions to only come from previous theorems\n",
    "            # for dict to be properly created, you must delete and recreate test.pt by rerunning \n",
    "            # pf_data = ProofDataset(root=\"data/\",file_limit=file_limit)\n",
    "            \n",
    "            #dict = pf_data.class_corr\n",
    "            #dict_keys = [k for k in dict.keys()]            \n",
    "\n",
    "            #def return_next_lowest_idx(num):\n",
    "                #if num in dict_keys:\n",
    "                    #return dict[num]\n",
    "                #while num not in dict_keys:\n",
    "                    #try:\n",
    "                        #num -= 1\n",
    "                        #return dict[num]\n",
    "                    #except:\n",
    "                        #pass\n",
    "\n",
    "            #with torch.no_grad():\n",
    "                #cur_graph_batch.apply_(return_next_lowest_idx)\n",
    "                #masked_lbls = (torch.arange(out.size(1)) < (cur_graph_batch[..., None]+1))*(out.cpu())\n",
    "                #try:\n",
    "                    #masked_lbls = (torch.arange(out.size(1)) < (cur_graph_batch[..., None]+1))*(out.cpu())\n",
    "                    #masked_lbls = torch.where(masked_lbls==0,np.nan,masked_lbls)\n",
    "                    #masked_lbls = masked_lbls.detach().numpy()                \n",
    "                    #pred = np.nanargmax(masked_lbls,axis=1)\n",
    "                    #pred = torch.from_numpy(pred)\n",
    "                    #acc += accuracy(pred, data.y.cpu()) / length\n",
    "                #except Exception as e:\n",
    "                    #print(\"a lil error\")\n",
    "                    #out = out + .00000001\n",
    "                    #masked_lbls = (torch.arange(out.size(1)) < (cur_graph_batch[..., None]+1))*(out.cpu())\n",
    "                    #masked_lbls = torch.where(masked_lbls==0,np.nan,masked_lbls)\n",
    "                    #masked_lbls = masked_lbls.detach().numpy()\n",
    "                    #pred = np.nanargmax(masked_lbls,axis=1)\n",
    "                    #pred = torch.from_numpy(pred)\n",
    "                    #acc += accuracy(pred, data.y.cpu()) / length\n",
    "                    #out = out - .00000001\n",
    "\n",
    "            #comment out the follow pred and acc lines if enforcing predictions as described above\n",
    "            pred = out.argmax(dim=1)\n",
    "            acc += accuracy(pred, data.y) / length\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # run model on validation set\n",
    "            val_loss, val_acc, val_f1, top3_acc = test(model, val_loader)\n",
    "\n",
    "        # Print metrics every epoch\n",
    "        if(epoch % 10 == 0):\n",
    "            print(f'Epoch {epoch:>3} | Train Loss: {total_loss:.2f} | Train Acc: {acc*100:>5.2f}% | Val Loss: {val_loss:.2f} | Val Acc: {val_acc*100:.2f}% | Top3 Val Acc: {top3_acc*100:.2f}%| F Score: {val_f1:.2f}')\n",
    "            \n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, loader):\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    top3_acc = 0\n",
    "    fscore = 0\n",
    "    \n",
    "    \n",
    "    for data in loader:\n",
    "        data = data.to(device, non_blocking=True)\n",
    "        length = len(loader)\n",
    "        out = model(data.x, data.edge_index.long())\n",
    "        prob = torch.exp(out)\n",
    "        prob_sorted = torch.topk(prob,k=3).indices\n",
    "        data.y = data.y.type(torch.LongTensor).to(device, non_blocking=True)\n",
    "        loss += criterion(out, data.y) / length\n",
    "        pred = out.argmax(dim=1)\n",
    "        acc += accuracy(pred, data.y) / length\n",
    "        top3_acc += torch.sum(torch.sum(prob_sorted==data.y.unsqueeze(1),dim=1),dim=0) / (length*(data.y.shape[0]))\n",
    "        fscore += f1(pred.cpu(), data.y.cpu(), average='macro')    # micro looks better, but macro prob more accurate\n",
    "\n",
    "    return loss, acc, fscore, top3_acc\n",
    "\n",
    "def accuracy(pred_y, y):\n",
    "    \"\"\"Calculate accuracy.\"\"\"\n",
    "    return ((pred_y == y).sum() / len(y)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GIN(\n",
       "  (conv1): GINConv(nn=Sequential(\n",
       "    (0): Linear(in_features=512, out_features=800, bias=True)\n",
       "    (1): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=800, out_features=800, bias=True)\n",
       "    (4): ReLU()\n",
       "  ))\n",
       "  (conv2): GINConv(nn=Sequential(\n",
       "    (0): Linear(in_features=800, out_features=800, bias=True)\n",
       "    (1): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=800, out_features=800, bias=True)\n",
       "    (4): ReLU()\n",
       "  ))\n",
       "  (conv3): GINConv(nn=Sequential(\n",
       "    (0): Linear(in_features=800, out_features=800, bias=True)\n",
       "    (1): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=800, out_features=800, bias=True)\n",
       "    (4): ReLU()\n",
       "  ))\n",
       "  (conv4): GINConv(nn=Sequential(\n",
       "    (0): Linear(in_features=800, out_features=800, bias=True)\n",
       "    (1): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=800, out_features=800, bias=True)\n",
       "    (4): ReLU()\n",
       "  ))\n",
       "  (lin1): Linear(in_features=3200, out_features=3200, bias=True)\n",
       "  (lin2): Linear(in_features=3200, out_features=555, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize (and reset weights of) model\n",
    "# dim_h is hyperparameter of number of hidden layers\n",
    "\n",
    "gin_trained = None\n",
    "gin = None\n",
    "gin = GIN(dim_h=800).to(device)\n",
    "gin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | Train Loss: 1093.11 | Train Acc: 24.38% | Val Loss: 4.67 | Val Acc: 17.15% | Top3 Val Acc: 0.4268786907196045%| F Score: 0.00\n",
      "Epoch  10 | Train Loss: 2.60 | Train Acc: 56.86% | Val Loss: 3.55 | Val Acc: 41.00% | Top3 Val Acc: 0.4855004549026489%| F Score: 0.00\n",
      "Epoch  20 | Train Loss: 2.27 | Train Acc: 57.74% | Val Loss: 3.09 | Val Acc: 41.69% | Top3 Val Acc: 0.5132522583007812%| F Score: 0.01\n",
      "Epoch  30 | Train Loss: 1.99 | Train Acc: 58.82% | Val Loss: 2.83 | Val Acc: 42.81% | Top3 Val Acc: 0.547552227973938%| F Score: 0.01\n",
      "Epoch  40 | Train Loss: 1.91 | Train Acc: 58.80% | Val Loss: 2.72 | Val Acc: 42.56% | Top3 Val Acc: 0.5768631100654602%| F Score: 0.02\n",
      "Epoch  50 | Train Loss: 1.65 | Train Acc: 60.59% | Val Loss: 2.49 | Val Acc: 44.56% | Top3 Val Acc: 0.6092921495437622%| F Score: 0.05\n",
      "Epoch  60 | Train Loss: 1.55 | Train Acc: 61.57% | Val Loss: 2.31 | Val Acc: 45.84% | Top3 Val Acc: 0.6345493793487549%| F Score: 0.09\n",
      "Epoch  70 | Train Loss: 1.43 | Train Acc: 62.92% | Val Loss: 2.19 | Val Acc: 46.93% | Top3 Val Acc: 0.6772684454917908%| F Score: 0.08\n",
      "Epoch  80 | Train Loss: 1.38 | Train Acc: 63.87% | Val Loss: 2.16 | Val Acc: 48.61% | Top3 Val Acc: 0.6838166117668152%| F Score: 0.12\n",
      "Epoch  90 | Train Loss: 1.22 | Train Acc: 66.97% | Val Loss: 1.94 | Val Acc: 52.39% | Top3 Val Acc: 0.715622067451477%| F Score: 0.18\n",
      "Epoch 100 | Train Loss: 1.29 | Train Acc: 66.03% | Val Loss: 2.00 | Val Acc: 52.17% | Top3 Val Acc: 0.7171811461448669%| F Score: 0.18\n",
      "Epoch 110 | Train Loss: 1.01 | Train Acc: 71.24% | Val Loss: 2.01 | Val Acc: 50.55% | Top3 Val Acc: 0.7090739011764526%| F Score: 0.21\n",
      "Epoch 120 | Train Loss: 1.05 | Train Acc: 69.97% | Val Loss: 2.05 | Val Acc: 51.14% | Top3 Val Acc: 0.7125038504600525%| F Score: 0.22\n",
      "Epoch 130 | Train Loss: 0.86 | Train Acc: 74.38% | Val Loss: 2.16 | Val Acc: 51.82% | Top3 Val Acc: 0.7140629887580872%| F Score: 0.21\n",
      "Epoch 140 | Train Loss: 0.76 | Train Acc: 77.33% | Val Loss: 1.90 | Val Acc: 54.35% | Top3 Val Acc: 0.7539756298065186%| F Score: 0.26\n",
      "Epoch 150 | Train Loss: 0.67 | Train Acc: 79.89% | Val Loss: 2.06 | Val Acc: 56.22% | Top3 Val Acc: 0.759588360786438%| F Score: 0.29\n",
      "Epoch 160 | Train Loss: 0.62 | Train Acc: 80.92% | Val Loss: 2.05 | Val Acc: 57.90% | Top3 Val Acc: 0.7773619890213013%| F Score: 0.34\n",
      "Epoch 170 | Train Loss: 0.65 | Train Acc: 79.89% | Val Loss: 2.06 | Val Acc: 58.81% | Top3 Val Acc: 0.7751792669296265%| F Score: 0.31\n",
      "Epoch 180 | Train Loss: 0.72 | Train Acc: 79.03% | Val Loss: 2.64 | Val Acc: 49.30% | Top3 Val Acc: 0.7178047895431519%| F Score: 0.23\n",
      "Epoch 190 | Train Loss: 0.41 | Train Acc: 87.82% | Val Loss: 2.20 | Val Acc: 60.52% | Top3 Val Acc: 0.7829747200012207%| F Score: 0.39\n",
      "Epoch 200 | Train Loss: 0.33 | Train Acc: 90.52% | Val Loss: 2.32 | Val Acc: 61.68% | Top3 Val Acc: 0.7845337986946106%| F Score: 0.37\n",
      "Epoch 210 | Train Loss: 0.34 | Train Acc: 90.38% | Val Loss: 2.28 | Val Acc: 63.11% | Top3 Val Acc: 0.7976301312446594%| F Score: 0.41\n",
      "Epoch 220 | Train Loss: 0.33 | Train Acc: 90.85% | Val Loss: 2.20 | Val Acc: 63.11% | Top3 Val Acc: 0.7920174598693848%| F Score: 0.41\n",
      "Epoch 230 | Train Loss: 0.25 | Train Acc: 92.75% | Val Loss: 2.76 | Val Acc: 61.74% | Top3 Val Acc: 0.7913938164710999%| F Score: 0.37\n",
      "Epoch 240 | Train Loss: 0.36 | Train Acc: 90.02% | Val Loss: 2.34 | Val Acc: 63.24% | Top3 Val Acc: 0.7932646870613098%| F Score: 0.42\n",
      "Epoch 250 | Train Loss: 0.22 | Train Acc: 93.92% | Val Loss: 2.59 | Val Acc: 63.80% | Top3 Val Acc: 0.7954474091529846%| F Score: 0.42\n",
      "Epoch 260 | Train Loss: 0.25 | Train Acc: 93.26% | Val Loss: 2.54 | Val Acc: 63.49% | Top3 Val Acc: 0.7932646870613098%| F Score: 0.41\n",
      "Epoch 270 | Train Loss: 1.12 | Train Acc: 70.91% | Val Loss: 2.17 | Val Acc: 61.30% | Top3 Val Acc: 0.7817274332046509%| F Score: 0.37\n",
      "Epoch 280 | Train Loss: 1.15 | Train Acc: 72.12% | Val Loss: 2.13 | Val Acc: 59.28% | Top3 Val Acc: 0.7680074572563171%| F Score: 0.38\n",
      "Epoch 290 | Train Loss: 0.21 | Train Acc: 94.01% | Val Loss: 2.62 | Val Acc: 63.42% | Top3 Val Acc: 0.7876519560813904%| F Score: 0.41\n",
      "Epoch 300 | Train Loss: 0.29 | Train Acc: 92.00% | Val Loss: 2.58 | Val Acc: 63.08% | Top3 Val Acc: 0.79014652967453%| F Score: 0.40\n",
      "Epoch 310 | Train Loss: 0.19 | Train Acc: 94.47% | Val Loss: 3.16 | Val Acc: 63.27% | Top3 Val Acc: 0.7888992428779602%| F Score: 0.40\n",
      "Epoch 320 | Train Loss: 0.18 | Train Acc: 94.48% | Val Loss: 3.08 | Val Acc: 64.17% | Top3 Val Acc: 0.79014652967453%| F Score: 0.41\n",
      "Epoch 330 | Train Loss: 0.21 | Train Acc: 94.09% | Val Loss: 2.77 | Val Acc: 63.64% | Top3 Val Acc: 0.7895228862762451%| F Score: 0.41\n",
      "Epoch 340 | Train Loss: 0.18 | Train Acc: 94.61% | Val Loss: 3.23 | Val Acc: 63.74% | Top3 Val Acc: 0.7935765385627747%| F Score: 0.42\n",
      "Epoch 350 | Train Loss: 0.18 | Train Acc: 94.82% | Val Loss: 3.12 | Val Acc: 64.14% | Top3 Val Acc: 0.7898346781730652%| F Score: 0.41\n",
      "Epoch 360 | Train Loss: 1.37 | Train Acc: 69.64% | Val Loss: 2.20 | Val Acc: 57.37% | Top3 Val Acc: 0.749610185623169%| F Score: 0.31\n",
      "Epoch 370 | Train Loss: 0.17 | Train Acc: 94.73% | Val Loss: 3.24 | Val Acc: 63.74% | Top3 Val Acc: 0.7870283722877502%| F Score: 0.41\n",
      "Epoch 380 | Train Loss: 0.17 | Train Acc: 94.84% | Val Loss: 3.12 | Val Acc: 63.70% | Top3 Val Acc: 0.7923292517662048%| F Score: 0.41\n",
      "Epoch 390 | Train Loss: 0.17 | Train Acc: 94.79% | Val Loss: 3.45 | Val Acc: 63.52% | Top3 Val Acc: 0.7904583215713501%| F Score: 0.41\n",
      "Epoch 400 | Train Loss: 0.17 | Train Acc: 94.81% | Val Loss: 3.24 | Val Acc: 64.48% | Top3 Val Acc: 0.7923292517662048%| F Score: 0.43\n",
      "Epoch 410 | Train Loss: 0.17 | Train Acc: 94.82% | Val Loss: 3.35 | Val Acc: 63.77% | Top3 Val Acc: 0.7873401641845703%| F Score: 0.41\n",
      "Epoch 420 | Train Loss: 0.17 | Train Acc: 94.85% | Val Loss: 3.34 | Val Acc: 64.23% | Top3 Val Acc: 0.7876519560813904%| F Score: 0.41\n",
      "Epoch 430 | Train Loss: 0.76 | Train Acc: 80.03% | Val Loss: 2.42 | Val Acc: 62.77% | Top3 Val Acc: 0.7851574420928955%| F Score: 0.41\n",
      "Epoch 440 | Train Loss: 0.17 | Train Acc: 94.86% | Val Loss: 3.37 | Val Acc: 63.74% | Top3 Val Acc: 0.7879638075828552%| F Score: 0.41\n",
      "Epoch 450 | Train Loss: 0.20 | Train Acc: 94.23% | Val Loss: 3.01 | Val Acc: 63.95% | Top3 Val Acc: 0.7888992428779602%| F Score: 0.42\n",
      "Epoch 460 | Train Loss: 0.17 | Train Acc: 94.94% | Val Loss: 3.46 | Val Acc: 64.08% | Top3 Val Acc: 0.7882755994796753%| F Score: 0.42\n",
      "Epoch 470 | Train Loss: 0.16 | Train Acc: 94.93% | Val Loss: 3.43 | Val Acc: 63.70% | Top3 Val Acc: 0.7895228862762451%| F Score: 0.41\n",
      "Epoch 480 | Train Loss: 0.17 | Train Acc: 94.92% | Val Loss: 3.39 | Val Acc: 63.49% | Top3 Val Acc: 0.7842220067977905%| F Score: 0.41\n",
      "Epoch 490 | Train Loss: 0.17 | Train Acc: 94.84% | Val Loss: 3.58 | Val Acc: 63.27% | Top3 Val Acc: 0.7870283722877502%| F Score: 0.41\n",
      "Epoch 500 | Train Loss: 0.18 | Train Acc: 94.69% | Val Loss: 3.30 | Val Acc: 63.45% | Top3 Val Acc: 0.7873401641845703%| F Score: 0.42\n",
      "Epoch 510 | Train Loss: 0.16 | Train Acc: 94.93% | Val Loss: 3.69 | Val Acc: 63.36% | Top3 Val Acc: 0.7867165207862854%| F Score: 0.41\n",
      "Epoch 520 | Train Loss: 0.16 | Train Acc: 94.92% | Val Loss: 3.62 | Val Acc: 63.36% | Top3 Val Acc: 0.7904583215713501%| F Score: 0.42\n",
      "Epoch 530 | Train Loss: 0.16 | Train Acc: 94.89% | Val Loss: 3.55 | Val Acc: 63.36% | Top3 Val Acc: 0.7864047288894653%| F Score: 0.41\n",
      "Epoch 540 | Train Loss: 0.16 | Train Acc: 94.97% | Val Loss: 3.76 | Val Acc: 63.36% | Top3 Val Acc: 0.7835983633995056%| F Score: 0.41\n",
      "Epoch 550 | Train Loss: 0.19 | Train Acc: 94.44% | Val Loss: 3.16 | Val Acc: 63.58% | Top3 Val Acc: 0.7904583215713501%| F Score: 0.41\n",
      "Epoch 560 | Train Loss: 0.16 | Train Acc: 94.94% | Val Loss: 3.78 | Val Acc: 63.67% | Top3 Val Acc: 0.7888992428779602%| F Score: 0.41\n",
      "Epoch 570 | Train Loss: 1.05 | Train Acc: 75.78% | Val Loss: 2.42 | Val Acc: 61.37% | Top3 Val Acc: 0.7729965448379517%| F Score: 0.38\n",
      "Epoch 580 | Train Loss: 0.16 | Train Acc: 94.90% | Val Loss: 3.80 | Val Acc: 63.61% | Top3 Val Acc: 0.7835983633995056%| F Score: 0.41\n",
      "Epoch 590 | Train Loss: 0.16 | Train Acc: 94.92% | Val Loss: 3.92 | Val Acc: 63.95% | Top3 Val Acc: 0.7820392847061157%| F Score: 0.41\n",
      "Epoch 600 | Train Loss: 0.16 | Train Acc: 94.93% | Val Loss: 3.69 | Val Acc: 63.92% | Top3 Val Acc: 0.7851574420928955%| F Score: 0.42\n",
      "Epoch 610 | Train Loss: 0.16 | Train Acc: 95.04% | Val Loss: 4.07 | Val Acc: 64.20% | Top3 Val Acc: 0.7817274332046509%| F Score: 0.42\n",
      "Epoch 620 | Train Loss: 0.29 | Train Acc: 91.25% | Val Loss: 3.14 | Val Acc: 63.89% | Top3 Val Acc: 0.781103789806366%| F Score: 0.41\n",
      "Epoch 630 | Train Loss: 0.16 | Train Acc: 94.97% | Val Loss: 3.92 | Val Acc: 63.99% | Top3 Val Acc: 0.7829747200012207%| F Score: 0.41\n",
      "Epoch 640 | Train Loss: 0.16 | Train Acc: 95.01% | Val Loss: 4.14 | Val Acc: 63.74% | Top3 Val Acc: 0.7839101552963257%| F Score: 0.40\n",
      "Epoch 650 | Train Loss: 0.16 | Train Acc: 95.03% | Val Loss: 4.04 | Val Acc: 63.36% | Top3 Val Acc: 0.779232919216156%| F Score: 0.40\n",
      "Epoch 660 | Train Loss: 0.20 | Train Acc: 93.87% | Val Loss: 3.49 | Val Acc: 63.64% | Top3 Val Acc: 0.7814156413078308%| F Score: 0.41\n",
      "Epoch 670 | Train Loss: 0.16 | Train Acc: 95.05% | Val Loss: 4.12 | Val Acc: 63.64% | Top3 Val Acc: 0.781103789806366%| F Score: 0.41\n",
      "Epoch 680 | Train Loss: 1.05 | Train Acc: 75.05% | Val Loss: 2.98 | Val Acc: 63.02% | Top3 Val Acc: 0.7776737809181213%| F Score: 0.40\n",
      "Epoch 690 | Train Loss: 0.16 | Train Acc: 95.07% | Val Loss: 4.13 | Val Acc: 63.52% | Top3 Val Acc: 0.7851574420928955%| F Score: 0.41\n",
      "Epoch 700 | Train Loss: 0.16 | Train Acc: 95.11% | Val Loss: 4.06 | Val Acc: 63.64% | Top3 Val Acc: 0.7839101552963257%| F Score: 0.41\n",
      "Epoch 710 | Train Loss: 0.21 | Train Acc: 93.78% | Val Loss: 3.50 | Val Acc: 63.58% | Top3 Val Acc: 0.7845337986946106%| F Score: 0.41\n",
      "Epoch 720 | Train Loss: 0.16 | Train Acc: 95.06% | Val Loss: 4.20 | Val Acc: 63.45% | Top3 Val Acc: 0.7826628684997559%| F Score: 0.41\n",
      "Epoch 730 | Train Loss: 0.53 | Train Acc: 85.39% | Val Loss: 3.29 | Val Acc: 63.21% | Top3 Val Acc: 0.7767383456230164%| F Score: 0.40\n",
      "Epoch 740 | Train Loss: 0.16 | Train Acc: 95.09% | Val Loss: 4.25 | Val Acc: 63.61% | Top3 Val Acc: 0.7854692339897156%| F Score: 0.41\n",
      "Epoch 750 | Train Loss: 0.16 | Train Acc: 95.08% | Val Loss: 4.41 | Val Acc: 63.17% | Top3 Val Acc: 0.7820392847061157%| F Score: 0.41\n",
      "Epoch 760 | Train Loss: 0.16 | Train Acc: 95.06% | Val Loss: 4.23 | Val Acc: 63.27% | Top3 Val Acc: 0.7842220067977905%| F Score: 0.40\n",
      "Epoch 770 | Train Loss: 0.16 | Train Acc: 95.04% | Val Loss: 4.49 | Val Acc: 63.52% | Top3 Val Acc: 0.7835983633995056%| F Score: 0.40\n",
      "Epoch 780 | Train Loss: 0.61 | Train Acc: 83.11% | Val Loss: 3.05 | Val Acc: 63.33% | Top3 Val Acc: 0.7758029103279114%| F Score: 0.41\n",
      "Epoch 790 | Train Loss: 0.16 | Train Acc: 95.07% | Val Loss: 4.22 | Val Acc: 63.49% | Top3 Val Acc: 0.780480146408081%| F Score: 0.41\n",
      "Epoch 800 | Train Loss: 0.16 | Train Acc: 95.06% | Val Loss: 4.47 | Val Acc: 63.14% | Top3 Val Acc: 0.7835983633995056%| F Score: 0.41\n",
      "Epoch 810 | Train Loss: 0.16 | Train Acc: 95.07% | Val Loss: 4.32 | Val Acc: 63.36% | Top3 Val Acc: 0.7782974243164062%| F Score: 0.42\n",
      "Epoch 820 | Train Loss: 0.16 | Train Acc: 95.02% | Val Loss: 3.97 | Val Acc: 63.83% | Top3 Val Acc: 0.7776737809181213%| F Score: 0.42\n",
      "Epoch 830 | Train Loss: 0.16 | Train Acc: 95.09% | Val Loss: 4.40 | Val Acc: 63.24% | Top3 Val Acc: 0.7839101552963257%| F Score: 0.41\n",
      "Epoch 840 | Train Loss: 0.17 | Train Acc: 94.94% | Val Loss: 3.77 | Val Acc: 62.15% | Top3 Val Acc: 0.7720611095428467%| F Score: 0.40\n",
      "Epoch 850 | Train Loss: 0.15 | Train Acc: 95.11% | Val Loss: 4.39 | Val Acc: 63.42% | Top3 Val Acc: 0.7848456501960754%| F Score: 0.41\n",
      "Epoch 860 | Train Loss: 0.15 | Train Acc: 95.10% | Val Loss: 4.65 | Val Acc: 63.27% | Top3 Val Acc: 0.7820392847061157%| F Score: 0.40\n",
      "Epoch 870 | Train Loss: 0.16 | Train Acc: 95.08% | Val Loss: 4.15 | Val Acc: 63.55% | Top3 Val Acc: 0.7842220067977905%| F Score: 0.41\n",
      "Epoch 880 | Train Loss: 0.15 | Train Acc: 95.12% | Val Loss: 4.61 | Val Acc: 63.55% | Top3 Val Acc: 0.7854692339897156%| F Score: 0.41\n",
      "Epoch 890 | Train Loss: 0.15 | Train Acc: 95.12% | Val Loss: 4.49 | Val Acc: 64.20% | Top3 Val Acc: 0.7851574420928955%| F Score: 0.41\n",
      "Epoch 900 | Train Loss: 0.15 | Train Acc: 95.08% | Val Loss: 4.64 | Val Acc: 64.02% | Top3 Val Acc: 0.7823510766029358%| F Score: 0.41\n",
      "Epoch 910 | Train Loss: 0.16 | Train Acc: 94.97% | Val Loss: 4.21 | Val Acc: 63.99% | Top3 Val Acc: 0.780480146408081%| F Score: 0.42\n",
      "Epoch 920 | Train Loss: 0.16 | Train Acc: 95.09% | Val Loss: 4.37 | Val Acc: 63.83% | Top3 Val Acc: 0.7779856324195862%| F Score: 0.40\n",
      "Epoch 930 | Train Loss: 0.15 | Train Acc: 95.10% | Val Loss: 4.68 | Val Acc: 63.55% | Top3 Val Acc: 0.7761147022247314%| F Score: 0.40\n",
      "Epoch 940 | Train Loss: 0.16 | Train Acc: 95.03% | Val Loss: 4.67 | Val Acc: 63.39% | Top3 Val Acc: 0.7814156413078308%| F Score: 0.40\n",
      "Epoch 950 | Train Loss: 0.35 | Train Acc: 89.78% | Val Loss: 3.87 | Val Acc: 63.58% | Top3 Val Acc: 0.7779856324195862%| F Score: 0.42\n",
      "Epoch 960 | Train Loss: 0.15 | Train Acc: 95.17% | Val Loss: 4.88 | Val Acc: 63.42% | Top3 Val Acc: 0.7789210677146912%| F Score: 0.40\n",
      "Epoch 970 | Train Loss: 0.16 | Train Acc: 95.07% | Val Loss: 4.69 | Val Acc: 63.52% | Top3 Val Acc: 0.780480146408081%| F Score: 0.41\n",
      "Epoch 980 | Train Loss: 0.15 | Train Acc: 95.11% | Val Loss: 4.38 | Val Acc: 63.92% | Top3 Val Acc: 0.780168354511261%| F Score: 0.42\n",
      "Epoch 990 | Train Loss: 0.15 | Train Acc: 95.11% | Val Loss: 4.92 | Val Acc: 63.42% | Top3 Val Acc: 0.7782974243164062%| F Score: 0.41\n",
      "Epoch 1000 | Train Loss: 0.15 | Train Acc: 95.08% | Val Loss: 4.35 | Val Acc: 64.08% | Top3 Val Acc: 0.7854692339897156%| F Score: 0.42\n"
     ]
    }
   ],
   "source": [
    "# reset weights and train model\n",
    "gin_trained = None\n",
    "gin_trained = train(gin, train_loader,lr=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restricting to conclusion nodes, our val_graph has accuracy being 53.80\n",
      "unk accuracy should be: 83.48\n",
      "top10 accuracy is: 85.20%\n"
     ]
    }
   ],
   "source": [
    "# get accuracy only for conclusion node from val set\n",
    "\n",
    "total_conc_labels = len(val_dataset)\n",
    "correct_conc_pred = 0\n",
    "\n",
    "# dict of the form true_label:(predicted_labels)\n",
    "incorrect_preds = {}\n",
    "\n",
    "for graph in val_dataset:\n",
    "    get_gin_predict = gin_trained(graph.x.to(device),graph.edge_index.long().to(device))\n",
    "    get_conc_predict = np.argmax(get_gin_predict[-1].detach().cpu().numpy(),axis=0)\n",
    "    if get_conc_predict == graph.y[-1].item():\n",
    "        correct_conc_pred += 1\n",
    "    if get_conc_predict != graph.y[-1].item():\n",
    "        if graph.y[-1].item() in incorrect_preds:\n",
    "            incorrect_preds[graph.y[-1].item()] = incorrect_preds[graph.y[-1].item()]+(get_conc_predict,)\n",
    "        else:\n",
    "            incorrect_preds[graph.y[-1].item()] = (get_conc_predict,)\n",
    "\n",
    "\n",
    "def get_conc_gin_label_acc(i):\n",
    "    correct_pred_i = 0\n",
    "    total_num_i = 0\n",
    "\n",
    "    for graph in val_dataset:\n",
    "        get_gin_predict = gin_trained(graph.x.to(device),graph.edge_index.long().to(device))\n",
    "        get_conc_predict = np.argmax(get_gin_predict[-1].detach().cpu().numpy(),axis=0)\n",
    "        if graph.y[-1].item() == i:\n",
    "            total_num_i += 1\n",
    "            if get_conc_predict == i:\n",
    "                correct_pred_i += 1\n",
    "    if total_num_i == 0:\n",
    "        return 0,0,0\n",
    "    return (correct_pred_i / total_num_i), total_num_i, correct_pred_i\n",
    "\n",
    "def get_topk_acc(k=3):\n",
    "\n",
    "    top3_corr = 0\n",
    "\n",
    "    for graph in val_dataset:\n",
    "        get_gin_predict = gin_trained(graph.x.to(device),graph.edge_index.long().to(device))\n",
    "        prob = torch.exp(get_gin_predict[-1])\n",
    "        prob_sorted = torch.topk(prob,k=k).indices\n",
    "        graph.y = graph.y.to(device)\n",
    "        top3_corr += torch.sum(prob_sorted==graph.y[-1])\n",
    "\n",
    "    return top3_corr / len(val_dataset)\n",
    "\n",
    "k=10\n",
    "print(f\"restricting to conclusion nodes, our val_graph has accuracy being {correct_conc_pred/total_conc_labels*100:.2f}\")\n",
    "print(f\"unk accuracy should be: {get_conc_gin_label_acc(554)[0]*100:.2f}\")\n",
    "print(f\"top{k} accuracy is: {get_topk_acc(10).item()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary to record label frequency based on val set\n",
    "# only consider conclusion/final nodes\n",
    "\n",
    "# get max label used in pf_data\n",
    "max_label = 0\n",
    "\n",
    "for i in val_indices:\n",
    "    if hidden_conc_pf_data.get(i).y[-1] > max_label:\n",
    "        max_label = hidden_conc_pf_data.get(i).y[-1].to(int).item()\n",
    "\n",
    "# initialize histogram for labels used in pf_data            \n",
    "label_count = {}\n",
    "\n",
    "for i in val_indices:\n",
    "    for j in range(max_label+1):        \n",
    "        label_count[j] = 0\n",
    "\n",
    "for i in val_indices:\n",
    "    label_count[hidden_conc_pf_data.get(i).y[-1].to(int).item()] += 1\n",
    "\n",
    "step_count = 0\n",
    "max = 0\n",
    "max_freq_index = None   # find the most frequently used index\n",
    "labels_never_used = 0\n",
    "labels_used_once = 0\n",
    "labels_used_twice = 0\n",
    "\n",
    "\n",
    "for k,v in label_count.items():\n",
    "    step_count += v\n",
    "    if v > max:     \n",
    "        max = v\n",
    "        max_freq_index = k\n",
    "\n",
    "    if v == 0:\n",
    "        labels_never_used += 1\n",
    "    if v == 1:\n",
    "        labels_used_once += 1\n",
    "    if v ==2:\n",
    "        labels_used_twice += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of steps is: 500\n",
      "highest frequency label is 554 and occurs 115 times\n",
      "final label used is 554\n",
      "{0: 2, 1: 22, 2: 0, 3: 1, 4: 3, 5: 1, 6: 2, 7: 0, 8: 19, 9: 0, 10: 2, 11: 1, 12: 0, 13: 0, 14: 0, 15: 0, 16: 4, 17: 1, 18: 2, 19: 0, 20: 0, 21: 0, 22: 1, 23: 1, 24: 0, 25: 3, 26: 1, 27: 0, 28: 0, 29: 0, 30: 1, 31: 1, 32: 0, 33: 0, 34: 0, 35: 2, 36: 0, 37: 0, 38: 0, 39: 0, 40: 1, 41: 10, 42: 1, 43: 0, 44: 15, 45: 7, 46: 0, 47: 1, 48: 4, 49: 3, 50: 0, 51: 0, 52: 2, 53: 6, 54: 2, 55: 3, 56: 0, 57: 0, 58: 7, 59: 2, 60: 0, 61: 0, 62: 8, 63: 0, 64: 2, 65: 0, 66: 1, 67: 0, 68: 0, 69: 0, 70: 0, 71: 1, 72: 1, 73: 0, 74: 3, 75: 3, 76: 0, 77: 0, 78: 0, 79: 0, 80: 2, 81: 2, 82: 0, 83: 28, 84: 2, 85: 1, 86: 7, 87: 7, 88: 1, 89: 1, 90: 1, 91: 1, 92: 1, 93: 0, 94: 1, 95: 0, 96: 0, 97: 6, 98: 2, 99: 0, 100: 1, 101: 18, 102: 0, 103: 0, 104: 5, 105: 0, 106: 1, 107: 0, 108: 0, 109: 0, 110: 2, 111: 2, 112: 0, 113: 0, 114: 0, 115: 0, 116: 0, 117: 0, 118: 0, 119: 1, 120: 0, 121: 0, 122: 0, 123: 1, 124: 0, 125: 0, 126: 1, 127: 3, 128: 0, 129: 0, 130: 3, 131: 0, 132: 0, 133: 0, 134: 5, 135: 1, 136: 2, 137: 1, 138: 0, 139: 0, 140: 1, 141: 0, 142: 1, 143: 0, 144: 1, 145: 0, 146: 0, 147: 0, 148: 0, 149: 0, 150: 0, 151: 1, 152: 0, 153: 0, 154: 0, 155: 0, 156: 0, 157: 2, 158: 2, 159: 1, 160: 0, 161: 0, 162: 0, 163: 0, 164: 0, 165: 0, 166: 0, 167: 1, 168: 0, 169: 2, 170: 2, 171: 3, 172: 1, 173: 0, 174: 2, 175: 0, 176: 0, 177: 0, 178: 0, 179: 0, 180: 0, 181: 0, 182: 0, 183: 0, 184: 1, 185: 0, 186: 0, 187: 0, 188: 2, 189: 5, 190: 0, 191: 0, 192: 2, 193: 4, 194: 0, 195: 0, 196: 0, 197: 0, 198: 3, 199: 0, 200: 0, 201: 0, 202: 2, 203: 1, 204: 1, 205: 0, 206: 0, 207: 0, 208: 1, 209: 0, 210: 1, 211: 0, 212: 0, 213: 0, 214: 0, 215: 0, 216: 0, 217: 0, 218: 0, 219: 0, 220: 0, 221: 0, 222: 0, 223: 0, 224: 0, 225: 0, 226: 0, 227: 0, 228: 1, 229: 0, 230: 1, 231: 5, 232: 3, 233: 3, 234: 0, 235: 0, 236: 0, 237: 1, 238: 0, 239: 0, 240: 0, 241: 0, 242: 2, 243: 0, 244: 0, 245: 0, 246: 0, 247: 0, 248: 0, 249: 0, 250: 1, 251: 0, 252: 0, 253: 0, 254: 0, 255: 0, 256: 0, 257: 0, 258: 1, 259: 0, 260: 0, 261: 0, 262: 0, 263: 0, 264: 0, 265: 0, 266: 0, 267: 2, 268: 1, 269: 0, 270: 0, 271: 0, 272: 0, 273: 1, 274: 0, 275: 0, 276: 0, 277: 0, 278: 0, 279: 0, 280: 1, 281: 0, 282: 4, 283: 3, 284: 1, 285: 0, 286: 0, 287: 0, 288: 0, 289: 0, 290: 0, 291: 0, 292: 0, 293: 0, 294: 0, 295: 0, 296: 0, 297: 0, 298: 0, 299: 0, 300: 0, 301: 0, 302: 0, 303: 0, 304: 0, 305: 0, 306: 0, 307: 0, 308: 0, 309: 0, 310: 0, 311: 0, 312: 0, 313: 0, 314: 0, 315: 0, 316: 0, 317: 0, 318: 0, 319: 0, 320: 0, 321: 0, 322: 0, 323: 0, 324: 0, 325: 0, 326: 0, 327: 0, 328: 1, 329: 0, 330: 0, 331: 0, 332: 0, 333: 0, 334: 0, 335: 0, 336: 0, 337: 0, 338: 0, 339: 0, 340: 0, 341: 0, 342: 0, 343: 0, 344: 0, 345: 0, 346: 0, 347: 0, 348: 0, 349: 0, 350: 0, 351: 0, 352: 0, 353: 0, 354: 0, 355: 0, 356: 0, 357: 0, 358: 0, 359: 1, 360: 0, 361: 0, 362: 0, 363: 0, 364: 0, 365: 0, 366: 0, 367: 0, 368: 0, 369: 0, 370: 0, 371: 0, 372: 0, 373: 0, 374: 0, 375: 0, 376: 0, 377: 0, 378: 0, 379: 0, 380: 4, 381: 2, 382: 0, 383: 0, 384: 0, 385: 0, 386: 0, 387: 2, 388: 0, 389: 0, 390: 1, 391: 1, 392: 0, 393: 0, 394: 0, 395: 0, 396: 2, 397: 5, 398: 2, 399: 1, 400: 0, 401: 4, 402: 2, 403: 0, 404: 2, 405: 0, 406: 1, 407: 10, 408: 0, 409: 0, 410: 0, 411: 0, 412: 0, 413: 0, 414: 0, 415: 0, 416: 0, 417: 0, 418: 0, 419: 0, 420: 0, 421: 0, 422: 0, 423: 0, 424: 3, 425: 0, 426: 0, 427: 0, 428: 0, 429: 0, 430: 0, 431: 0, 432: 0, 433: 0, 434: 0, 435: 0, 436: 0, 437: 2, 438: 0, 439: 0, 440: 0, 441: 0, 442: 0, 443: 0, 444: 0, 445: 1, 446: 0, 447: 0, 448: 0, 449: 0, 450: 0, 451: 0, 452: 0, 453: 0, 454: 0, 455: 0, 456: 0, 457: 0, 458: 0, 459: 0, 460: 0, 461: 0, 462: 0, 463: 0, 464: 0, 465: 0, 466: 0, 467: 0, 468: 0, 469: 0, 470: 0, 471: 0, 472: 0, 473: 0, 474: 0, 475: 0, 476: 0, 477: 0, 478: 0, 479: 0, 480: 0, 481: 0, 482: 0, 483: 0, 484: 0, 485: 0, 486: 0, 487: 0, 488: 0, 489: 0, 490: 0, 491: 0, 492: 0, 493: 0, 494: 0, 495: 0, 496: 0, 497: 1, 498: 0, 499: 0, 500: 0, 501: 0, 502: 0, 503: 0, 504: 0, 505: 0, 506: 0, 507: 0, 508: 0, 509: 0, 510: 0, 511: 0, 512: 0, 513: 0, 514: 0, 515: 0, 516: 0, 517: 0, 518: 0, 519: 0, 520: 0, 521: 0, 522: 0, 523: 0, 524: 0, 525: 0, 526: 0, 527: 0, 528: 0, 529: 0, 530: 0, 531: 0, 532: 0, 533: 0, 534: 0, 535: 0, 536: 0, 537: 0, 538: 0, 539: 0, 540: 0, 541: 0, 542: 0, 543: 0, 544: 0, 545: 0, 546: 0, 547: 0, 548: 0, 549: 0, 550: 0, 551: 0, 552: 0, 553: 0, 554: 115}\n",
      "555 unique labels are used\n",
      "424 unique labels never used\n",
      "58 unique labels used once\n",
      "33 unique labels used twice\n"
     ]
    }
   ],
   "source": [
    "print(f\"total number of steps is:\", step_count)\n",
    "print(f\"highest frequency label is {max_freq_index} and occurs {max} times\")\n",
    "print(f\"final label used is {len(label_count)-1}\")\n",
    "print(label_count)\n",
    "print(len(label_count),\"unique labels are used\")\n",
    "print(labels_never_used,\"unique labels never used\")\n",
    "print(labels_used_once, \"unique labels used once\")\n",
    "print(labels_used_twice, \"unique labels used twice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on label 0\n",
      "on label 200\n",
      "on label 400\n",
      "{0: (0.0, 2), 1: (0.7272727272727273, 22), 3: (0.0, 1), 4: (0.0, 3), 5: (0.0, 1), 6: (0.5, 2), 8: (0.5263157894736842, 19), 10: (0.5, 2), 11: (0.0, 1), 16: (0.25, 4), 17: (0.0, 1), 18: (0.5, 2), 22: (0.0, 1), 23: (0.0, 1), 25: (0.3333333333333333, 3), 26: (0.0, 1), 30: (0.0, 1), 31: (0.0, 1), 35: (0.0, 2), 40: (0.0, 1), 41: (0.2, 10), 42: (0.0, 1), 44: (0.2, 15), 45: (0.0, 7), 47: (0.0, 1), 48: (0.25, 4), 49: (0.0, 3), 52: (0.0, 2), 53: (0.0, 6), 54: (0.0, 2), 55: (0.3333333333333333, 3), 58: (0.42857142857142855, 7), 59: (0.0, 2), 62: (0.125, 8), 64: (0.0, 2), 66: (0.0, 1), 71: (0.0, 1), 72: (1.0, 1), 74: (0.0, 3), 75: (0.6666666666666666, 3), 80: (0.0, 2), 81: (1.0, 2), 83: (0.5357142857142857, 28), 84: (0.0, 2), 85: (0.0, 1), 86: (0.42857142857142855, 7), 87: (0.42857142857142855, 7), 88: (0.0, 1), 89: (0.0, 1), 90: (1.0, 1), 91: (0.0, 1), 92: (0.0, 1), 94: (0.0, 1), 97: (0.0, 6), 98: (0.0, 2), 100: (0.0, 1), 101: (0.6111111111111112, 18), 104: (0.6, 5), 106: (1.0, 1), 110: (0.0, 2), 111: (0.0, 2), 119: (0.0, 1), 123: (0.0, 1), 126: (0.0, 1), 127: (0.0, 3), 130: (1.0, 3), 134: (0.4, 5), 135: (0.0, 1), 136: (0.0, 2), 137: (0.0, 1), 140: (0.0, 1), 142: (0.0, 1), 144: (0.0, 1), 151: (0.0, 1), 157: (0.0, 2), 158: (0.0, 2), 159: (0.0, 1), 167: (1.0, 1), 169: (0.0, 2), 170: (0.0, 2), 171: (0.3333333333333333, 3), 172: (1.0, 1), 174: (1.0, 2), 184: (0.0, 1), 188: (0.0, 2), 189: (0.6, 5), 192: (0.0, 2), 193: (0.0, 4), 198: (0.3333333333333333, 3), 202: (0.0, 2), 203: (0.0, 1), 204: (0.0, 1), 208: (0.0, 1), 210: (0.0, 1), 228: (0.0, 1), 230: (0.0, 1), 231: (0.0, 5), 232: (0.3333333333333333, 3), 233: (0.0, 3), 237: (0.0, 1), 242: (0.5, 2), 250: (1.0, 1), 258: (0.0, 1), 267: (0.5, 2), 268: (0.0, 1), 273: (0.0, 1), 280: (0.0, 1), 282: (1.0, 4), 283: (0.3333333333333333, 3), 284: (0.0, 1), 328: (0.0, 1), 359: (0.0, 1), 380: (0.5, 4), 381: (0.5, 2), 387: (0.0, 2), 390: (0.0, 1), 391: (0.0, 1), 396: (0.0, 2), 397: (0.0, 5), 398: (1.0, 2), 399: (0.0, 1), 401: (0.25, 4), 402: (0.0, 2), 404: (0.0, 2), 406: (0.0, 1), 407: (0.5, 10), 424: (1.0, 3), 437: (0.0, 2), 445: (0.0, 1), 497: (1.0, 1), 554: (0.6869565217391305, 115)}\n",
      "if everything here is correct, our conc node val_acc should be: 0.402\n"
     ]
    }
   ],
   "source": [
    "# do the same as above, but now only for val conclusion nodes\n",
    "\n",
    "conc_label_acc_dict = {}\n",
    "\n",
    "def get_conc_gin_label_acc(i):\n",
    "    correct_pred_i = 0\n",
    "    total_num_i = 0\n",
    "\n",
    "    for graph in val_dataset:\n",
    "        get_gin_predict = gin_trained(graph.x.to(device),graph.edge_index.long().to(device))\n",
    "        get_conc_predict = np.argmax(get_gin_predict[-1].detach().cpu().numpy(),axis=0)\n",
    "        if graph.y[-1].item() == i:\n",
    "            total_num_i += 1\n",
    "            if get_conc_predict == i:\n",
    "                correct_pred_i += 1\n",
    "    if total_num_i == 0:\n",
    "        return 0,0,0\n",
    "    return (correct_pred_i / total_num_i), total_num_i, correct_pred_i\n",
    "\n",
    "\n",
    "total_conc_labels = 0\n",
    "total_conc_correct_preds = 0\n",
    "for i in range(hidden_conc_pf_data.num_classes):\n",
    "    if i % 200 ==0:\n",
    "        print(\"on label\",i)\n",
    "    try:\n",
    "        container = get_conc_gin_label_acc(i)\n",
    "        if container[1] != 0:\n",
    "            conc_label_acc_dict[i] = (container[0],container[1])  # pair of accuracy and label count for each label\n",
    "        total_conc_labels += container[1]\n",
    "        total_conc_correct_preds += container[2]\n",
    "    except Exception as e:\n",
    "        print(e,get_conc_gin_label_acc(i))\n",
    "\n",
    "print(conc_label_acc_dict)\n",
    "print(\"if everything here is correct, our conc node val_acc should be:\",total_conc_correct_preds/total_conc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0\n",
      "1 0.7272727272727273\n",
      "3 0.0\n",
      "4 0.0\n",
      "5 0.0\n",
      "6 0.5\n",
      "8 0.5263157894736842\n",
      "10 0.5\n",
      "11 0.0\n",
      "16 0.25\n",
      "17 0.0\n",
      "18 0.5\n",
      "22 0.0\n",
      "23 0.0\n",
      "25 0.3333333333333333\n",
      "26 0.0\n",
      "30 0.0\n",
      "31 0.0\n",
      "35 0.0\n",
      "40 0.0\n",
      "41 0.2\n",
      "42 0.0\n",
      "44 0.2\n",
      "45 0.0\n",
      "47 0.0\n",
      "48 0.25\n",
      "49 0.0\n",
      "52 0.0\n",
      "53 0.0\n",
      "54 0.0\n",
      "55 0.3333333333333333\n",
      "58 0.42857142857142855\n",
      "59 0.0\n",
      "62 0.125\n",
      "64 0.0\n",
      "66 0.0\n",
      "71 0.0\n",
      "72 1.0\n",
      "74 0.0\n",
      "75 0.6666666666666666\n",
      "80 0.0\n",
      "81 1.0\n",
      "83 0.5357142857142857\n",
      "84 0.0\n",
      "85 0.0\n",
      "86 0.42857142857142855\n",
      "87 0.42857142857142855\n",
      "88 0.0\n",
      "89 0.0\n",
      "90 1.0\n",
      "91 0.0\n",
      "92 0.0\n",
      "94 0.0\n",
      "97 0.0\n",
      "98 0.0\n",
      "100 0.0\n",
      "101 0.6111111111111112\n",
      "104 0.6\n",
      "106 1.0\n",
      "110 0.0\n",
      "111 0.0\n",
      "119 0.0\n",
      "123 0.0\n",
      "126 0.0\n",
      "127 0.0\n",
      "130 1.0\n",
      "134 0.4\n",
      "135 0.0\n",
      "136 0.0\n",
      "137 0.0\n",
      "140 0.0\n",
      "142 0.0\n",
      "144 0.0\n",
      "151 0.0\n",
      "157 0.0\n",
      "158 0.0\n",
      "159 0.0\n",
      "167 1.0\n",
      "169 0.0\n",
      "170 0.0\n",
      "171 0.3333333333333333\n",
      "172 1.0\n",
      "174 1.0\n",
      "184 0.0\n",
      "188 0.0\n",
      "189 0.6\n",
      "192 0.0\n",
      "193 0.0\n",
      "198 0.3333333333333333\n",
      "202 0.0\n",
      "203 0.0\n",
      "204 0.0\n",
      "208 0.0\n",
      "210 0.0\n",
      "228 0.0\n",
      "230 0.0\n",
      "231 0.0\n",
      "232 0.3333333333333333\n",
      "233 0.0\n",
      "237 0.0\n",
      "242 0.5\n",
      "250 1.0\n",
      "258 0.0\n",
      "267 0.5\n",
      "268 0.0\n",
      "273 0.0\n",
      "280 0.0\n",
      "282 1.0\n",
      "283 0.3333333333333333\n",
      "284 0.0\n",
      "328 0.0\n",
      "359 0.0\n",
      "380 0.5\n",
      "381 0.5\n",
      "387 0.0\n",
      "390 0.0\n",
      "391 0.0\n",
      "396 0.0\n",
      "397 0.0\n",
      "398 1.0\n",
      "399 0.0\n",
      "401 0.25\n",
      "402 0.0\n",
      "404 0.0\n",
      "406 0.0\n",
      "407 0.5\n",
      "424 1.0\n",
      "437 0.0\n",
      "445 0.0\n",
      "497 1.0\n",
      "554 0.6869565217391305\n"
     ]
    }
   ],
   "source": [
    "for k,v in conc_label_acc_dict.items():\n",
    "    print(k,v[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: (0.0, 1),\n",
       " 5: (0.0, 1),\n",
       " 11: (0.0, 1),\n",
       " 17: (0.0, 1),\n",
       " 22: (0.0, 1),\n",
       " 23: (0.0, 1),\n",
       " 26: (0.0, 1),\n",
       " 30: (0.0, 1),\n",
       " 31: (0.0, 1),\n",
       " 40: (0.0, 1),\n",
       " 42: (0.0, 1),\n",
       " 47: (0.0, 1),\n",
       " 66: (0.0, 1),\n",
       " 71: (0.0, 1),\n",
       " 85: (0.0, 1),\n",
       " 88: (0.0, 1),\n",
       " 89: (0.0, 1),\n",
       " 91: (0.0, 1),\n",
       " 92: (0.0, 1),\n",
       " 94: (0.0, 1),\n",
       " 100: (0.0, 1),\n",
       " 119: (0.0, 1),\n",
       " 123: (0.0, 1),\n",
       " 126: (0.0, 1),\n",
       " 135: (0.0, 1),\n",
       " 137: (0.0, 1),\n",
       " 140: (0.0, 1),\n",
       " 142: (0.0, 1),\n",
       " 144: (0.0, 1),\n",
       " 151: (0.0, 1),\n",
       " 159: (0.0, 1),\n",
       " 184: (0.0, 1),\n",
       " 203: (0.0, 1),\n",
       " 204: (0.0, 1),\n",
       " 208: (0.0, 1),\n",
       " 210: (0.0, 1),\n",
       " 228: (0.0, 1),\n",
       " 230: (0.0, 1),\n",
       " 237: (0.0, 1),\n",
       " 258: (0.0, 1),\n",
       " 268: (0.0, 1),\n",
       " 273: (0.0, 1),\n",
       " 280: (0.0, 1),\n",
       " 284: (0.0, 1),\n",
       " 328: (0.0, 1),\n",
       " 359: (0.0, 1),\n",
       " 390: (0.0, 1),\n",
       " 391: (0.0, 1),\n",
       " 399: (0.0, 1),\n",
       " 406: (0.0, 1),\n",
       " 445: (0.0, 1),\n",
       " 0: (0.0, 2),\n",
       " 35: (0.0, 2),\n",
       " 52: (0.0, 2),\n",
       " 54: (0.0, 2),\n",
       " 59: (0.0, 2),\n",
       " 64: (0.0, 2),\n",
       " 80: (0.0, 2),\n",
       " 84: (0.0, 2),\n",
       " 98: (0.0, 2),\n",
       " 110: (0.0, 2),\n",
       " 111: (0.0, 2),\n",
       " 136: (0.0, 2),\n",
       " 157: (0.0, 2),\n",
       " 158: (0.0, 2),\n",
       " 169: (0.0, 2),\n",
       " 170: (0.0, 2),\n",
       " 188: (0.0, 2),\n",
       " 192: (0.0, 2),\n",
       " 202: (0.0, 2),\n",
       " 387: (0.0, 2),\n",
       " 396: (0.0, 2),\n",
       " 402: (0.0, 2),\n",
       " 404: (0.0, 2),\n",
       " 437: (0.0, 2),\n",
       " 4: (0.0, 3),\n",
       " 49: (0.0, 3),\n",
       " 74: (0.0, 3),\n",
       " 127: (0.0, 3),\n",
       " 233: (0.0, 3),\n",
       " 193: (0.0, 4),\n",
       " 231: (0.0, 5),\n",
       " 397: (0.0, 5),\n",
       " 53: (0.0, 6),\n",
       " 97: (0.0, 6),\n",
       " 45: (0.0, 7),\n",
       " 62: (0.125, 8),\n",
       " 41: (0.2, 10),\n",
       " 44: (0.2, 15),\n",
       " 16: (0.25, 4),\n",
       " 48: (0.25, 4),\n",
       " 401: (0.25, 4),\n",
       " 25: (0.3333333333333333, 3),\n",
       " 55: (0.3333333333333333, 3),\n",
       " 171: (0.3333333333333333, 3),\n",
       " 198: (0.3333333333333333, 3),\n",
       " 232: (0.3333333333333333, 3),\n",
       " 283: (0.3333333333333333, 3),\n",
       " 134: (0.4, 5),\n",
       " 58: (0.42857142857142855, 7),\n",
       " 86: (0.42857142857142855, 7),\n",
       " 87: (0.42857142857142855, 7),\n",
       " 6: (0.5, 2),\n",
       " 10: (0.5, 2),\n",
       " 18: (0.5, 2),\n",
       " 242: (0.5, 2),\n",
       " 267: (0.5, 2),\n",
       " 381: (0.5, 2),\n",
       " 380: (0.5, 4),\n",
       " 407: (0.5, 10),\n",
       " 8: (0.5263157894736842, 19),\n",
       " 83: (0.5357142857142857, 28),\n",
       " 104: (0.6, 5),\n",
       " 189: (0.6, 5),\n",
       " 101: (0.6111111111111112, 18),\n",
       " 75: (0.6666666666666666, 3),\n",
       " 554: (0.6869565217391305, 115),\n",
       " 1: (0.7272727272727273, 22),\n",
       " 72: (1.0, 1),\n",
       " 90: (1.0, 1),\n",
       " 106: (1.0, 1),\n",
       " 167: (1.0, 1),\n",
       " 172: (1.0, 1),\n",
       " 250: (1.0, 1),\n",
       " 497: (1.0, 1),\n",
       " 81: (1.0, 2),\n",
       " 174: (1.0, 2),\n",
       " 398: (1.0, 2),\n",
       " 130: (1.0, 3),\n",
       " 424: (1.0, 3),\n",
       " 282: (1.0, 4)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_label_acc_dict = {k: v for k, v in sorted(conc_label_acc_dict.items(), key=lambda item: item[1])}\n",
    "sorted_label_acc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restricting to conclusion nodes, our val_graph has accuracy being 0.402\n"
     ]
    }
   ],
   "source": [
    "# get accuracy only for conclusion node from val set\n",
    "\n",
    "total_conc_labels = len(val_dataset)\n",
    "correct_conc_pred = 0\n",
    "\n",
    "# dict of the form true_label:(predicted_labels)\n",
    "incorrect_preds = {}\n",
    "\n",
    "for graph in val_dataset:\n",
    "    get_gin_predict = gin_trained(graph.x.to(device),graph.edge_index.long().to(device))\n",
    "    get_conc_predict = np.argmax(get_gin_predict[-1].detach().cpu().numpy(),axis=0)\n",
    "    if get_conc_predict == graph.y[-1].item():\n",
    "        correct_conc_pred += 1\n",
    "    if get_conc_predict != graph.y[-1].item():\n",
    "        if graph.y[-1].item() in incorrect_preds:\n",
    "            incorrect_preds[graph.y[-1].item()] = incorrect_preds[graph.y[-1].item()]+(get_conc_predict,)\n",
    "        else:\n",
    "            incorrect_preds[graph.y[-1].item()] = (get_conc_predict,)\n",
    "\n",
    "print(\"restricting to conclusion nodes, our val_graph has accuracy being\",correct_conc_pred/total_conc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'label_acc_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sorted_label_acc_dict \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mlabel_acc_dict\u001b[49m\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m item: item[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mreverse()}\n",
      "\u001b[1;31mNameError\u001b[0m: name 'label_acc_dict' is not defined"
     ]
    }
   ],
   "source": [
    "sorted_label_acc_dict = {k: v for k, v in sorted(label_acc_dict.items(), key=lambda item: item[1]).reverse()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{423: (tensor(1.), 11),\n",
       " 412: (tensor(1.), 9),\n",
       " 470: (tensor(1.), 8),\n",
       " 284: (tensor(1.), 7),\n",
       " 334: (tensor(1.), 7),\n",
       " 104: (tensor(1.), 6),\n",
       " 480: (tensor(1.), 6),\n",
       " 269: (tensor(1.), 5),\n",
       " 358: (tensor(1.), 5),\n",
       " 439: (tensor(1.), 5),\n",
       " 484: (tensor(1.), 5),\n",
       " 527: (tensor(1.), 5),\n",
       " 242: (tensor(1.), 4),\n",
       " 282: (tensor(1.), 4),\n",
       " 326: (tensor(1.), 4),\n",
       " 355: (tensor(1.), 4),\n",
       " 382: (tensor(1.), 4),\n",
       " 385: (tensor(1.), 4),\n",
       " 469: (tensor(1.), 4),\n",
       " 478: (tensor(1.), 4),\n",
       " 177: (tensor(1.), 3),\n",
       " 327: (tensor(1.), 3),\n",
       " 349: (tensor(1.), 3),\n",
       " 363: (tensor(1.), 3),\n",
       " 375: (tensor(1.), 3),\n",
       " 422: (tensor(1.), 3),\n",
       " 424: (tensor(1.), 3),\n",
       " 429: (tensor(1.), 3),\n",
       " 434: (tensor(1.), 3),\n",
       " 466: (tensor(1.), 3),\n",
       " 467: (tensor(1.), 3),\n",
       " 532: (tensor(1.), 3),\n",
       " 548: (tensor(1.), 3),\n",
       " 552: (tensor(1.), 3),\n",
       " 14: (tensor(1.), 2),\n",
       " 70: (tensor(1.), 2),\n",
       " 82: (tensor(1.), 2),\n",
       " 149: (tensor(1.), 2),\n",
       " 234: (tensor(1.), 2),\n",
       " 239: (tensor(1.), 2),\n",
       " 243: (tensor(1.), 2),\n",
       " 285: (tensor(1.), 2),\n",
       " 286: (tensor(1.), 2),\n",
       " 309: (tensor(1.), 2),\n",
       " 322: (tensor(1.), 2),\n",
       " 331: (tensor(1.), 2),\n",
       " 343: (tensor(1.), 2),\n",
       " 351: (tensor(1.), 2),\n",
       " 353: (tensor(1.), 2),\n",
       " 366: (tensor(1.), 2),\n",
       " 370: (tensor(1.), 2),\n",
       " 379: (tensor(1.), 2),\n",
       " 392: (tensor(1.), 2),\n",
       " 419: (tensor(1.), 2),\n",
       " 438: (tensor(1.), 2),\n",
       " 487: (tensor(1.), 2),\n",
       " 490: (tensor(1.), 2),\n",
       " 495: (tensor(1.), 2),\n",
       " 516: (tensor(1.), 2),\n",
       " 538: (tensor(1.), 2),\n",
       " 2: (tensor(1.), 1),\n",
       " 9: (tensor(1.), 1),\n",
       " 15: (tensor(1.), 1),\n",
       " 22: (tensor(1.), 1),\n",
       " 69: (tensor(1.), 1),\n",
       " 73: (tensor(1.), 1),\n",
       " 91: (tensor(1.), 1),\n",
       " 103: (tensor(1.), 1),\n",
       " 105: (tensor(1.), 1),\n",
       " 106: (tensor(1.), 1),\n",
       " 139: (tensor(1.), 1),\n",
       " 172: (tensor(1.), 1),\n",
       " 190: (tensor(1.), 1),\n",
       " 191: (tensor(1.), 1),\n",
       " 197: (tensor(1.), 1),\n",
       " 236: (tensor(1.), 1),\n",
       " 241: (tensor(1.), 1),\n",
       " 252: (tensor(1.), 1),\n",
       " 257: (tensor(1.), 1),\n",
       " 265: (tensor(1.), 1),\n",
       " 266: (tensor(1.), 1),\n",
       " 274: (tensor(1.), 1),\n",
       " 279: (tensor(1.), 1),\n",
       " 294: (tensor(1.), 1),\n",
       " 308: (tensor(1.), 1),\n",
       " 310: (tensor(1.), 1),\n",
       " 311: (tensor(1.), 1),\n",
       " 313: (tensor(1.), 1),\n",
       " 323: (tensor(1.), 1),\n",
       " 325: (tensor(1.), 1),\n",
       " 344: (tensor(1.), 1),\n",
       " 350: (tensor(1.), 1),\n",
       " 352: (tensor(1.), 1),\n",
       " 361: (tensor(1.), 1),\n",
       " 364: (tensor(1.), 1),\n",
       " 365: (tensor(1.), 1),\n",
       " 371: (tensor(1.), 1),\n",
       " 377: (tensor(1.), 1),\n",
       " 378: (tensor(1.), 1),\n",
       " 386: (tensor(1.), 1),\n",
       " 388: (tensor(1.), 1),\n",
       " 389: (tensor(1.), 1),\n",
       " 405: (tensor(1.), 1),\n",
       " 417: (tensor(1.), 1),\n",
       " 421: (tensor(1.), 1),\n",
       " 432: (tensor(1.), 1),\n",
       " 451: (tensor(1.), 1),\n",
       " 454: (tensor(1.), 1),\n",
       " 456: (tensor(1.), 1),\n",
       " 461: (tensor(1.), 1),\n",
       " 468: (tensor(1.), 1),\n",
       " 473: (tensor(1.), 1),\n",
       " 475: (tensor(1.), 1),\n",
       " 485: (tensor(1.), 1),\n",
       " 488: (tensor(1.), 1),\n",
       " 492: (tensor(1.), 1),\n",
       " 493: (tensor(1.), 1),\n",
       " 497: (tensor(1.), 1),\n",
       " 509: (tensor(1.), 1),\n",
       " 514: (tensor(1.), 1),\n",
       " 517: (tensor(1.), 1),\n",
       " 528: (tensor(1.), 1),\n",
       " 533: (tensor(1.), 1),\n",
       " 542: (tensor(1.), 1),\n",
       " 549: (tensor(1.), 1),\n",
       " 550: (tensor(1.), 1),\n",
       " 551: (tensor(1.), 1),\n",
       " 0: (0.9981818181818182, 550),\n",
       " 444: (tensor(0.9231), 13),\n",
       " 376: (tensor(0.8889), 9),\n",
       " 415: (tensor(0.8889), 9),\n",
       " 1: (tensor(0.8810), 42),\n",
       " 58: (tensor(0.8750), 8),\n",
       " 207: (tensor(0.8750), 8),\n",
       " 534: (tensor(0.8750), 8),\n",
       " 291: (tensor(0.8571), 7),\n",
       " 491: (tensor(0.8333), 6),\n",
       " 256: (tensor(0.8000), 10),\n",
       " 122: (tensor(0.8000), 5),\n",
       " 189: (tensor(0.8000), 5),\n",
       " 315: (tensor(0.8000), 5),\n",
       " 393: (tensor(0.8000), 5),\n",
       " 554: (tensor(0.7967), 777),\n",
       " 440: (tensor(0.7778), 9),\n",
       " 11: (tensor(0.7600), 25),\n",
       " 407: (tensor(0.7500), 12),\n",
       " 163: (tensor(0.7500), 4),\n",
       " 167: (tensor(0.7500), 4),\n",
       " 235: (tensor(0.7500), 4),\n",
       " 250: (tensor(0.7500), 4),\n",
       " 283: (tensor(0.7500), 4),\n",
       " 342: (tensor(0.7500), 4),\n",
       " 380: (tensor(0.7500), 4),\n",
       " 4: (tensor(0.7407), 27),\n",
       " 154: (tensor(0.7143), 7),\n",
       " 409: (tensor(0.7143), 7),\n",
       " 153: (tensor(0.7000), 10),\n",
       " 387: (tensor(0.7000), 10),\n",
       " 44: (tensor(0.6667), 18),\n",
       " 48: (tensor(0.6667), 12),\n",
       " 264: (tensor(0.6667), 9),\n",
       " 535: (tensor(0.6667), 6),\n",
       " 46: (tensor(0.6667), 3),\n",
       " 55: (tensor(0.6667), 3),\n",
       " 81: (tensor(0.6667), 3),\n",
       " 127: (tensor(0.6667), 3),\n",
       " 138: (tensor(0.6667), 3),\n",
       " 292: (tensor(0.6667), 3),\n",
       " 301: (tensor(0.6667), 3),\n",
       " 303: (tensor(0.6667), 3),\n",
       " 362: (tensor(0.6667), 3),\n",
       " 395: (tensor(0.6667), 3),\n",
       " 426: (tensor(0.6667), 3),\n",
       " 500: (tensor(0.6667), 3),\n",
       " 539: (tensor(0.6667), 3),\n",
       " 8: (tensor(0.6500), 40),\n",
       " 83: (tensor(0.6364), 55),\n",
       " 101: (tensor(0.6250), 24),\n",
       " 47: (tensor(0.6000), 10),\n",
       " 13: (tensor(0.6000), 5),\n",
       " 76: (tensor(0.6000), 5),\n",
       " 114: (tensor(0.6000), 5),\n",
       " 208: (tensor(0.6000), 5),\n",
       " 214: (tensor(0.6000), 5),\n",
       " 501: (tensor(0.6000), 5),\n",
       " 137: (tensor(0.5882), 17),\n",
       " 273: (tensor(0.5882), 17),\n",
       " 86: (tensor(0.5833), 12),\n",
       " 414: (tensor(0.5833), 12),\n",
       " 75: (tensor(0.5714), 7),\n",
       " 510: (tensor(0.5714), 7),\n",
       " 398: (tensor(0.5455), 11),\n",
       " 87: (tensor(0.5000), 14),\n",
       " 280: (tensor(0.5000), 14),\n",
       " 222: (tensor(0.5000), 8),\n",
       " 40: (tensor(0.5000), 6),\n",
       " 68: (tensor(0.5000), 6),\n",
       " 160: (tensor(0.5000), 6),\n",
       " 293: (tensor(0.5000), 6),\n",
       " 324: (tensor(0.5000), 6),\n",
       " 50: (tensor(0.5000), 4),\n",
       " 80: (tensor(0.5000), 4),\n",
       " 90: (tensor(0.5000), 4),\n",
       " 148: (tensor(0.5000), 4),\n",
       " 184: (tensor(0.5000), 4),\n",
       " 287: (tensor(0.5000), 4),\n",
       " 329: (tensor(0.5000), 4),\n",
       " 347: (tensor(0.5000), 4),\n",
       " 390: (tensor(0.5000), 4),\n",
       " 404: (tensor(0.5000), 4),\n",
       " 445: (tensor(0.5000), 4),\n",
       " 531: (tensor(0.5000), 4),\n",
       " 17: (tensor(0.5000), 2),\n",
       " 27: (tensor(0.5000), 2),\n",
       " 108: (tensor(0.5000), 2),\n",
       " 113: (tensor(0.5000), 2),\n",
       " 142: (tensor(0.5000), 2),\n",
       " 159: (tensor(0.5000), 2),\n",
       " 164: (tensor(0.5000), 2),\n",
       " 196: (tensor(0.5000), 2),\n",
       " 263: (tensor(0.5000), 2),\n",
       " 267: (tensor(0.5000), 2),\n",
       " 304: (tensor(0.5000), 2),\n",
       " 317: (tensor(0.5000), 2),\n",
       " 321: (tensor(0.5000), 2),\n",
       " 356: (tensor(0.5000), 2),\n",
       " 437: (tensor(0.5000), 2),\n",
       " 441: (tensor(0.5000), 2),\n",
       " 450: (tensor(0.5000), 2),\n",
       " 518: (tensor(0.5000), 2),\n",
       " 522: (tensor(0.5000), 2),\n",
       " 524: (tensor(0.5000), 2),\n",
       " 546: (tensor(0.5000), 2),\n",
       " 297: (tensor(0.4688), 32),\n",
       " 41: (tensor(0.4615), 13),\n",
       " 16: (tensor(0.4375), 16),\n",
       " 62: (tensor(0.4286), 14),\n",
       " 57: (tensor(0.4286), 7),\n",
       " 92: (tensor(0.4286), 7),\n",
       " 107: (tensor(0.4286), 7),\n",
       " 408: (tensor(0.4286), 7),\n",
       " 134: (tensor(0.4000), 10),\n",
       " 52: (tensor(0.4000), 5),\n",
       " 61: (tensor(0.4000), 5),\n",
       " 123: (tensor(0.4000), 5),\n",
       " 150: (tensor(0.4000), 5),\n",
       " 180: (tensor(0.4000), 5),\n",
       " 183: (tensor(0.4000), 5),\n",
       " 233: (tensor(0.4000), 5),\n",
       " 338: (tensor(0.4000), 5),\n",
       " 374: (tensor(0.4000), 5),\n",
       " 452: (tensor(0.4000), 5),\n",
       " 151: (tensor(0.3333), 15),\n",
       " 202: (tensor(0.3333), 9),\n",
       " 479: (tensor(0.3333), 9),\n",
       " 51: (tensor(0.3333), 6),\n",
       " 333: (tensor(0.3333), 6),\n",
       " 416: (tensor(0.3333), 6),\n",
       " 19: (tensor(0.3333), 3),\n",
       " 25: (tensor(0.3333), 3),\n",
       " 94: (tensor(0.3333), 3),\n",
       " 102: (tensor(0.3333), 3),\n",
       " 128: (tensor(0.3333), 3),\n",
       " 130: (tensor(0.3333), 3),\n",
       " 136: (tensor(0.3333), 3),\n",
       " 145: (tensor(0.3333), 3),\n",
       " 157: (tensor(0.3333), 3),\n",
       " 212: (tensor(0.3333), 3),\n",
       " 300: (tensor(0.3333), 3),\n",
       " 354: (tensor(0.3333), 3),\n",
       " 413: (tensor(0.3333), 3),\n",
       " 435: (tensor(0.3333), 3),\n",
       " 472: (tensor(0.3333), 3),\n",
       " 499: (tensor(0.3333), 3),\n",
       " 520: (tensor(0.3333), 3),\n",
       " 18: (tensor(0.3000), 10),\n",
       " 97: (tensor(0.2500), 16),\n",
       " 231: (tensor(0.2500), 8),\n",
       " 6: (tensor(0.2500), 4),\n",
       " 23: (tensor(0.2500), 4),\n",
       " 33: (tensor(0.2500), 4),\n",
       " 112: (tensor(0.2500), 4),\n",
       " 135: (tensor(0.2500), 4),\n",
       " 171: (tensor(0.2500), 4),\n",
       " 182: (tensor(0.2500), 4),\n",
       " 218: (tensor(0.2500), 4),\n",
       " 401: (tensor(0.2500), 4),\n",
       " 431: (tensor(0.2500), 4),\n",
       " 447: (tensor(0.2500), 4),\n",
       " 502: (tensor(0.2500), 4),\n",
       " 515: (tensor(0.2500), 4),\n",
       " 544: (tensor(0.2500), 4),\n",
       " 152: (tensor(0.2222), 9),\n",
       " 49: (tensor(0.2143), 14),\n",
       " 71: (tensor(0.2000), 5),\n",
       " 84: (tensor(0.2000), 5),\n",
       " 198: (tensor(0.2000), 5),\n",
       " 35: (tensor(0.1667), 6),\n",
       " 64: (tensor(0.1667), 6),\n",
       " 109: (tensor(0.1667), 6),\n",
       " 124: (tensor(0.1667), 6),\n",
       " 141: (tensor(0.1667), 6),\n",
       " 232: (tensor(0.1667), 6),\n",
       " 240: (tensor(0.1667), 6),\n",
       " 305: (tensor(0.1667), 6),\n",
       " 477: (tensor(0.1667), 6),\n",
       " 504: (tensor(0.1667), 6),\n",
       " 53: (tensor(0.1579), 19),\n",
       " 396: (tensor(0.1429), 7),\n",
       " 181: (tensor(0.1250), 8),\n",
       " 179: (tensor(0.1111), 9),\n",
       " 85: (tensor(0.1000), 10),\n",
       " 425: (tensor(0.1000), 10),\n",
       " 442: (tensor(0.1000), 10),\n",
       " 244: (tensor(0.), 16),\n",
       " 45: (tensor(0.), 8),\n",
       " 209: (tensor(0.), 8),\n",
       " 397: (tensor(0.), 8),\n",
       " 3: (tensor(0.), 7),\n",
       " 161: (tensor(0.), 7),\n",
       " 100: (tensor(0.), 6),\n",
       " 93: (tensor(0.), 5),\n",
       " 175: (tensor(0.), 5),\n",
       " 193: (tensor(0.), 5),\n",
       " 328: (tensor(0.), 5),\n",
       " 7: (tensor(0.), 4),\n",
       " 72: (tensor(0.), 4),\n",
       " 117: (tensor(0.), 4),\n",
       " 143: (tensor(0.), 4),\n",
       " 192: (tensor(0.), 4),\n",
       " 217: (tensor(0.), 4),\n",
       " 219: (tensor(0.), 4),\n",
       " 221: (tensor(0.), 4),\n",
       " 307: (tensor(0.), 4),\n",
       " 339: (tensor(0.), 4),\n",
       " 443: (tensor(0.), 4),\n",
       " 26: (tensor(0.), 3),\n",
       " 29: (tensor(0.), 3),\n",
       " 67: (tensor(0.), 3),\n",
       " 74: (tensor(0.), 3),\n",
       " 98: (tensor(0.), 3),\n",
       " 118: (tensor(0.), 3),\n",
       " 158: (tensor(0.), 3),\n",
       " 170: (tensor(0.), 3),\n",
       " 186: (tensor(0.), 3),\n",
       " 203: (tensor(0.), 3),\n",
       " 215: (tensor(0.), 3),\n",
       " 224: (tensor(0.), 3),\n",
       " 246: (tensor(0.), 3),\n",
       " 276: (tensor(0.), 3),\n",
       " 302: (tensor(0.), 3),\n",
       " 306: (tensor(0.), 3),\n",
       " 402: (tensor(0.), 3),\n",
       " 418: (tensor(0.), 3),\n",
       " 460: (tensor(0.), 3),\n",
       " 511: (tensor(0.), 3),\n",
       " 513: (tensor(0.), 3),\n",
       " 525: (tensor(0.), 3),\n",
       " 10: (tensor(0.), 2),\n",
       " 20: (tensor(0.), 2),\n",
       " 32: (tensor(0.), 2),\n",
       " 54: (tensor(0.), 2),\n",
       " 59: (tensor(0.), 2),\n",
       " 63: (tensor(0.), 2),\n",
       " 66: (tensor(0.), 2),\n",
       " 78: (tensor(0.), 2),\n",
       " 89: (tensor(0.), 2),\n",
       " 110: (tensor(0.), 2),\n",
       " 111: (tensor(0.), 2),\n",
       " 115: (tensor(0.), 2),\n",
       " 131: (tensor(0.), 2),\n",
       " 132: (tensor(0.), 2),\n",
       " 140: (tensor(0.), 2),\n",
       " 168: (tensor(0.), 2),\n",
       " 169: (tensor(0.), 2),\n",
       " 174: (tensor(0.), 2),\n",
       " 188: (tensor(0.), 2),\n",
       " 199: (tensor(0.), 2),\n",
       " 227: (tensor(0.), 2),\n",
       " 258: (tensor(0.), 2),\n",
       " 268: (tensor(0.), 2),\n",
       " 270: (tensor(0.), 2),\n",
       " 271: (tensor(0.), 2),\n",
       " 272: (tensor(0.), 2),\n",
       " 295: (tensor(0.), 2),\n",
       " 345: (tensor(0.), 2),\n",
       " 359: (tensor(0.), 2),\n",
       " 368: (tensor(0.), 2),\n",
       " 369: (tensor(0.), 2),\n",
       " 381: (tensor(0.), 2),\n",
       " 411: (tensor(0.), 2),\n",
       " 455: (tensor(0.), 2),\n",
       " 457: (tensor(0.), 2),\n",
       " 508: (tensor(0.), 2),\n",
       " 543: (tensor(0.), 2),\n",
       " 5: (tensor(0.), 1),\n",
       " 12: (tensor(0.), 1),\n",
       " 30: (tensor(0.), 1),\n",
       " 31: (tensor(0.), 1),\n",
       " 37: (tensor(0.), 1),\n",
       " 38: (tensor(0.), 1),\n",
       " 42: (tensor(0.), 1),\n",
       " 77: (tensor(0.), 1),\n",
       " 88: (tensor(0.), 1),\n",
       " 96: (tensor(0.), 1),\n",
       " 99: (tensor(0.), 1),\n",
       " 119: (tensor(0.), 1),\n",
       " 121: (tensor(0.), 1),\n",
       " 126: (tensor(0.), 1),\n",
       " 133: (tensor(0.), 1),\n",
       " 144: (tensor(0.), 1),\n",
       " 147: (tensor(0.), 1),\n",
       " 173: (tensor(0.), 1),\n",
       " 176: (tensor(0.), 1),\n",
       " 185: (tensor(0.), 1),\n",
       " 194: (tensor(0.), 1),\n",
       " 201: (tensor(0.), 1),\n",
       " 204: (tensor(0.), 1),\n",
       " 205: (tensor(0.), 1),\n",
       " 206: (tensor(0.), 1),\n",
       " 210: (tensor(0.), 1),\n",
       " 211: (tensor(0.), 1),\n",
       " 213: (tensor(0.), 1),\n",
       " 223: (tensor(0.), 1),\n",
       " 228: (tensor(0.), 1),\n",
       " 230: (tensor(0.), 1),\n",
       " 237: (tensor(0.), 1),\n",
       " 238: (tensor(0.), 1),\n",
       " 253: (tensor(0.), 1),\n",
       " 281: (tensor(0.), 1),\n",
       " 290: (tensor(0.), 1),\n",
       " 296: (tensor(0.), 1),\n",
       " 320: (tensor(0.), 1),\n",
       " 336: (tensor(0.), 1),\n",
       " 337: (tensor(0.), 1),\n",
       " 346: (tensor(0.), 1),\n",
       " 357: (tensor(0.), 1),\n",
       " 360: (tensor(0.), 1),\n",
       " 372: (tensor(0.), 1),\n",
       " 391: (tensor(0.), 1),\n",
       " 399: (tensor(0.), 1),\n",
       " 403: (tensor(0.), 1),\n",
       " 406: (tensor(0.), 1),\n",
       " 464: (tensor(0.), 1),\n",
       " 471: (tensor(0.), 1),\n",
       " 483: (tensor(0.), 1),\n",
       " 486: (tensor(0.), 1),\n",
       " 494: (tensor(0.), 1),\n",
       " 498: (tensor(0.), 1),\n",
       " 503: (tensor(0.), 1),\n",
       " 506: (tensor(0.), 1),\n",
       " 507: (tensor(0.), 1),\n",
       " 526: (tensor(0.), 1),\n",
       " 529: (tensor(0.), 1),\n",
       " 536: (tensor(0.), 1),\n",
       " 541: (tensor(0.), 1),\n",
       " 21: (0, 0),\n",
       " 24: (0, 0),\n",
       " 28: (0, 0),\n",
       " 34: (0, 0),\n",
       " 36: (0, 0),\n",
       " 39: (0, 0),\n",
       " 43: (0, 0),\n",
       " 56: (0, 0),\n",
       " 60: (0, 0),\n",
       " 65: (0, 0),\n",
       " 79: (0, 0),\n",
       " 95: (0, 0),\n",
       " 116: (0, 0),\n",
       " 120: (0, 0),\n",
       " 125: (0, 0),\n",
       " 129: (0, 0),\n",
       " 146: (0, 0),\n",
       " 155: (0, 0),\n",
       " 156: (0, 0),\n",
       " 162: (0, 0),\n",
       " 165: (0, 0),\n",
       " 166: (0, 0),\n",
       " 178: (0, 0),\n",
       " 187: (0, 0),\n",
       " 195: (0, 0),\n",
       " 200: (0, 0),\n",
       " 216: (0, 0),\n",
       " 220: (0, 0),\n",
       " 225: (0, 0),\n",
       " 226: (0, 0),\n",
       " 229: (0, 0),\n",
       " 245: (0, 0),\n",
       " 247: (0, 0),\n",
       " 248: (0, 0),\n",
       " 249: (0, 0),\n",
       " 251: (0, 0),\n",
       " 254: (0, 0),\n",
       " 255: (0, 0),\n",
       " 259: (0, 0),\n",
       " 260: (0, 0),\n",
       " 261: (0, 0),\n",
       " 262: (0, 0),\n",
       " 275: (0, 0),\n",
       " 277: (0, 0),\n",
       " 278: (0, 0),\n",
       " 288: (0, 0),\n",
       " 289: (0, 0),\n",
       " 298: (0, 0),\n",
       " 299: (0, 0),\n",
       " 312: (0, 0),\n",
       " 314: (0, 0),\n",
       " 316: (0, 0),\n",
       " 318: (0, 0),\n",
       " 319: (0, 0),\n",
       " 330: (0, 0),\n",
       " 332: (0, 0),\n",
       " 335: (0, 0),\n",
       " 340: (0, 0),\n",
       " 341: (0, 0),\n",
       " 348: (0, 0),\n",
       " 367: (0, 0),\n",
       " 373: (0, 0),\n",
       " 383: (0, 0),\n",
       " 384: (0, 0),\n",
       " 394: (0, 0),\n",
       " 400: (0, 0),\n",
       " 410: (0, 0),\n",
       " 420: (0, 0),\n",
       " 427: (0, 0),\n",
       " 428: (0, 0),\n",
       " 430: (0, 0),\n",
       " 433: (0, 0),\n",
       " 436: (0, 0),\n",
       " 446: (0, 0),\n",
       " 448: (0, 0),\n",
       " 449: (0, 0),\n",
       " 453: (0, 0),\n",
       " 458: (0, 0),\n",
       " 459: (0, 0),\n",
       " 462: (0, 0),\n",
       " 463: (0, 0),\n",
       " 465: (0, 0),\n",
       " 474: (0, 0),\n",
       " 476: (0, 0),\n",
       " 481: (0, 0),\n",
       " 482: (0, 0),\n",
       " 489: (0, 0),\n",
       " 496: (0, 0),\n",
       " 505: (0, 0),\n",
       " 512: (0, 0),\n",
       " 519: (0, 0),\n",
       " 521: (0, 0),\n",
       " 523: (0, 0),\n",
       " 530: (0, 0),\n",
       " 537: (0, 0),\n",
       " 540: (0, 0),\n",
       " 545: (0, 0),\n",
       " 547: (0, 0),\n",
       " 553: (0, 0)}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_label_acc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8/24 EDA for final conclusion nodes from val set\n",
    "\n",
    "# get "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(val_dataset\u001b[38;5;241m.\u001b[39mnum_classes):\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m         container \u001b[38;5;241m=\u001b[39m \u001b[43mget_gin_label_acc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m         label_acc_dict[i] \u001b[38;5;241m=\u001b[39m (container[\u001b[38;5;241m0\u001b[39m],label_count[i])  \u001b[38;5;66;03m# pair of accuracy and label count for each label\u001b[39;00m\n\u001b[0;32m     32\u001b[0m         total_labels \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m container[\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[1;32mIn[89], line 11\u001b[0m, in \u001b[0;36mget_gin_label_acc\u001b[1;34m(i)\u001b[0m\n\u001b[0;32m      8\u001b[0m total_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m graph \u001b[38;5;129;01min\u001b[39;00m val_dataset:\n\u001b[1;32m---> 11\u001b[0m     get_gin_predict \u001b[38;5;241m=\u001b[39m \u001b[43mgin_trained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     get_predict \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(get_gin_predict\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Jared\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jared\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[80], line 21\u001b[0m, in \u001b[0;36mGIN.forward\u001b[1;34m(self, x, edge_index)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# Node embeddings\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     h1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x, edge_index)\n\u001b[1;32m---> 21\u001b[0m     h2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     h3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(h2, edge_index)\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# Graph-level readout\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m#h1 = global_add_pool(h1, batch)\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m#h2 = global_add_pool(h2, batch)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m#h3 = global_add_pool(h3, batch)\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# Concatenate graph embeddings\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jared\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jared\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jared\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch_geometric\\nn\\conv\\gin_conv.py:90\u001b[0m, in \u001b[0;36mGINConv.forward\u001b[1;34m(self, x, edge_index, size)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x_r \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     88\u001b[0m     out \u001b[38;5;241m=\u001b[39m out \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps) \u001b[38;5;241m*\u001b[39m x_r\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m(out)\n",
      "File \u001b[1;32mc:\\Users\\Jared\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1675\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1666\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[0;32m   1668\u001b[0m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[0;32m   1669\u001b[0m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[0;32m   1670\u001b[0m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1673\u001b[0m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[0;32m   1674\u001b[0m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[1;32m-> 1675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m   1676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[0;32m   1677\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# calculate true positive % for label i, i.e. (correct i predictions) / (total i labels)\n",
    "# but now restrict to final conclusion node\n",
    "\n",
    "label_acc_dict = {}\n",
    "\n",
    "def get_gin_label_acc(i):\n",
    "    correct_preds = 0\n",
    "    total_labels = 0\n",
    "\n",
    "    for graph in val_dataset:\n",
    "        get_gin_predict = gin_trained(graph.x.to(device),graph.edge_index.long().to(device))\n",
    "        get_predict = np.argmax(get_gin_predict.detach().cpu().numpy(),axis=1)\n",
    "        if i>0:\n",
    "            total_labels += sum(torch.div(graph.y[graph.y==i],i))\n",
    "        elif i==0:\n",
    "            total_labels += (graph.y[graph.y==0]).nelement()\n",
    "        for idx, num in enumerate(get_predict):\n",
    "            if num == i:\n",
    "                if num==(graph.y.tolist())[idx]:\n",
    "                    correct_preds += 1\n",
    "    if total_labels == 0:\n",
    "        return torch.tensor([0])\n",
    "    return (correct_preds / total_labels), total_labels, correct_preds\n",
    "\n",
    "\n",
    "total_labels = 0\n",
    "total_correct_preds = 0\n",
    "for i in range(hidden_conc_pf_data.num_classes):\n",
    "    try:\n",
    "        container = get_gin_label_acc(i)\n",
    "        label_acc_dict[i] = (container[0],label_count[i])  # pair of accuracy and label count for each label\n",
    "        total_labels += container[1]\n",
    "        total_correct_preds += container[2]\n",
    "    except Exception as e:\n",
    "        pass\n",
    "        #print(e,get_gin_label_acc(i))\n",
    "\n",
    "print(label_acc_dict)\n",
    "print(\"if everything here is correct, our conlusion node val_acc should be:\",total_correct_preds/total_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap\n",
    "\n",
    "lr = [.001,.00005,.00001]\n",
    "h = [400, 1600,3200,6400]\n",
    "\n",
    "for rate in lr:\n",
    "    for hidden in h:\n",
    "        print(rate,hidden)\n",
    "        gin_trained = None\n",
    "        gin = None\n",
    "        gin = GIN(dim_h=hidden).to(device)\n",
    "        print(gin)\n",
    "\n",
    "        # reset weights and train model\n",
    "        gin_trained = None\n",
    "        train(gin, train_loader,lr=lr)\n",
    "\n",
    "with open('h_'+str(h)+'lr_'+str(lr)+'.txt', 'w') as f:\n",
    "        f.write(cap.stdout)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('h.txt', 'w') as f:\n",
    "        f.write(cap.stdout)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 5.20 | Test Acc: 65.39% | F Score: 0.44\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc, val_f1 = test(gin_trained, val_loader)\n",
    "print(f'Test Loss: {val_loss:.2f} | Test Acc: {val_acc*100:.2f}% | F Score: {val_f1:.2f}')\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
