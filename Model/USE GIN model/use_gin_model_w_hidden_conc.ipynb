{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from use_dataset import ProofDataset, HiddenConcProofDataset\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import random\n",
    "from sklearn.metrics import f1_score as f1\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, ReLU, BatchNorm1d, Sequential\n",
    "from torch_geometric.nn import GINConv\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use file_limit=5000 to only load and verify the first 5000 graphs (~60 MB)\n",
    "\n",
    "file_limit = 5000    # desired number of graphs to work with\n",
    "vocab_size = 1598   # number of characters in our vocabulary\n",
    "\n",
    "pf_data = ProofDataset(root=\"data/\",read_name=\"5000_relabeled_data_at_least_5.json\" , write_name=\"10000_relabeled_data_at_least_5_w_stmts.pt\" ,file_limit=file_limit)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make train/val/test for GIN\n",
    "\n",
    "# set seed for random # generation\n",
    "random.seed(10)\n",
    "length = file_limit\n",
    "total_indices = [i+file_limit for i in range(file_limit)]\n",
    "\n",
    "# create index vectors to filter dataset\n",
    "train_indices = [i for i in range(file_limit)]\n",
    "train_indices = train_indices + random.sample(total_indices, int(length*.8))\n",
    "train_indices.sort()\n",
    "\n",
    "val_index_options = [x for x in total_indices if x not in train_indices]\n",
    "val_indices = random.sample(val_index_options, int(length*.1))\n",
    "val_indices.sort()\n",
    "\n",
    "test_index_options = [x for x in total_indices if x not in train_indices if x not in val_indices]\n",
    "test_indices = random.sample(test_index_options, int(length*.1))\n",
    "test_indices.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our model is judged on how well it can predict the label for the final/conclusion node as seen in the val set created above\n",
    "# to not give our model too much info, we zero out the features on the final/conclusion node for each graph in the val set\n",
    "\n",
    "data_list = []\n",
    "\n",
    "for idx, graph in enumerate(pf_data):\n",
    "    # inherit features, labels, and edges\n",
    "    x = graph.x.clone()\n",
    "    y = graph.y.clone()\n",
    "    edge_index = graph.edge_index.clone()\n",
    "    x[-1] = torch.zeros(512) # zero out ALL conclusion nodes\n",
    "\n",
    "    #if idx in val_indices:\n",
    "        #x[-1] = torch.zeros(512)    # zero out features of final/conclusion node for each graph in val set\n",
    "\n",
    "# replace features of conlusion/final nodes from val set with average of neighboring node embeddings\n",
    "    #if idx in val_indices:\n",
    "\n",
    "        #connected_nodes = []\n",
    "        #sum = torch.zeros(512)\n",
    "\n",
    "        #for i, edge in enumerate(edge_index[0]): # case of outgoing edge coming from conc node\n",
    "            #if edge == (pf_data[idx].num_nodes-1):  \n",
    "                #connected_nodes.append(int(edge_index[1][i].item()))\n",
    "        #for i, edge in enumerate(edge_index[1]): # case of outgoing edge going to conc node\n",
    "            #if edge == (pf_data[idx].num_nodes-1):\n",
    "                #connected_nodes.append(int(edge_index[0][i].item()))\n",
    "\n",
    "        #for i in connected_nodes:\n",
    "            #if not torch.equal(pf_data[idx].x[i], pf_data[idx].x[-1]):\n",
    "            #sum += pf_data[idx].x[i]\n",
    "        #if len(connected_nodes) > 0:\n",
    "            #x[-1] = sum/(len(connected_nodes))\n",
    "        #else:\n",
    "            #x[-1] = sum\n",
    "\n",
    "    data_list.append(Data(x=x,y=y,edge_index=edge_index))\n",
    "\n",
    "hidden_conc_pf_data = HiddenConcProofDataset(root=\"data/\",read_name=\"5000_relabeled_data.json\",write_name=\"overwritten_labels\", data_list=data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set   = 9000 graphs\n",
      "Validation set = 500 graphs\n",
      "Test set       = 500 graphs\n"
     ]
    }
   ],
   "source": [
    "# Create training, validation, and test sets\n",
    "train_dataset = hidden_conc_pf_data[train_indices]\n",
    "val_dataset = hidden_conc_pf_data[val_indices]\n",
    "test_dataset = hidden_conc_pf_data[test_indices]\n",
    "\n",
    "print(f'Training set   = {len(train_dataset)} graphs')\n",
    "print(f'Validation set = {len(val_dataset)} graphs')\n",
    "print(f'Test set       = {len(test_dataset)} graphs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mini-batches\n",
    "# batch_size is number of graphs\n",
    "train_loader = DataLoader(train_dataset, batch_size=1000, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=500, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=500, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loader:\n",
      " - Batch 0: DataBatch(x=[3984, 512], edge_index=[2, 2984], y=[3984], batch=[3984], ptr=[1001])\n",
      " - Batch 1: DataBatch(x=[3876, 512], edge_index=[2, 2876], y=[3876], batch=[3876], ptr=[1001])\n",
      " - Batch 2: DataBatch(x=[4065, 512], edge_index=[2, 3065], y=[4065], batch=[4065], ptr=[1001])\n",
      " - Batch 3: DataBatch(x=[3737, 512], edge_index=[2, 2737], y=[3737], batch=[3737], ptr=[1001])\n",
      " - Batch 4: DataBatch(x=[3754, 512], edge_index=[2, 2754], y=[3754], batch=[3754], ptr=[1001])\n",
      " - Batch 5: DataBatch(x=[4068, 512], edge_index=[2, 3068], y=[4068], batch=[4068], ptr=[1001])\n",
      " - Batch 6: DataBatch(x=[3743, 512], edge_index=[2, 2743], y=[3743], batch=[3743], ptr=[1001])\n",
      " - Batch 7: DataBatch(x=[3860, 512], edge_index=[2, 2860], y=[3860], batch=[3860], ptr=[1001])\n",
      " - Batch 8: DataBatch(x=[3603, 512], edge_index=[2, 2603], y=[3603], batch=[3603], ptr=[1001])\n",
      "\n",
      "Validation loader:\n",
      " - Batch 0: DataBatch(x=[3207, 512], edge_index=[2, 2707], y=[3207], batch=[3207], ptr=[501])\n",
      "\n",
      "Test loader:\n",
      " - Batch 0: DataBatch(x=[3037, 512], edge_index=[2, 2537], y=[3037], batch=[3037], ptr=[501])\n"
     ]
    }
   ],
   "source": [
    "print('\\nTrain loader:')\n",
    "for i, batch in enumerate(train_loader):\n",
    "    print(f' - Batch {i}: {batch}')\n",
    "\n",
    "print('\\nValidation loader:')\n",
    "for i, batch in enumerate(val_loader):\n",
    "    print(f' - Batch {i}: {batch}')\n",
    "\n",
    "print('\\nTest loader:')\n",
    "for i, batch in enumerate(test_loader):\n",
    "    print(f' - Batch {i}: {batch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary to record label frequency\n",
    "\n",
    "# get max label used in hidden_conc_pf_data\n",
    "max_label = 0\n",
    "\n",
    "for i in range(file_limit*2):\n",
    "    for j in hidden_conc_pf_data.get(i).y:\n",
    "        if j > max_label:\n",
    "            max_label = j.to(int).item()\n",
    "\n",
    "# initialize histogram for labels used in hidden_conc_pf_data            \n",
    "label_count = {}\n",
    "\n",
    "for i in range(file_limit*2):\n",
    "    for j in range(max_label+1):        \n",
    "        label_count[j] = 0\n",
    "\n",
    "for i in range(file_limit*2):\n",
    "    for j in hidden_conc_pf_data.get(i).y:\n",
    "        label_count[j.to(int).item()] += 1\n",
    "\n",
    "step_count = 0\n",
    "max = 0\n",
    "max_freq_index = None   #find the most frequently used index\n",
    "labels_never_used = 0\n",
    "labels_used_once = 0\n",
    "labels_used_twice = 0\n",
    "\n",
    "\n",
    "for k,v in label_count.items():\n",
    "    step_count += v\n",
    "    if v > max:     \n",
    "        max = v\n",
    "        max_freq_index = k\n",
    "\n",
    "    if v == 0:\n",
    "        labels_never_used += 1\n",
    "    if v == 1:\n",
    "        labels_used_once += 1\n",
    "    if v ==2:\n",
    "        labels_used_twice += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of steps is: 40934\n",
      "highest frequency label is 554 and occurs 12143 times\n",
      "final label used is 554\n",
      "{0: 10280, 1: 545, 2: 52, 3: 24, 4: 270, 5: 14, 6: 40, 7: 42, 8: 390, 9: 26, 10: 28, 11: 245, 12: 23, 13: 57, 14: 12, 15: 21, 16: 108, 17: 47, 18: 75, 19: 14, 20: 18, 21: 26, 22: 17, 23: 13, 24: 18, 25: 17, 26: 12, 27: 15, 28: 50, 29: 18, 30: 13, 31: 14, 32: 12, 33: 42, 34: 12, 35: 20, 36: 13, 37: 14, 38: 18, 39: 22, 40: 29, 41: 83, 42: 25, 43: 12, 44: 127, 45: 60, 46: 27, 47: 83, 48: 130, 49: 100, 50: 24, 51: 24, 52: 54, 53: 115, 54: 29, 55: 25, 56: 38, 57: 59, 58: 78, 59: 59, 60: 23, 61: 29, 62: 125, 63: 41, 64: 44, 65: 11, 66: 17, 67: 12, 68: 32, 69: 15, 70: 19, 71: 21, 72: 25, 73: 19, 74: 20, 75: 33, 76: 52, 77: 25, 78: 13, 79: 13, 80: 26, 81: 16, 82: 17, 83: 474, 84: 39, 85: 74, 86: 172, 87: 73, 88: 20, 89: 20, 90: 67, 91: 21, 92: 61, 93: 24, 94: 21, 95: 28, 96: 45, 97: 107, 98: 15, 99: 16, 100: 39, 101: 242, 102: 37, 103: 12, 104: 113, 105: 24, 106: 11, 107: 60, 108: 14, 109: 41, 110: 11, 111: 13, 112: 32, 113: 22, 114: 49, 115: 17, 116: 11, 117: 13, 118: 51, 119: 27, 120: 13, 121: 16, 122: 69, 123: 25, 124: 49, 125: 14, 126: 11, 127: 27, 128: 13, 129: 12, 130: 21, 131: 16, 132: 22, 133: 18, 134: 144, 135: 36, 136: 29, 137: 193, 138: 39, 139: 26, 140: 12, 141: 45, 142: 38, 143: 62, 144: 11, 145: 14, 146: 32, 147: 16, 148: 11, 149: 22, 150: 22, 151: 110, 152: 122, 153: 95, 154: 72, 155: 12, 156: 13, 157: 15, 158: 12, 159: 20, 160: 78, 161: 26, 162: 17, 163: 26, 164: 13, 165: 11, 166: 13, 167: 30, 168: 14, 169: 28, 170: 19, 171: 43, 172: 24, 173: 20, 174: 20, 175: 17, 176: 11, 177: 26, 178: 18, 179: 82, 180: 63, 181: 66, 182: 44, 183: 48, 184: 92, 185: 13, 186: 17, 187: 12, 188: 16, 189: 30, 190: 16, 191: 14, 192: 24, 193: 13, 194: 13, 195: 12, 196: 13, 197: 14, 198: 18, 199: 43, 200: 11, 201: 18, 202: 59, 203: 17, 204: 22, 205: 17, 206: 12, 207: 39, 208: 28, 209: 42, 210: 14, 211: 12, 212: 26, 213: 11, 214: 41, 215: 37, 216: 13, 217: 25, 218: 22, 219: 18, 220: 11, 221: 25, 222: 57, 223: 11, 224: 24, 225: 14, 226: 12, 227: 18, 228: 14, 229: 18, 230: 19, 231: 44, 232: 44, 233: 47, 234: 17, 235: 16, 236: 22, 237: 17, 238: 21, 239: 24, 240: 23, 241: 26, 242: 44, 243: 11, 244: 33, 245: 35, 246: 25, 247: 13, 248: 15, 249: 11, 250: 27, 251: 12, 252: 11, 253: 19, 254: 11, 255: 46, 256: 43, 257: 11, 258: 30, 259: 21, 260: 19, 261: 18, 262: 38, 263: 34, 264: 39, 265: 20, 266: 26, 267: 34, 268: 18, 269: 27, 270: 61, 271: 24, 272: 12, 273: 157, 274: 18, 275: 11, 276: 25, 277: 18, 278: 30, 279: 22, 280: 129, 281: 24, 282: 35, 283: 29, 284: 40, 285: 19, 286: 19, 287: 19, 288: 16, 289: 11, 290: 15, 291: 55, 292: 18, 293: 85, 294: 16, 295: 49, 296: 14, 297: 322, 298: 12, 299: 19, 300: 18, 301: 50, 302: 33, 303: 37, 304: 26, 305: 35, 306: 16, 307: 14, 308: 19, 309: 14, 310: 27, 311: 21, 312: 13, 313: 18, 314: 13, 315: 24, 316: 11, 317: 21, 318: 24, 319: 17, 320: 13, 321: 24, 322: 12, 323: 15, 324: 34, 325: 12, 326: 32, 327: 22, 328: 17, 329: 13, 330: 12, 331: 42, 332: 30, 333: 50, 334: 43, 335: 18, 336: 22, 337: 32, 338: 48, 339: 39, 340: 31, 341: 15, 342: 15, 343: 15, 344: 11, 345: 33, 346: 13, 347: 20, 348: 16, 349: 41, 350: 15, 351: 19, 352: 11, 353: 11, 354: 18, 355: 34, 356: 21, 357: 12, 358: 24, 359: 19, 360: 16, 361: 12, 362: 25, 363: 16, 364: 17, 365: 13, 366: 25, 367: 17, 368: 20, 369: 13, 370: 33, 371: 12, 372: 12, 373: 14, 374: 28, 375: 13, 376: 51, 377: 26, 378: 11, 379: 25, 380: 35, 381: 21, 382: 23, 383: 17, 384: 37, 385: 55, 386: 22, 387: 76, 388: 27, 389: 13, 390: 27, 391: 34, 392: 38, 393: 21, 394: 14, 395: 20, 396: 78, 397: 27, 398: 57, 399: 18, 400: 11, 401: 54, 402: 33, 403: 27, 404: 34, 405: 34, 406: 15, 407: 54, 408: 28, 409: 49, 410: 17, 411: 28, 412: 71, 413: 13, 414: 45, 415: 96, 416: 49, 417: 25, 418: 18, 419: 16, 420: 11, 421: 11, 422: 20, 423: 66, 424: 23, 425: 137, 426: 21, 427: 18, 428: 11, 429: 18, 430: 12, 431: 19, 432: 11, 433: 16, 434: 43, 435: 28, 436: 11, 437: 11, 438: 17, 439: 21, 440: 125, 441: 91, 442: 44, 443: 43, 444: 80, 445: 17, 446: 22, 447: 62, 448: 21, 449: 12, 450: 15, 451: 38, 452: 13, 453: 13, 454: 18, 455: 15, 456: 15, 457: 13, 458: 18, 459: 17, 460: 42, 461: 14, 462: 16, 463: 11, 464: 14, 465: 11, 466: 14, 467: 86, 468: 22, 469: 30, 470: 60, 471: 15, 472: 28, 473: 13, 474: 11, 475: 13, 476: 19, 477: 26, 478: 41, 479: 23, 480: 32, 481: 14, 482: 17, 483: 26, 484: 41, 485: 29, 486: 35, 487: 16, 488: 38, 489: 13, 490: 59, 491: 53, 492: 11, 493: 45, 494: 12, 495: 29, 496: 13, 497: 23, 498: 11, 499: 30, 500: 13, 501: 22, 502: 15, 503: 11, 504: 16, 505: 11, 506: 19, 507: 12, 508: 22, 509: 45, 510: 45, 511: 13, 512: 15, 513: 12, 514: 14, 515: 12, 516: 38, 517: 12, 518: 17, 519: 11, 520: 21, 521: 11, 522: 19, 523: 11, 524: 11, 525: 15, 526: 20, 527: 15, 528: 23, 529: 12, 530: 14, 531: 43, 532: 44, 533: 15, 534: 28, 535: 27, 536: 12, 537: 12, 538: 19, 539: 17, 540: 11, 541: 12, 542: 24, 543: 20, 544: 19, 545: 12, 546: 11, 547: 13, 548: 15, 549: 19, 550: 50, 551: 21, 552: 24, 553: 21, 554: 12143}\n",
      "555 unique labels are used\n",
      "0 unique labels never used\n",
      "0 unique labels used once\n",
      "0 unique labels used twice\n"
     ]
    }
   ],
   "source": [
    "print(f\"total number of steps is:\", step_count)\n",
    "print(f\"highest frequency label is {max_freq_index} and occurs {max} times\")\n",
    "print(f\"final label used is {len(label_count)-1}\")\n",
    "print(label_count)\n",
    "print(len(label_count),\"unique labels are used\")\n",
    "print(labels_never_used,\"unique labels never used\")\n",
    "print(labels_used_once, \"unique labels used once\")\n",
    "print(labels_used_twice, \"unique labels used twice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make array of label frequencies for sklearn compute_class_weight using entire dataset\n",
    "# really should be doing this for train set (otherwise, data leakage...)\n",
    "# however, train set may not include certain labels, which leads to error in compute_class_weight\n",
    "\n",
    "# make array of unique classes\n",
    "#class_num_arr = [i for i in range(len(label_count))]\n",
    "#class_num_arr = np.array(class_num_arr)\n",
    "\n",
    "# make array of all data points with labels\n",
    "#lbl_arr = np.array([])\n",
    "#for i in range(file_limit*2):\n",
    "    #for y in hidden_conc_pf_data.get(i).y:\n",
    "        #lbl_arr = np.append(lbl_arr,[y.numpy()],axis=0).astype(int)\n",
    "\n",
    "#class_weights = compute_class_weight(class_weight=\"balanced\",classes = class_num_arr, y=lbl_arr)\n",
    "#class_weights = torch.from_numpy(class_weights).float().to(device)\n",
    "#class_weights[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIN(torch.nn.Module):\n",
    "    \"\"\"GIN\"\"\"\n",
    "    def __init__(self, dim_h, norm_mode='None', norm_scale=10):\n",
    "        super(GIN, self).__init__()\n",
    "        self.conv1 = GINConv(\n",
    "            Sequential(Linear(hidden_conc_pf_data.num_node_features, dim_h),\n",
    "                       BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()))\n",
    "        \n",
    "        self.conv2 = GINConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()))\n",
    "        \n",
    "        self.conv3 = GINConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()))\n",
    "        \n",
    "        self.lin1 = Linear(dim_h*3, dim_h*3)\n",
    "        self.lin2 = Linear(dim_h*3, hidden_conc_pf_data.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h1 = self.conv1(x, edge_index)\n",
    "        h2 = self.conv2(h1, edge_index)\n",
    "        h3 = self.conv3(h2, edge_index)\n",
    "\n",
    "        # Concatenate graph embeddings\n",
    "        h = torch.cat((h1, h2, h3), dim=1)\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "\n",
    "        h = self.lin1(h)\n",
    "        h = h.relu()\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "\n",
    "        h = self.lin2(h)        \n",
    "        return F.log_softmax(h, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GIN model training\n",
    "\n",
    "def train(model, loader, lr):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    # commented out code is to use class weights to account for imbalanced dataset\n",
    "    #criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "    epochs = 1000\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs+1):\n",
    "        total_loss = 0\n",
    "        acc = 0\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "        #cur_graph = 0   # used to keep track of current statement to enforce preds of only PREVIOUS labels in training\n",
    "\n",
    "        # Train on batches\n",
    "        for data in loader:       \n",
    "            #cur_graph += torch.max(data.batch) + 1\n",
    "            data = data.to(device, non_blocking=True)\n",
    "            data.y = data.y.to(torch.float).to(device, non_blocking=True)\n",
    "            optimizer.zero_grad()\n",
    "            length = len(loader)\n",
    "            out = model(data.x, data.edge_index.long())\n",
    "            data.y = data.y.type(torch.LongTensor).to(device, non_blocking=True)\n",
    "            out = out.type(torch.float32).to(device, non_blocking=True)\n",
    "            loss = criterion(out, data.y)\n",
    "            total_loss += loss / length\n",
    "\n",
    "            # commented out code below is meant to enforce predictions to only come from previous theorems\n",
    "            # for dict to be properly created, you must delete and recreate test.pt by rerunning \n",
    "            # pf_data = ProofDataset(root=\"data/\",file_limit=file_limit)\n",
    "            \n",
    "            #dict = pf_data.class_corr\n",
    "            #dict_keys = [k for k in dict.keys()]            \n",
    "\n",
    "            #def return_next_lowest_idx(num):\n",
    "                #if num in dict_keys:\n",
    "                    #return dict[num]\n",
    "                #while num not in dict_keys:\n",
    "                    #try:\n",
    "                        #num -= 1\n",
    "                        #return dict[num]\n",
    "                    #except:\n",
    "                        #pass\n",
    "\n",
    "            #with torch.no_grad():\n",
    "                #cur_graph_batch.apply_(return_next_lowest_idx)\n",
    "                #masked_lbls = (torch.arange(out.size(1)) < (cur_graph_batch[..., None]+1))*(out.cpu())\n",
    "                #try:\n",
    "                    #masked_lbls = (torch.arange(out.size(1)) < (cur_graph_batch[..., None]+1))*(out.cpu())\n",
    "                    #masked_lbls = torch.where(masked_lbls==0,np.nan,masked_lbls)\n",
    "                    #masked_lbls = masked_lbls.detach().numpy()                \n",
    "                    #pred = np.nanargmax(masked_lbls,axis=1)\n",
    "                    #pred = torch.from_numpy(pred)\n",
    "                    #acc += accuracy(pred, data.y.cpu()) / length\n",
    "                #except Exception as e:\n",
    "                    #print(\"a lil error\")\n",
    "                    #out = out + .00000001\n",
    "                    #masked_lbls = (torch.arange(out.size(1)) < (cur_graph_batch[..., None]+1))*(out.cpu())\n",
    "                    #masked_lbls = torch.where(masked_lbls==0,np.nan,masked_lbls)\n",
    "                    #masked_lbls = masked_lbls.detach().numpy()\n",
    "                    #pred = np.nanargmax(masked_lbls,axis=1)\n",
    "                    #pred = torch.from_numpy(pred)\n",
    "                    #acc += accuracy(pred, data.y.cpu()) / length\n",
    "                    #out = out - .00000001\n",
    "\n",
    "            #comment out the follow pred and acc lines if enforcing predictions as described above\n",
    "            pred = out.argmax(dim=1)\n",
    "            acc += accuracy(pred, data.y) / length\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # run model on validation set\n",
    "            val_loss, val_acc, val_f1, top5_acc = test(model, val_loader)\n",
    "\n",
    "        # Print metrics every epoch\n",
    "        if(epoch % 10 == 0):\n",
    "            print(f'Epoch {epoch:>3} | Train Loss: {total_loss:.2f} | Train Acc: {acc*100:>5.2f}% | Val Loss: {val_loss:.2f} | Val Acc: {val_acc*100:.2f}% | Top5 Val Acc: {top5_acc*100:.2f}%| F Score: {val_f1:.2f}')\n",
    "            \n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, loader):\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    top5_acc = 0\n",
    "    fscore = 0    \n",
    "    \n",
    "    for data in loader:\n",
    "        data = data.to(device, non_blocking=True)\n",
    "        length = len(loader)\n",
    "        out = model(data.x, data.edge_index.long())\n",
    "        prob = torch.exp(out)\n",
    "        prob_sorted = torch.topk(prob,k=5).indices\n",
    "        data.y = data.y.type(torch.LongTensor).to(device, non_blocking=True)\n",
    "        loss += criterion(out, data.y) / length\n",
    "        pred = out.argmax(dim=1)\n",
    "        acc += accuracy(pred, data.y) / length\n",
    "        top5_acc += torch.sum(torch.sum(prob_sorted==data.y.unsqueeze(1),dim=1),dim=0) / (length*(data.y.shape[0]))\n",
    "        fscore += f1(pred.cpu(), data.y.cpu(), average='macro')    # micro looks better, but macro prob more accurate\n",
    "\n",
    "    return loss, acc, fscore, top5_acc\n",
    "\n",
    "def accuracy(pred_y, y):\n",
    "    \"\"\"Calculate accuracy.\"\"\"\n",
    "    return ((pred_y == y).sum() / len(y)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GIN(\n",
       "  (conv1): GINConv(nn=Sequential(\n",
       "    (0): Linear(in_features=512, out_features=800, bias=True)\n",
       "    (1): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=800, out_features=800, bias=True)\n",
       "    (4): ReLU()\n",
       "  ))\n",
       "  (conv2): GINConv(nn=Sequential(\n",
       "    (0): Linear(in_features=800, out_features=800, bias=True)\n",
       "    (1): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=800, out_features=800, bias=True)\n",
       "    (4): ReLU()\n",
       "  ))\n",
       "  (conv3): GINConv(nn=Sequential(\n",
       "    (0): Linear(in_features=800, out_features=800, bias=True)\n",
       "    (1): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=800, out_features=800, bias=True)\n",
       "    (4): ReLU()\n",
       "  ))\n",
       "  (conv4): GINConv(nn=Sequential(\n",
       "    (0): Linear(in_features=800, out_features=800, bias=True)\n",
       "    (1): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=800, out_features=800, bias=True)\n",
       "    (4): ReLU()\n",
       "  ))\n",
       "  (lin1): Linear(in_features=3200, out_features=3200, bias=True)\n",
       "  (lin2): Linear(in_features=3200, out_features=555, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize (and reset weights of) model\n",
    "# dim_h is hyperparameter of number of hidden layers\n",
    "\n",
    "gin_trained = None\n",
    "gin = None\n",
    "gin = GIN(dim_h=800).to(device)\n",
    "gin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | Train Loss: 1093.11 | Train Acc: 24.38% | Val Loss: 4.67 | Val Acc: 17.15% | Top3 Val Acc: 0.4268786907196045%| F Score: 0.00\n",
      "Epoch  10 | Train Loss: 2.60 | Train Acc: 56.86% | Val Loss: 3.55 | Val Acc: 41.00% | Top3 Val Acc: 0.4855004549026489%| F Score: 0.00\n",
      "Epoch  20 | Train Loss: 2.27 | Train Acc: 57.74% | Val Loss: 3.09 | Val Acc: 41.69% | Top3 Val Acc: 0.5132522583007812%| F Score: 0.01\n",
      "Epoch  30 | Train Loss: 1.99 | Train Acc: 58.82% | Val Loss: 2.83 | Val Acc: 42.81% | Top3 Val Acc: 0.547552227973938%| F Score: 0.01\n",
      "Epoch  40 | Train Loss: 1.91 | Train Acc: 58.80% | Val Loss: 2.72 | Val Acc: 42.56% | Top3 Val Acc: 0.5768631100654602%| F Score: 0.02\n",
      "Epoch  50 | Train Loss: 1.65 | Train Acc: 60.59% | Val Loss: 2.49 | Val Acc: 44.56% | Top3 Val Acc: 0.6092921495437622%| F Score: 0.05\n",
      "Epoch  60 | Train Loss: 1.55 | Train Acc: 61.57% | Val Loss: 2.31 | Val Acc: 45.84% | Top3 Val Acc: 0.6345493793487549%| F Score: 0.09\n",
      "Epoch  70 | Train Loss: 1.43 | Train Acc: 62.92% | Val Loss: 2.19 | Val Acc: 46.93% | Top3 Val Acc: 0.6772684454917908%| F Score: 0.08\n",
      "Epoch  80 | Train Loss: 1.38 | Train Acc: 63.87% | Val Loss: 2.16 | Val Acc: 48.61% | Top3 Val Acc: 0.6838166117668152%| F Score: 0.12\n",
      "Epoch  90 | Train Loss: 1.22 | Train Acc: 66.97% | Val Loss: 1.94 | Val Acc: 52.39% | Top3 Val Acc: 0.715622067451477%| F Score: 0.18\n",
      "Epoch 100 | Train Loss: 1.29 | Train Acc: 66.03% | Val Loss: 2.00 | Val Acc: 52.17% | Top3 Val Acc: 0.7171811461448669%| F Score: 0.18\n",
      "Epoch 110 | Train Loss: 1.01 | Train Acc: 71.24% | Val Loss: 2.01 | Val Acc: 50.55% | Top3 Val Acc: 0.7090739011764526%| F Score: 0.21\n",
      "Epoch 120 | Train Loss: 1.05 | Train Acc: 69.97% | Val Loss: 2.05 | Val Acc: 51.14% | Top3 Val Acc: 0.7125038504600525%| F Score: 0.22\n",
      "Epoch 130 | Train Loss: 0.86 | Train Acc: 74.38% | Val Loss: 2.16 | Val Acc: 51.82% | Top3 Val Acc: 0.7140629887580872%| F Score: 0.21\n",
      "Epoch 140 | Train Loss: 0.76 | Train Acc: 77.33% | Val Loss: 1.90 | Val Acc: 54.35% | Top3 Val Acc: 0.7539756298065186%| F Score: 0.26\n",
      "Epoch 150 | Train Loss: 0.67 | Train Acc: 79.89% | Val Loss: 2.06 | Val Acc: 56.22% | Top3 Val Acc: 0.759588360786438%| F Score: 0.29\n",
      "Epoch 160 | Train Loss: 0.62 | Train Acc: 80.92% | Val Loss: 2.05 | Val Acc: 57.90% | Top3 Val Acc: 0.7773619890213013%| F Score: 0.34\n",
      "Epoch 170 | Train Loss: 0.65 | Train Acc: 79.89% | Val Loss: 2.06 | Val Acc: 58.81% | Top3 Val Acc: 0.7751792669296265%| F Score: 0.31\n",
      "Epoch 180 | Train Loss: 0.72 | Train Acc: 79.03% | Val Loss: 2.64 | Val Acc: 49.30% | Top3 Val Acc: 0.7178047895431519%| F Score: 0.23\n",
      "Epoch 190 | Train Loss: 0.41 | Train Acc: 87.82% | Val Loss: 2.20 | Val Acc: 60.52% | Top3 Val Acc: 0.7829747200012207%| F Score: 0.39\n",
      "Epoch 200 | Train Loss: 0.33 | Train Acc: 90.52% | Val Loss: 2.32 | Val Acc: 61.68% | Top3 Val Acc: 0.7845337986946106%| F Score: 0.37\n",
      "Epoch 210 | Train Loss: 0.34 | Train Acc: 90.38% | Val Loss: 2.28 | Val Acc: 63.11% | Top3 Val Acc: 0.7976301312446594%| F Score: 0.41\n",
      "Epoch 220 | Train Loss: 0.33 | Train Acc: 90.85% | Val Loss: 2.20 | Val Acc: 63.11% | Top3 Val Acc: 0.7920174598693848%| F Score: 0.41\n",
      "Epoch 230 | Train Loss: 0.25 | Train Acc: 92.75% | Val Loss: 2.76 | Val Acc: 61.74% | Top3 Val Acc: 0.7913938164710999%| F Score: 0.37\n",
      "Epoch 240 | Train Loss: 0.36 | Train Acc: 90.02% | Val Loss: 2.34 | Val Acc: 63.24% | Top3 Val Acc: 0.7932646870613098%| F Score: 0.42\n",
      "Epoch 250 | Train Loss: 0.22 | Train Acc: 93.92% | Val Loss: 2.59 | Val Acc: 63.80% | Top3 Val Acc: 0.7954474091529846%| F Score: 0.42\n",
      "Epoch 260 | Train Loss: 0.25 | Train Acc: 93.26% | Val Loss: 2.54 | Val Acc: 63.49% | Top3 Val Acc: 0.7932646870613098%| F Score: 0.41\n",
      "Epoch 270 | Train Loss: 1.12 | Train Acc: 70.91% | Val Loss: 2.17 | Val Acc: 61.30% | Top3 Val Acc: 0.7817274332046509%| F Score: 0.37\n",
      "Epoch 280 | Train Loss: 1.15 | Train Acc: 72.12% | Val Loss: 2.13 | Val Acc: 59.28% | Top3 Val Acc: 0.7680074572563171%| F Score: 0.38\n",
      "Epoch 290 | Train Loss: 0.21 | Train Acc: 94.01% | Val Loss: 2.62 | Val Acc: 63.42% | Top3 Val Acc: 0.7876519560813904%| F Score: 0.41\n",
      "Epoch 300 | Train Loss: 0.29 | Train Acc: 92.00% | Val Loss: 2.58 | Val Acc: 63.08% | Top3 Val Acc: 0.79014652967453%| F Score: 0.40\n",
      "Epoch 310 | Train Loss: 0.19 | Train Acc: 94.47% | Val Loss: 3.16 | Val Acc: 63.27% | Top3 Val Acc: 0.7888992428779602%| F Score: 0.40\n",
      "Epoch 320 | Train Loss: 0.18 | Train Acc: 94.48% | Val Loss: 3.08 | Val Acc: 64.17% | Top3 Val Acc: 0.79014652967453%| F Score: 0.41\n",
      "Epoch 330 | Train Loss: 0.21 | Train Acc: 94.09% | Val Loss: 2.77 | Val Acc: 63.64% | Top3 Val Acc: 0.7895228862762451%| F Score: 0.41\n",
      "Epoch 340 | Train Loss: 0.18 | Train Acc: 94.61% | Val Loss: 3.23 | Val Acc: 63.74% | Top3 Val Acc: 0.7935765385627747%| F Score: 0.42\n",
      "Epoch 350 | Train Loss: 0.18 | Train Acc: 94.82% | Val Loss: 3.12 | Val Acc: 64.14% | Top3 Val Acc: 0.7898346781730652%| F Score: 0.41\n",
      "Epoch 360 | Train Loss: 1.37 | Train Acc: 69.64% | Val Loss: 2.20 | Val Acc: 57.37% | Top3 Val Acc: 0.749610185623169%| F Score: 0.31\n",
      "Epoch 370 | Train Loss: 0.17 | Train Acc: 94.73% | Val Loss: 3.24 | Val Acc: 63.74% | Top3 Val Acc: 0.7870283722877502%| F Score: 0.41\n",
      "Epoch 380 | Train Loss: 0.17 | Train Acc: 94.84% | Val Loss: 3.12 | Val Acc: 63.70% | Top3 Val Acc: 0.7923292517662048%| F Score: 0.41\n",
      "Epoch 390 | Train Loss: 0.17 | Train Acc: 94.79% | Val Loss: 3.45 | Val Acc: 63.52% | Top3 Val Acc: 0.7904583215713501%| F Score: 0.41\n",
      "Epoch 400 | Train Loss: 0.17 | Train Acc: 94.81% | Val Loss: 3.24 | Val Acc: 64.48% | Top3 Val Acc: 0.7923292517662048%| F Score: 0.43\n",
      "Epoch 410 | Train Loss: 0.17 | Train Acc: 94.82% | Val Loss: 3.35 | Val Acc: 63.77% | Top3 Val Acc: 0.7873401641845703%| F Score: 0.41\n",
      "Epoch 420 | Train Loss: 0.17 | Train Acc: 94.85% | Val Loss: 3.34 | Val Acc: 64.23% | Top3 Val Acc: 0.7876519560813904%| F Score: 0.41\n",
      "Epoch 430 | Train Loss: 0.76 | Train Acc: 80.03% | Val Loss: 2.42 | Val Acc: 62.77% | Top3 Val Acc: 0.7851574420928955%| F Score: 0.41\n",
      "Epoch 440 | Train Loss: 0.17 | Train Acc: 94.86% | Val Loss: 3.37 | Val Acc: 63.74% | Top3 Val Acc: 0.7879638075828552%| F Score: 0.41\n",
      "Epoch 450 | Train Loss: 0.20 | Train Acc: 94.23% | Val Loss: 3.01 | Val Acc: 63.95% | Top3 Val Acc: 0.7888992428779602%| F Score: 0.42\n",
      "Epoch 460 | Train Loss: 0.17 | Train Acc: 94.94% | Val Loss: 3.46 | Val Acc: 64.08% | Top3 Val Acc: 0.7882755994796753%| F Score: 0.42\n",
      "Epoch 470 | Train Loss: 0.16 | Train Acc: 94.93% | Val Loss: 3.43 | Val Acc: 63.70% | Top3 Val Acc: 0.7895228862762451%| F Score: 0.41\n",
      "Epoch 480 | Train Loss: 0.17 | Train Acc: 94.92% | Val Loss: 3.39 | Val Acc: 63.49% | Top3 Val Acc: 0.7842220067977905%| F Score: 0.41\n",
      "Epoch 490 | Train Loss: 0.17 | Train Acc: 94.84% | Val Loss: 3.58 | Val Acc: 63.27% | Top3 Val Acc: 0.7870283722877502%| F Score: 0.41\n",
      "Epoch 500 | Train Loss: 0.18 | Train Acc: 94.69% | Val Loss: 3.30 | Val Acc: 63.45% | Top3 Val Acc: 0.7873401641845703%| F Score: 0.42\n",
      "Epoch 510 | Train Loss: 0.16 | Train Acc: 94.93% | Val Loss: 3.69 | Val Acc: 63.36% | Top3 Val Acc: 0.7867165207862854%| F Score: 0.41\n",
      "Epoch 520 | Train Loss: 0.16 | Train Acc: 94.92% | Val Loss: 3.62 | Val Acc: 63.36% | Top3 Val Acc: 0.7904583215713501%| F Score: 0.42\n",
      "Epoch 530 | Train Loss: 0.16 | Train Acc: 94.89% | Val Loss: 3.55 | Val Acc: 63.36% | Top3 Val Acc: 0.7864047288894653%| F Score: 0.41\n",
      "Epoch 540 | Train Loss: 0.16 | Train Acc: 94.97% | Val Loss: 3.76 | Val Acc: 63.36% | Top3 Val Acc: 0.7835983633995056%| F Score: 0.41\n",
      "Epoch 550 | Train Loss: 0.19 | Train Acc: 94.44% | Val Loss: 3.16 | Val Acc: 63.58% | Top3 Val Acc: 0.7904583215713501%| F Score: 0.41\n",
      "Epoch 560 | Train Loss: 0.16 | Train Acc: 94.94% | Val Loss: 3.78 | Val Acc: 63.67% | Top3 Val Acc: 0.7888992428779602%| F Score: 0.41\n",
      "Epoch 570 | Train Loss: 1.05 | Train Acc: 75.78% | Val Loss: 2.42 | Val Acc: 61.37% | Top3 Val Acc: 0.7729965448379517%| F Score: 0.38\n",
      "Epoch 580 | Train Loss: 0.16 | Train Acc: 94.90% | Val Loss: 3.80 | Val Acc: 63.61% | Top3 Val Acc: 0.7835983633995056%| F Score: 0.41\n",
      "Epoch 590 | Train Loss: 0.16 | Train Acc: 94.92% | Val Loss: 3.92 | Val Acc: 63.95% | Top3 Val Acc: 0.7820392847061157%| F Score: 0.41\n",
      "Epoch 600 | Train Loss: 0.16 | Train Acc: 94.93% | Val Loss: 3.69 | Val Acc: 63.92% | Top3 Val Acc: 0.7851574420928955%| F Score: 0.42\n",
      "Epoch 610 | Train Loss: 0.16 | Train Acc: 95.04% | Val Loss: 4.07 | Val Acc: 64.20% | Top3 Val Acc: 0.7817274332046509%| F Score: 0.42\n",
      "Epoch 620 | Train Loss: 0.29 | Train Acc: 91.25% | Val Loss: 3.14 | Val Acc: 63.89% | Top3 Val Acc: 0.781103789806366%| F Score: 0.41\n",
      "Epoch 630 | Train Loss: 0.16 | Train Acc: 94.97% | Val Loss: 3.92 | Val Acc: 63.99% | Top3 Val Acc: 0.7829747200012207%| F Score: 0.41\n",
      "Epoch 640 | Train Loss: 0.16 | Train Acc: 95.01% | Val Loss: 4.14 | Val Acc: 63.74% | Top3 Val Acc: 0.7839101552963257%| F Score: 0.40\n",
      "Epoch 650 | Train Loss: 0.16 | Train Acc: 95.03% | Val Loss: 4.04 | Val Acc: 63.36% | Top3 Val Acc: 0.779232919216156%| F Score: 0.40\n",
      "Epoch 660 | Train Loss: 0.20 | Train Acc: 93.87% | Val Loss: 3.49 | Val Acc: 63.64% | Top3 Val Acc: 0.7814156413078308%| F Score: 0.41\n",
      "Epoch 670 | Train Loss: 0.16 | Train Acc: 95.05% | Val Loss: 4.12 | Val Acc: 63.64% | Top3 Val Acc: 0.781103789806366%| F Score: 0.41\n",
      "Epoch 680 | Train Loss: 1.05 | Train Acc: 75.05% | Val Loss: 2.98 | Val Acc: 63.02% | Top3 Val Acc: 0.7776737809181213%| F Score: 0.40\n",
      "Epoch 690 | Train Loss: 0.16 | Train Acc: 95.07% | Val Loss: 4.13 | Val Acc: 63.52% | Top3 Val Acc: 0.7851574420928955%| F Score: 0.41\n",
      "Epoch 700 | Train Loss: 0.16 | Train Acc: 95.11% | Val Loss: 4.06 | Val Acc: 63.64% | Top3 Val Acc: 0.7839101552963257%| F Score: 0.41\n",
      "Epoch 710 | Train Loss: 0.21 | Train Acc: 93.78% | Val Loss: 3.50 | Val Acc: 63.58% | Top3 Val Acc: 0.7845337986946106%| F Score: 0.41\n",
      "Epoch 720 | Train Loss: 0.16 | Train Acc: 95.06% | Val Loss: 4.20 | Val Acc: 63.45% | Top3 Val Acc: 0.7826628684997559%| F Score: 0.41\n",
      "Epoch 730 | Train Loss: 0.53 | Train Acc: 85.39% | Val Loss: 3.29 | Val Acc: 63.21% | Top3 Val Acc: 0.7767383456230164%| F Score: 0.40\n",
      "Epoch 740 | Train Loss: 0.16 | Train Acc: 95.09% | Val Loss: 4.25 | Val Acc: 63.61% | Top3 Val Acc: 0.7854692339897156%| F Score: 0.41\n",
      "Epoch 750 | Train Loss: 0.16 | Train Acc: 95.08% | Val Loss: 4.41 | Val Acc: 63.17% | Top3 Val Acc: 0.7820392847061157%| F Score: 0.41\n",
      "Epoch 760 | Train Loss: 0.16 | Train Acc: 95.06% | Val Loss: 4.23 | Val Acc: 63.27% | Top3 Val Acc: 0.7842220067977905%| F Score: 0.40\n",
      "Epoch 770 | Train Loss: 0.16 | Train Acc: 95.04% | Val Loss: 4.49 | Val Acc: 63.52% | Top3 Val Acc: 0.7835983633995056%| F Score: 0.40\n",
      "Epoch 780 | Train Loss: 0.61 | Train Acc: 83.11% | Val Loss: 3.05 | Val Acc: 63.33% | Top3 Val Acc: 0.7758029103279114%| F Score: 0.41\n",
      "Epoch 790 | Train Loss: 0.16 | Train Acc: 95.07% | Val Loss: 4.22 | Val Acc: 63.49% | Top3 Val Acc: 0.780480146408081%| F Score: 0.41\n",
      "Epoch 800 | Train Loss: 0.16 | Train Acc: 95.06% | Val Loss: 4.47 | Val Acc: 63.14% | Top3 Val Acc: 0.7835983633995056%| F Score: 0.41\n",
      "Epoch 810 | Train Loss: 0.16 | Train Acc: 95.07% | Val Loss: 4.32 | Val Acc: 63.36% | Top3 Val Acc: 0.7782974243164062%| F Score: 0.42\n",
      "Epoch 820 | Train Loss: 0.16 | Train Acc: 95.02% | Val Loss: 3.97 | Val Acc: 63.83% | Top3 Val Acc: 0.7776737809181213%| F Score: 0.42\n",
      "Epoch 830 | Train Loss: 0.16 | Train Acc: 95.09% | Val Loss: 4.40 | Val Acc: 63.24% | Top3 Val Acc: 0.7839101552963257%| F Score: 0.41\n",
      "Epoch 840 | Train Loss: 0.17 | Train Acc: 94.94% | Val Loss: 3.77 | Val Acc: 62.15% | Top3 Val Acc: 0.7720611095428467%| F Score: 0.40\n",
      "Epoch 850 | Train Loss: 0.15 | Train Acc: 95.11% | Val Loss: 4.39 | Val Acc: 63.42% | Top3 Val Acc: 0.7848456501960754%| F Score: 0.41\n",
      "Epoch 860 | Train Loss: 0.15 | Train Acc: 95.10% | Val Loss: 4.65 | Val Acc: 63.27% | Top3 Val Acc: 0.7820392847061157%| F Score: 0.40\n",
      "Epoch 870 | Train Loss: 0.16 | Train Acc: 95.08% | Val Loss: 4.15 | Val Acc: 63.55% | Top3 Val Acc: 0.7842220067977905%| F Score: 0.41\n",
      "Epoch 880 | Train Loss: 0.15 | Train Acc: 95.12% | Val Loss: 4.61 | Val Acc: 63.55% | Top3 Val Acc: 0.7854692339897156%| F Score: 0.41\n",
      "Epoch 890 | Train Loss: 0.15 | Train Acc: 95.12% | Val Loss: 4.49 | Val Acc: 64.20% | Top3 Val Acc: 0.7851574420928955%| F Score: 0.41\n",
      "Epoch 900 | Train Loss: 0.15 | Train Acc: 95.08% | Val Loss: 4.64 | Val Acc: 64.02% | Top3 Val Acc: 0.7823510766029358%| F Score: 0.41\n",
      "Epoch 910 | Train Loss: 0.16 | Train Acc: 94.97% | Val Loss: 4.21 | Val Acc: 63.99% | Top3 Val Acc: 0.780480146408081%| F Score: 0.42\n",
      "Epoch 920 | Train Loss: 0.16 | Train Acc: 95.09% | Val Loss: 4.37 | Val Acc: 63.83% | Top3 Val Acc: 0.7779856324195862%| F Score: 0.40\n",
      "Epoch 930 | Train Loss: 0.15 | Train Acc: 95.10% | Val Loss: 4.68 | Val Acc: 63.55% | Top3 Val Acc: 0.7761147022247314%| F Score: 0.40\n",
      "Epoch 940 | Train Loss: 0.16 | Train Acc: 95.03% | Val Loss: 4.67 | Val Acc: 63.39% | Top3 Val Acc: 0.7814156413078308%| F Score: 0.40\n",
      "Epoch 950 | Train Loss: 0.35 | Train Acc: 89.78% | Val Loss: 3.87 | Val Acc: 63.58% | Top3 Val Acc: 0.7779856324195862%| F Score: 0.42\n",
      "Epoch 960 | Train Loss: 0.15 | Train Acc: 95.17% | Val Loss: 4.88 | Val Acc: 63.42% | Top3 Val Acc: 0.7789210677146912%| F Score: 0.40\n",
      "Epoch 970 | Train Loss: 0.16 | Train Acc: 95.07% | Val Loss: 4.69 | Val Acc: 63.52% | Top3 Val Acc: 0.780480146408081%| F Score: 0.41\n",
      "Epoch 980 | Train Loss: 0.15 | Train Acc: 95.11% | Val Loss: 4.38 | Val Acc: 63.92% | Top3 Val Acc: 0.780168354511261%| F Score: 0.42\n",
      "Epoch 990 | Train Loss: 0.15 | Train Acc: 95.11% | Val Loss: 4.92 | Val Acc: 63.42% | Top3 Val Acc: 0.7782974243164062%| F Score: 0.41\n",
      "Epoch 1000 | Train Loss: 0.15 | Train Acc: 95.08% | Val Loss: 4.35 | Val Acc: 64.08% | Top3 Val Acc: 0.7854692339897156%| F Score: 0.42\n"
     ]
    }
   ],
   "source": [
    "# reset weights and train model\n",
    "gin_trained = None\n",
    "gin_trained = train(gin, train_loader,lr=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# get accuracy only for conclusion node from val set\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m total_conc_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mval_dataset\u001b[49m)\n\u001b[0;32m      4\u001b[0m correct_conc_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# dict of the form true_label:(predicted_labels)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'val_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# get accuracy only for conclusion node from val set\n",
    "\n",
    "total_conc_labels = len(val_dataset)\n",
    "correct_conc_pred = 0\n",
    "\n",
    "# dict of the form true_label:(predicted_labels)\n",
    "incorrect_preds = {}\n",
    "\n",
    "for graph in val_dataset:\n",
    "    get_gin_predict = gin_trained(graph.x.to(device),graph.edge_index.long().to(device))\n",
    "    get_conc_predict = np.argmax(get_gin_predict[-1].detach().cpu().numpy(),axis=0)\n",
    "    if get_conc_predict == graph.y[-1].item():\n",
    "        correct_conc_pred += 1\n",
    "    if get_conc_predict != graph.y[-1].item():\n",
    "        if graph.y[-1].item() in incorrect_preds:\n",
    "            incorrect_preds[graph.y[-1].item()] = incorrect_preds[graph.y[-1].item()]+(get_conc_predict,)\n",
    "        else:\n",
    "            incorrect_preds[graph.y[-1].item()] = (get_conc_predict,)\n",
    "\n",
    "\n",
    "def get_conc_gin_label_acc(i):\n",
    "    correct_pred_i = 0\n",
    "    total_num_i = 0\n",
    "\n",
    "    for graph in val_dataset:\n",
    "        get_gin_predict = gin_trained(graph.x.to(device),graph.edge_index.long().to(device))\n",
    "        get_conc_predict = np.argmax(get_gin_predict[-1].detach().cpu().numpy(),axis=0)\n",
    "        if graph.y[-1].item() == i:\n",
    "            total_num_i += 1\n",
    "            if get_conc_predict == i:\n",
    "                correct_pred_i += 1\n",
    "    if total_num_i == 0:\n",
    "        return 0,0,0\n",
    "    return (correct_pred_i / total_num_i), total_num_i, correct_pred_i\n",
    "\n",
    "def get_topk_acc(k=3):\n",
    "\n",
    "    top3_corr = 0\n",
    "\n",
    "    for graph in val_dataset:\n",
    "        get_gin_predict = gin_trained(graph.x.to(device),graph.edge_index.long().to(device))\n",
    "        prob = torch.exp(get_gin_predict[-1])\n",
    "        prob_sorted = torch.topk(prob,k=k).indices\n",
    "        graph.y = graph.y.to(device)\n",
    "        top3_corr += torch.sum(prob_sorted==graph.y[-1])\n",
    "\n",
    "    return top3_corr / len(val_dataset)\n",
    "\n",
    "k=10\n",
    "print(f\"restricting to conclusion nodes, our val_graph has accuracy being {correct_conc_pred/total_conc_labels*100:.2f}\")\n",
    "print(f\"'unk' accuracy should be: {get_conc_gin_label_acc(554)[0]*100:.2f}\")\n",
    "print(f\"top{k} accuracy is: {get_topk_acc(3).item()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary to record label frequency based on val set\n",
    "# only consider conclusion/final nodes\n",
    "\n",
    "# get max label used in pf_data\n",
    "max_label = 0\n",
    "\n",
    "for i in val_indices:\n",
    "    if hidden_conc_pf_data.get(i).y[-1] > max_label:\n",
    "        max_label = hidden_conc_pf_data.get(i).y[-1].to(int).item()\n",
    "\n",
    "# initialize histogram for labels used in pf_data            \n",
    "label_count = {}\n",
    "\n",
    "for i in val_indices:\n",
    "    for j in range(max_label+1):        \n",
    "        label_count[j] = 0\n",
    "\n",
    "for i in val_indices:\n",
    "    label_count[hidden_conc_pf_data.get(i).y[-1].to(int).item()] += 1\n",
    "\n",
    "step_count = 0\n",
    "max = 0\n",
    "max_freq_index = None   # find the most frequently used index\n",
    "labels_never_used = 0\n",
    "labels_used_once = 0\n",
    "labels_used_twice = 0\n",
    "\n",
    "\n",
    "for k,v in label_count.items():\n",
    "    step_count += v\n",
    "    if v > max:     \n",
    "        max = v\n",
    "        max_freq_index = k\n",
    "\n",
    "    if v == 0:\n",
    "        labels_never_used += 1\n",
    "    if v == 1:\n",
    "        labels_used_once += 1\n",
    "    if v ==2:\n",
    "        labels_used_twice += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of steps is: 500\n",
      "highest frequency label is 554 and occurs 115 times\n",
      "final label used is 554\n",
      "{0: 2, 1: 22, 2: 0, 3: 1, 4: 3, 5: 1, 6: 2, 7: 0, 8: 19, 9: 0, 10: 2, 11: 1, 12: 0, 13: 0, 14: 0, 15: 0, 16: 4, 17: 1, 18: 2, 19: 0, 20: 0, 21: 0, 22: 1, 23: 1, 24: 0, 25: 3, 26: 1, 27: 0, 28: 0, 29: 0, 30: 1, 31: 1, 32: 0, 33: 0, 34: 0, 35: 2, 36: 0, 37: 0, 38: 0, 39: 0, 40: 1, 41: 10, 42: 1, 43: 0, 44: 15, 45: 7, 46: 0, 47: 1, 48: 4, 49: 3, 50: 0, 51: 0, 52: 2, 53: 6, 54: 2, 55: 3, 56: 0, 57: 0, 58: 7, 59: 2, 60: 0, 61: 0, 62: 8, 63: 0, 64: 2, 65: 0, 66: 1, 67: 0, 68: 0, 69: 0, 70: 0, 71: 1, 72: 1, 73: 0, 74: 3, 75: 3, 76: 0, 77: 0, 78: 0, 79: 0, 80: 2, 81: 2, 82: 0, 83: 28, 84: 2, 85: 1, 86: 7, 87: 7, 88: 1, 89: 1, 90: 1, 91: 1, 92: 1, 93: 0, 94: 1, 95: 0, 96: 0, 97: 6, 98: 2, 99: 0, 100: 1, 101: 18, 102: 0, 103: 0, 104: 5, 105: 0, 106: 1, 107: 0, 108: 0, 109: 0, 110: 2, 111: 2, 112: 0, 113: 0, 114: 0, 115: 0, 116: 0, 117: 0, 118: 0, 119: 1, 120: 0, 121: 0, 122: 0, 123: 1, 124: 0, 125: 0, 126: 1, 127: 3, 128: 0, 129: 0, 130: 3, 131: 0, 132: 0, 133: 0, 134: 5, 135: 1, 136: 2, 137: 1, 138: 0, 139: 0, 140: 1, 141: 0, 142: 1, 143: 0, 144: 1, 145: 0, 146: 0, 147: 0, 148: 0, 149: 0, 150: 0, 151: 1, 152: 0, 153: 0, 154: 0, 155: 0, 156: 0, 157: 2, 158: 2, 159: 1, 160: 0, 161: 0, 162: 0, 163: 0, 164: 0, 165: 0, 166: 0, 167: 1, 168: 0, 169: 2, 170: 2, 171: 3, 172: 1, 173: 0, 174: 2, 175: 0, 176: 0, 177: 0, 178: 0, 179: 0, 180: 0, 181: 0, 182: 0, 183: 0, 184: 1, 185: 0, 186: 0, 187: 0, 188: 2, 189: 5, 190: 0, 191: 0, 192: 2, 193: 4, 194: 0, 195: 0, 196: 0, 197: 0, 198: 3, 199: 0, 200: 0, 201: 0, 202: 2, 203: 1, 204: 1, 205: 0, 206: 0, 207: 0, 208: 1, 209: 0, 210: 1, 211: 0, 212: 0, 213: 0, 214: 0, 215: 0, 216: 0, 217: 0, 218: 0, 219: 0, 220: 0, 221: 0, 222: 0, 223: 0, 224: 0, 225: 0, 226: 0, 227: 0, 228: 1, 229: 0, 230: 1, 231: 5, 232: 3, 233: 3, 234: 0, 235: 0, 236: 0, 237: 1, 238: 0, 239: 0, 240: 0, 241: 0, 242: 2, 243: 0, 244: 0, 245: 0, 246: 0, 247: 0, 248: 0, 249: 0, 250: 1, 251: 0, 252: 0, 253: 0, 254: 0, 255: 0, 256: 0, 257: 0, 258: 1, 259: 0, 260: 0, 261: 0, 262: 0, 263: 0, 264: 0, 265: 0, 266: 0, 267: 2, 268: 1, 269: 0, 270: 0, 271: 0, 272: 0, 273: 1, 274: 0, 275: 0, 276: 0, 277: 0, 278: 0, 279: 0, 280: 1, 281: 0, 282: 4, 283: 3, 284: 1, 285: 0, 286: 0, 287: 0, 288: 0, 289: 0, 290: 0, 291: 0, 292: 0, 293: 0, 294: 0, 295: 0, 296: 0, 297: 0, 298: 0, 299: 0, 300: 0, 301: 0, 302: 0, 303: 0, 304: 0, 305: 0, 306: 0, 307: 0, 308: 0, 309: 0, 310: 0, 311: 0, 312: 0, 313: 0, 314: 0, 315: 0, 316: 0, 317: 0, 318: 0, 319: 0, 320: 0, 321: 0, 322: 0, 323: 0, 324: 0, 325: 0, 326: 0, 327: 0, 328: 1, 329: 0, 330: 0, 331: 0, 332: 0, 333: 0, 334: 0, 335: 0, 336: 0, 337: 0, 338: 0, 339: 0, 340: 0, 341: 0, 342: 0, 343: 0, 344: 0, 345: 0, 346: 0, 347: 0, 348: 0, 349: 0, 350: 0, 351: 0, 352: 0, 353: 0, 354: 0, 355: 0, 356: 0, 357: 0, 358: 0, 359: 1, 360: 0, 361: 0, 362: 0, 363: 0, 364: 0, 365: 0, 366: 0, 367: 0, 368: 0, 369: 0, 370: 0, 371: 0, 372: 0, 373: 0, 374: 0, 375: 0, 376: 0, 377: 0, 378: 0, 379: 0, 380: 4, 381: 2, 382: 0, 383: 0, 384: 0, 385: 0, 386: 0, 387: 2, 388: 0, 389: 0, 390: 1, 391: 1, 392: 0, 393: 0, 394: 0, 395: 0, 396: 2, 397: 5, 398: 2, 399: 1, 400: 0, 401: 4, 402: 2, 403: 0, 404: 2, 405: 0, 406: 1, 407: 10, 408: 0, 409: 0, 410: 0, 411: 0, 412: 0, 413: 0, 414: 0, 415: 0, 416: 0, 417: 0, 418: 0, 419: 0, 420: 0, 421: 0, 422: 0, 423: 0, 424: 3, 425: 0, 426: 0, 427: 0, 428: 0, 429: 0, 430: 0, 431: 0, 432: 0, 433: 0, 434: 0, 435: 0, 436: 0, 437: 2, 438: 0, 439: 0, 440: 0, 441: 0, 442: 0, 443: 0, 444: 0, 445: 1, 446: 0, 447: 0, 448: 0, 449: 0, 450: 0, 451: 0, 452: 0, 453: 0, 454: 0, 455: 0, 456: 0, 457: 0, 458: 0, 459: 0, 460: 0, 461: 0, 462: 0, 463: 0, 464: 0, 465: 0, 466: 0, 467: 0, 468: 0, 469: 0, 470: 0, 471: 0, 472: 0, 473: 0, 474: 0, 475: 0, 476: 0, 477: 0, 478: 0, 479: 0, 480: 0, 481: 0, 482: 0, 483: 0, 484: 0, 485: 0, 486: 0, 487: 0, 488: 0, 489: 0, 490: 0, 491: 0, 492: 0, 493: 0, 494: 0, 495: 0, 496: 0, 497: 1, 498: 0, 499: 0, 500: 0, 501: 0, 502: 0, 503: 0, 504: 0, 505: 0, 506: 0, 507: 0, 508: 0, 509: 0, 510: 0, 511: 0, 512: 0, 513: 0, 514: 0, 515: 0, 516: 0, 517: 0, 518: 0, 519: 0, 520: 0, 521: 0, 522: 0, 523: 0, 524: 0, 525: 0, 526: 0, 527: 0, 528: 0, 529: 0, 530: 0, 531: 0, 532: 0, 533: 0, 534: 0, 535: 0, 536: 0, 537: 0, 538: 0, 539: 0, 540: 0, 541: 0, 542: 0, 543: 0, 544: 0, 545: 0, 546: 0, 547: 0, 548: 0, 549: 0, 550: 0, 551: 0, 552: 0, 553: 0, 554: 115}\n",
      "555 unique labels are used\n",
      "424 unique labels never used\n",
      "58 unique labels used once\n",
      "33 unique labels used twice\n"
     ]
    }
   ],
   "source": [
    "print(f\"total number of steps is:\", step_count)\n",
    "print(f\"highest frequency label is {max_freq_index} and occurs {max} times\")\n",
    "print(f\"final label used is {len(label_count)-1}\")\n",
    "print(label_count)\n",
    "print(len(label_count),\"unique labels are used\")\n",
    "print(labels_never_used,\"unique labels never used\")\n",
    "print(labels_used_once, \"unique labels used once\")\n",
    "print(labels_used_twice, \"unique labels used twice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on label 0\n",
      "on label 200\n",
      "on label 400\n",
      "{0: (0.0, 2), 1: (0.7272727272727273, 22), 3: (0.0, 1), 4: (0.0, 3), 5: (0.0, 1), 6: (0.5, 2), 8: (0.5263157894736842, 19), 10: (0.5, 2), 11: (0.0, 1), 16: (0.25, 4), 17: (0.0, 1), 18: (0.5, 2), 22: (0.0, 1), 23: (0.0, 1), 25: (0.3333333333333333, 3), 26: (0.0, 1), 30: (0.0, 1), 31: (0.0, 1), 35: (0.0, 2), 40: (0.0, 1), 41: (0.2, 10), 42: (0.0, 1), 44: (0.2, 15), 45: (0.0, 7), 47: (0.0, 1), 48: (0.25, 4), 49: (0.0, 3), 52: (0.0, 2), 53: (0.0, 6), 54: (0.0, 2), 55: (0.3333333333333333, 3), 58: (0.42857142857142855, 7), 59: (0.0, 2), 62: (0.125, 8), 64: (0.0, 2), 66: (0.0, 1), 71: (0.0, 1), 72: (1.0, 1), 74: (0.0, 3), 75: (0.6666666666666666, 3), 80: (0.0, 2), 81: (1.0, 2), 83: (0.5357142857142857, 28), 84: (0.0, 2), 85: (0.0, 1), 86: (0.42857142857142855, 7), 87: (0.42857142857142855, 7), 88: (0.0, 1), 89: (0.0, 1), 90: (1.0, 1), 91: (0.0, 1), 92: (0.0, 1), 94: (0.0, 1), 97: (0.0, 6), 98: (0.0, 2), 100: (0.0, 1), 101: (0.6111111111111112, 18), 104: (0.6, 5), 106: (1.0, 1), 110: (0.0, 2), 111: (0.0, 2), 119: (0.0, 1), 123: (0.0, 1), 126: (0.0, 1), 127: (0.0, 3), 130: (1.0, 3), 134: (0.4, 5), 135: (0.0, 1), 136: (0.0, 2), 137: (0.0, 1), 140: (0.0, 1), 142: (0.0, 1), 144: (0.0, 1), 151: (0.0, 1), 157: (0.0, 2), 158: (0.0, 2), 159: (0.0, 1), 167: (1.0, 1), 169: (0.0, 2), 170: (0.0, 2), 171: (0.3333333333333333, 3), 172: (1.0, 1), 174: (1.0, 2), 184: (0.0, 1), 188: (0.0, 2), 189: (0.6, 5), 192: (0.0, 2), 193: (0.0, 4), 198: (0.3333333333333333, 3), 202: (0.0, 2), 203: (0.0, 1), 204: (0.0, 1), 208: (0.0, 1), 210: (0.0, 1), 228: (0.0, 1), 230: (0.0, 1), 231: (0.0, 5), 232: (0.3333333333333333, 3), 233: (0.0, 3), 237: (0.0, 1), 242: (0.5, 2), 250: (1.0, 1), 258: (0.0, 1), 267: (0.5, 2), 268: (0.0, 1), 273: (0.0, 1), 280: (0.0, 1), 282: (1.0, 4), 283: (0.3333333333333333, 3), 284: (0.0, 1), 328: (0.0, 1), 359: (0.0, 1), 380: (0.5, 4), 381: (0.5, 2), 387: (0.0, 2), 390: (0.0, 1), 391: (0.0, 1), 396: (0.0, 2), 397: (0.0, 5), 398: (1.0, 2), 399: (0.0, 1), 401: (0.25, 4), 402: (0.0, 2), 404: (0.0, 2), 406: (0.0, 1), 407: (0.5, 10), 424: (1.0, 3), 437: (0.0, 2), 445: (0.0, 1), 497: (1.0, 1), 554: (0.6869565217391305, 115)}\n",
      "if everything here is correct, our conc node val_acc should be: 0.402\n"
     ]
    }
   ],
   "source": [
    "# make acc dict for val conclusion nodes\n",
    "\n",
    "conc_label_acc_dict = {}\n",
    "\n",
    "def get_conc_gin_label_acc(i):\n",
    "    correct_pred_i = 0\n",
    "    total_num_i = 0\n",
    "\n",
    "    for graph in val_dataset:\n",
    "        get_gin_predict = gin_trained(graph.x.to(device),graph.edge_index.long().to(device))\n",
    "        get_conc_predict = np.argmax(get_gin_predict[-1].detach().cpu().numpy(),axis=0)\n",
    "        if graph.y[-1].item() == i:\n",
    "            total_num_i += 1\n",
    "            if get_conc_predict == i:\n",
    "                correct_pred_i += 1\n",
    "    if total_num_i == 0:\n",
    "        return 0,0,0\n",
    "    return (correct_pred_i / total_num_i), total_num_i, correct_pred_i\n",
    "\n",
    "\n",
    "total_conc_labels = 0\n",
    "total_conc_correct_preds = 0\n",
    "for i in range(hidden_conc_pf_data.num_classes):\n",
    "    if i % 200 ==0:\n",
    "        print(\"on label\",i)\n",
    "    try:\n",
    "        container = get_conc_gin_label_acc(i)\n",
    "        if container[1] != 0:\n",
    "            conc_label_acc_dict[i] = (container[0],container[1])  # pair of accuracy and label count for each label\n",
    "        total_conc_labels += container[1]\n",
    "        total_conc_correct_preds += container[2]\n",
    "    except Exception as e:\n",
    "        print(e,get_conc_gin_label_acc(i))\n",
    "\n",
    "print(conc_label_acc_dict)\n",
    "print(\"if everything here is correct, our conc node val_acc should be:\",total_conc_correct_preds/total_conc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: (0.0, 1),\n",
       " 5: (0.0, 1),\n",
       " 11: (0.0, 1),\n",
       " 17: (0.0, 1),\n",
       " 22: (0.0, 1),\n",
       " 23: (0.0, 1),\n",
       " 26: (0.0, 1),\n",
       " 30: (0.0, 1),\n",
       " 31: (0.0, 1),\n",
       " 40: (0.0, 1),\n",
       " 42: (0.0, 1),\n",
       " 47: (0.0, 1),\n",
       " 66: (0.0, 1),\n",
       " 71: (0.0, 1),\n",
       " 85: (0.0, 1),\n",
       " 88: (0.0, 1),\n",
       " 89: (0.0, 1),\n",
       " 91: (0.0, 1),\n",
       " 92: (0.0, 1),\n",
       " 94: (0.0, 1),\n",
       " 100: (0.0, 1),\n",
       " 119: (0.0, 1),\n",
       " 123: (0.0, 1),\n",
       " 126: (0.0, 1),\n",
       " 135: (0.0, 1),\n",
       " 137: (0.0, 1),\n",
       " 140: (0.0, 1),\n",
       " 142: (0.0, 1),\n",
       " 144: (0.0, 1),\n",
       " 151: (0.0, 1),\n",
       " 159: (0.0, 1),\n",
       " 184: (0.0, 1),\n",
       " 203: (0.0, 1),\n",
       " 204: (0.0, 1),\n",
       " 208: (0.0, 1),\n",
       " 210: (0.0, 1),\n",
       " 228: (0.0, 1),\n",
       " 230: (0.0, 1),\n",
       " 237: (0.0, 1),\n",
       " 258: (0.0, 1),\n",
       " 268: (0.0, 1),\n",
       " 273: (0.0, 1),\n",
       " 280: (0.0, 1),\n",
       " 284: (0.0, 1),\n",
       " 328: (0.0, 1),\n",
       " 359: (0.0, 1),\n",
       " 390: (0.0, 1),\n",
       " 391: (0.0, 1),\n",
       " 399: (0.0, 1),\n",
       " 406: (0.0, 1),\n",
       " 445: (0.0, 1),\n",
       " 0: (0.0, 2),\n",
       " 35: (0.0, 2),\n",
       " 52: (0.0, 2),\n",
       " 54: (0.0, 2),\n",
       " 59: (0.0, 2),\n",
       " 64: (0.0, 2),\n",
       " 80: (0.0, 2),\n",
       " 84: (0.0, 2),\n",
       " 98: (0.0, 2),\n",
       " 110: (0.0, 2),\n",
       " 111: (0.0, 2),\n",
       " 136: (0.0, 2),\n",
       " 157: (0.0, 2),\n",
       " 158: (0.0, 2),\n",
       " 169: (0.0, 2),\n",
       " 170: (0.0, 2),\n",
       " 188: (0.0, 2),\n",
       " 192: (0.0, 2),\n",
       " 202: (0.0, 2),\n",
       " 387: (0.0, 2),\n",
       " 396: (0.0, 2),\n",
       " 402: (0.0, 2),\n",
       " 404: (0.0, 2),\n",
       " 437: (0.0, 2),\n",
       " 4: (0.0, 3),\n",
       " 49: (0.0, 3),\n",
       " 74: (0.0, 3),\n",
       " 127: (0.0, 3),\n",
       " 233: (0.0, 3),\n",
       " 193: (0.0, 4),\n",
       " 231: (0.0, 5),\n",
       " 397: (0.0, 5),\n",
       " 53: (0.0, 6),\n",
       " 97: (0.0, 6),\n",
       " 45: (0.0, 7),\n",
       " 62: (0.125, 8),\n",
       " 41: (0.2, 10),\n",
       " 44: (0.2, 15),\n",
       " 16: (0.25, 4),\n",
       " 48: (0.25, 4),\n",
       " 401: (0.25, 4),\n",
       " 25: (0.3333333333333333, 3),\n",
       " 55: (0.3333333333333333, 3),\n",
       " 171: (0.3333333333333333, 3),\n",
       " 198: (0.3333333333333333, 3),\n",
       " 232: (0.3333333333333333, 3),\n",
       " 283: (0.3333333333333333, 3),\n",
       " 134: (0.4, 5),\n",
       " 58: (0.42857142857142855, 7),\n",
       " 86: (0.42857142857142855, 7),\n",
       " 87: (0.42857142857142855, 7),\n",
       " 6: (0.5, 2),\n",
       " 10: (0.5, 2),\n",
       " 18: (0.5, 2),\n",
       " 242: (0.5, 2),\n",
       " 267: (0.5, 2),\n",
       " 381: (0.5, 2),\n",
       " 380: (0.5, 4),\n",
       " 407: (0.5, 10),\n",
       " 8: (0.5263157894736842, 19),\n",
       " 83: (0.5357142857142857, 28),\n",
       " 104: (0.6, 5),\n",
       " 189: (0.6, 5),\n",
       " 101: (0.6111111111111112, 18),\n",
       " 75: (0.6666666666666666, 3),\n",
       " 554: (0.6869565217391305, 115),\n",
       " 1: (0.7272727272727273, 22),\n",
       " 72: (1.0, 1),\n",
       " 90: (1.0, 1),\n",
       " 106: (1.0, 1),\n",
       " 167: (1.0, 1),\n",
       " 172: (1.0, 1),\n",
       " 250: (1.0, 1),\n",
       " 497: (1.0, 1),\n",
       " 81: (1.0, 2),\n",
       " 174: (1.0, 2),\n",
       " 398: (1.0, 2),\n",
       " 130: (1.0, 3),\n",
       " 424: (1.0, 3),\n",
       " 282: (1.0, 4)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort the above dict by accuracy\n",
    "\n",
    "sorted_conc_label_acc_dict = {k: v for k, v in sorted(conc_label_acc_dict.items(), key=lambda item: item[1])}\n",
    "sorted_conc_label_acc_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
