{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from use_dataset import ProofDataset, HiddenConcProofDataset\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import random\n",
    "from sklearn.metrics import f1_score as f1\n",
    "#from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, ReLU, BatchNorm1d, Sequential\n",
    "from torch_geometric.nn import GINConv\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use file_limit=10000 to only load and verify the first 10000 graphs (creates file of size ~420 MB)\n",
    "\n",
    "file_limit = 10000    # desired number of graphs to work with\n",
    "vocab_size = 1598   # number of characters in our vocabulary\n",
    "\n",
    "pf_data = ProofDataset(root=\"data/\",read_name=\"10000_relabeled_data_at_least_10.json\" , write_name=\"10000_relabeled_data_at_least_10.pt\" ,file_limit=file_limit)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make train/val/test for GIN\n",
    "\n",
    "# set seed for random # generation\n",
    "random.seed(10)\n",
    "length = file_limit\n",
    "total_indices = [i+file_limit for i in range(file_limit)]\n",
    "\n",
    "# create index vectors to filter dataset\n",
    "train_indices = [i for i in range(file_limit)]\n",
    "train_indices = train_indices + random.sample(total_indices, int(length*.8))\n",
    "train_indices.sort()\n",
    "\n",
    "val_index_options = [x for x in total_indices if x not in train_indices]\n",
    "val_indices = random.sample(val_index_options, int(length*.1))\n",
    "val_indices.sort()\n",
    "\n",
    "test_index_options = [x for x in total_indices if x not in train_indices if x not in val_indices]\n",
    "test_indices = random.sample(test_index_options, int(length*.1))\n",
    "test_indices.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# our model is judged on how well it can predict the label for the final/conclusion node as seen in the val set created above\n",
    "# to not give our model too much info, we zero out the features on the final/conclusion node for each graph in the val set\n",
    "\n",
    "data_list = []\n",
    "\n",
    "for idx, graph in enumerate(pf_data):\n",
    "    # inherit features, labels, and edges\n",
    "    x = graph.x.clone()\n",
    "    y = graph.y.clone()\n",
    "    edge_index = graph.edge_index.clone()\n",
    "    x[-1] = torch.zeros(512) # zero out ALL conclusion nodes\n",
    "\n",
    "    #if idx in val_indices:\n",
    "        #x[-1] = torch.zeros(512)    # zero out features of final/conclusion node for each graph in val set\n",
    "\n",
    "# replace features of conlusion/final nodes from val set with average of neighboring node embeddings\n",
    "    #if idx in val_indices:\n",
    "\n",
    "        #connected_nodes = []\n",
    "        #sum = torch.zeros(512)\n",
    "\n",
    "        #for i, edge in enumerate(edge_index[0]): # case of outgoing edge coming from conc node\n",
    "            #if edge == (pf_data[idx].num_nodes-1):  \n",
    "                #connected_nodes.append(int(edge_index[1][i].item()))\n",
    "        #for i, edge in enumerate(edge_index[1]): # case of outgoing edge going to conc node\n",
    "            #if edge == (pf_data[idx].num_nodes-1):\n",
    "                #connected_nodes.append(int(edge_index[0][i].item()))\n",
    "\n",
    "        #for i in connected_nodes:\n",
    "            #if not torch.equal(pf_data[idx].x[i], pf_data[idx].x[-1]):\n",
    "            #sum += pf_data[idx].x[i]\n",
    "        #if len(connected_nodes) > 0:\n",
    "            #x[-1] = sum/(len(connected_nodes))\n",
    "        #else:\n",
    "            #x[-1] = sum\n",
    "\n",
    "    data_list.append(Data(x=x,y=y,edge_index=edge_index))\n",
    "\n",
    "# read_name not used, but needed to instantiate class\n",
    "hidden_conc_pf_data = HiddenConcProofDataset(root=\"data/\",read_name=\"10000_relabeled_data_at_least_10_w_stmts.json\",write_name=\"overwritten_labels.pt\", data_list=data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set   = 18000 graphs\n",
      "Validation set = 1000 graphs\n",
      "Test set       = 1000 graphs\n"
     ]
    }
   ],
   "source": [
    "# Create training, validation, and test sets\n",
    "train_dataset = hidden_conc_pf_data[train_indices]\n",
    "val_dataset = hidden_conc_pf_data[val_indices]\n",
    "test_dataset = hidden_conc_pf_data[test_indices]\n",
    "\n",
    "print(f'Training set   = {len(train_dataset)} graphs')\n",
    "print(f'Validation set = {len(val_dataset)} graphs')\n",
    "print(f'Test set       = {len(test_dataset)} graphs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mini-batches\n",
    "# batch_size is number of graphs\n",
    "train_loader = DataLoader(train_dataset, batch_size=750, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=500, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=500, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loader:\n",
      " - Batch 0: DataBatch(x=[7562, 512], edge_index=[2, 6812], y=[7562], batch=[7562], ptr=[751])\n",
      " - Batch 1: DataBatch(x=[7012, 512], edge_index=[2, 6262], y=[7012], batch=[7012], ptr=[751])\n",
      " - Batch 2: DataBatch(x=[6329, 512], edge_index=[2, 5579], y=[6329], batch=[6329], ptr=[751])\n",
      " - Batch 3: DataBatch(x=[6330, 512], edge_index=[2, 5580], y=[6330], batch=[6330], ptr=[751])\n",
      " - Batch 4: DataBatch(x=[9286, 512], edge_index=[2, 8536], y=[9286], batch=[9286], ptr=[751])\n",
      " - Batch 5: DataBatch(x=[7190, 512], edge_index=[2, 6440], y=[7190], batch=[7190], ptr=[751])\n",
      " - Batch 6: DataBatch(x=[7955, 512], edge_index=[2, 7205], y=[7955], batch=[7955], ptr=[751])\n",
      " - Batch 7: DataBatch(x=[11743, 512], edge_index=[2, 10993], y=[11743], batch=[11743], ptr=[751])\n",
      " - Batch 8: DataBatch(x=[8107, 512], edge_index=[2, 7357], y=[8107], batch=[8107], ptr=[751])\n",
      " - Batch 9: DataBatch(x=[8510, 512], edge_index=[2, 7760], y=[8510], batch=[8510], ptr=[751])\n",
      " - Batch 10: DataBatch(x=[13047, 512], edge_index=[2, 12297], y=[13047], batch=[13047], ptr=[751])\n",
      " - Batch 11: DataBatch(x=[6218, 512], edge_index=[2, 5468], y=[6218], batch=[6218], ptr=[751])\n",
      " - Batch 12: DataBatch(x=[6026, 512], edge_index=[2, 5276], y=[6026], batch=[6026], ptr=[751])\n",
      " - Batch 13: DataBatch(x=[6202, 512], edge_index=[2, 5452], y=[6202], batch=[6202], ptr=[751])\n",
      " - Batch 14: DataBatch(x=[6634, 512], edge_index=[2, 5884], y=[6634], batch=[6634], ptr=[751])\n",
      " - Batch 15: DataBatch(x=[7894, 512], edge_index=[2, 7144], y=[7894], batch=[7894], ptr=[751])\n",
      " - Batch 16: DataBatch(x=[6291, 512], edge_index=[2, 5541], y=[6291], batch=[6291], ptr=[751])\n",
      " - Batch 17: DataBatch(x=[8416, 512], edge_index=[2, 7666], y=[8416], batch=[8416], ptr=[751])\n",
      " - Batch 18: DataBatch(x=[6023, 512], edge_index=[2, 5273], y=[6023], batch=[6023], ptr=[751])\n",
      " - Batch 19: DataBatch(x=[5311, 512], edge_index=[2, 4561], y=[5311], batch=[5311], ptr=[751])\n",
      " - Batch 20: DataBatch(x=[7482, 512], edge_index=[2, 6732], y=[7482], batch=[7482], ptr=[751])\n",
      " - Batch 21: DataBatch(x=[5145, 512], edge_index=[2, 4395], y=[5145], batch=[5145], ptr=[751])\n",
      " - Batch 22: DataBatch(x=[9313, 512], edge_index=[2, 8563], y=[9313], batch=[9313], ptr=[751])\n",
      " - Batch 23: DataBatch(x=[6667, 512], edge_index=[2, 5917], y=[6667], batch=[6667], ptr=[751])\n",
      "\n",
      "Validation loader:\n",
      " - Batch 0: DataBatch(x=[2838, 512], edge_index=[2, 2338], y=[2838], batch=[2838], ptr=[501])\n",
      " - Batch 1: DataBatch(x=[13532, 512], edge_index=[2, 13032], y=[13532], batch=[13532], ptr=[501])\n",
      "\n",
      "Test loader:\n",
      " - Batch 0: DataBatch(x=[2861, 512], edge_index=[2, 2361], y=[2861], batch=[2861], ptr=[501])\n",
      " - Batch 1: DataBatch(x=[11770, 512], edge_index=[2, 11270], y=[11770], batch=[11770], ptr=[501])\n"
     ]
    }
   ],
   "source": [
    "print('\\nTrain loader:')\n",
    "for i, batch in enumerate(train_loader):\n",
    "    print(f' - Batch {i}: {batch}')\n",
    "\n",
    "print('\\nValidation loader:')\n",
    "for i, batch in enumerate(val_loader):\n",
    "    print(f' - Batch {i}: {batch}')\n",
    "\n",
    "print('\\nTest loader:')\n",
    "for i, batch in enumerate(test_loader):\n",
    "    print(f' - Batch {i}: {batch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary to record label frequency\n",
    "\n",
    "# get max label used in hidden_conc_pf_data\n",
    "max_label = 0\n",
    "\n",
    "for i in range(file_limit*2):\n",
    "    for j in hidden_conc_pf_data.get(i).y:\n",
    "        if j > max_label:\n",
    "            max_label = j.to(int).item()\n",
    "\n",
    "# initialize histogram for labels used in hidden_conc_pf_data            \n",
    "label_count = {}\n",
    "\n",
    "for i in range(file_limit*2):\n",
    "    for j in range(max_label+1):        \n",
    "        label_count[j] = 0\n",
    "\n",
    "for i in range(file_limit*2):\n",
    "    for j in hidden_conc_pf_data.get(i).y:\n",
    "        label_count[j.to(int).item()] += 1\n",
    "\n",
    "step_count = 0\n",
    "max = 0\n",
    "max_freq_index = None   #find the most frequently used index\n",
    "labels_never_used = 0\n",
    "labels_used_once = 0\n",
    "labels_used_twice = 0\n",
    "\n",
    "for k,v in label_count.items():\n",
    "    step_count += v\n",
    "    if v > max:     \n",
    "        max = v\n",
    "        max_freq_index = k\n",
    "\n",
    "    if v == 0:\n",
    "        labels_never_used += 1\n",
    "    if v == 1:\n",
    "        labels_used_once += 1\n",
    "    if v ==2:\n",
    "        labels_used_twice += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of steps is: 211694\n",
      "highest frequency label is 0 and occurs 44150 times\n",
      "final label used is 2276\n",
      "{0: 44150, 1: 1863, 2: 66, 3: 77, 4: 111, 5: 1285, 6: 18, 7: 127, 8: 28, 9: 357, 10: 55, 11: 4640, 12: 697, 13: 72, 14: 145, 15: 81, 16: 762, 17: 47, 18: 184, 19: 15, 20: 69, 21: 27, 22: 93, 23: 323, 24: 26, 25: 243, 26: 318, 27: 30, 28: 40, 29: 100, 30: 23, 31: 37, 32: 11, 33: 51, 34: 29, 35: 13, 36: 14, 37: 144, 38: 26, 39: 42, 40: 27, 41: 14, 42: 15, 43: 22, 44: 15, 45: 26, 46: 12, 47: 37, 48: 227, 49: 92, 50: 11, 51: 11, 52: 11, 53: 24, 54: 23, 55: 28, 56: 50, 57: 152, 58: 52, 59: 47, 60: 45, 61: 39, 62: 15, 63: 25, 64: 26, 65: 11, 66: 30, 67: 39, 68: 45, 69: 60, 70: 51, 71: 14, 72: 12, 73: 50, 74: 15, 75: 21, 76: 46, 77: 66, 78: 97, 79: 13, 80: 34, 81: 26, 82: 22, 83: 62, 84: 70, 85: 14, 86: 32, 87: 33, 88: 21, 89: 25, 90: 236, 91: 12, 92: 27, 93: 13, 94: 13, 95: 52, 96: 77, 97: 46, 98: 300, 99: 192, 100: 29, 101: 221, 102: 672, 103: 888, 104: 15, 105: 28, 106: 26, 107: 110, 108: 159, 109: 86, 110: 73, 111: 115, 112: 176, 113: 305, 114: 265, 115: 750, 116: 134, 117: 1138, 118: 181, 119: 45, 120: 144, 121: 316, 122: 96, 123: 85, 124: 100, 125: 155, 126: 164, 127: 114, 128: 52, 129: 73, 130: 70, 131: 123, 132: 143, 133: 34, 134: 473, 135: 342, 136: 81, 137: 80, 138: 65, 139: 49, 140: 22, 141: 28, 142: 79, 143: 16, 144: 34, 145: 17, 146: 23, 147: 1097, 148: 92, 149: 170, 150: 350, 151: 288, 152: 13, 153: 77, 154: 80, 155: 263, 156: 13, 157: 70, 158: 323, 159: 19, 160: 120, 161: 65, 162: 17, 163: 83, 164: 24, 165: 17, 166: 105, 167: 247, 168: 28, 169: 56, 170: 14, 171: 75, 172: 12, 173: 418, 174: 76, 175: 37, 176: 16, 177: 41, 178: 12, 179: 55, 180: 240, 181: 30, 182: 13, 183: 13, 184: 308, 185: 16, 186: 85, 187: 12, 188: 34, 189: 29, 190: 26, 191: 60, 192: 74, 193: 21, 194: 29, 195: 14, 196: 29, 197: 20, 198: 36, 199: 27, 200: 78, 201: 26, 202: 16, 203: 17, 204: 321, 205: 100, 206: 44, 207: 25, 208: 617, 209: 105, 210: 94, 211: 37, 212: 36, 213: 14, 214: 38, 215: 12, 216: 23, 217: 15, 218: 13, 219: 11, 220: 14, 221: 14, 222: 41, 223: 24, 224: 16, 225: 18, 226: 49, 227: 23, 228: 14, 229: 12, 230: 679, 231: 204, 232: 11, 233: 88, 234: 13, 235: 1123, 236: 273, 237: 101, 238: 28, 239: 45, 240: 30, 241: 48, 242: 54, 243: 17, 244: 15, 245: 17, 246: 15, 247: 16, 248: 70, 249: 37, 250: 34, 251: 59, 252: 52, 253: 14, 254: 133, 255: 13, 256: 316, 257: 115, 258: 18, 259: 21, 260: 13, 261: 13, 262: 69, 263: 96, 264: 26, 265: 106, 266: 15, 267: 25, 268: 139, 269: 93, 270: 25, 271: 35, 272: 2336, 273: 1328, 274: 466, 275: 23, 276: 780, 277: 73, 278: 14, 279: 11, 280: 45, 281: 39, 282: 713, 283: 341, 284: 226, 285: 88, 286: 34, 287: 23, 288: 17, 289: 16, 290: 18, 291: 48, 292: 40, 293: 22, 294: 23, 295: 63, 296: 35, 297: 349, 298: 48, 299: 17, 300: 11, 301: 61, 302: 27, 303: 14, 304: 18, 305: 22, 306: 16, 307: 38, 308: 40, 309: 44, 310: 19, 311: 33, 312: 34, 313: 11, 314: 12, 315: 13, 316: 24, 317: 21, 318: 11, 319: 12, 320: 28, 321: 34, 322: 25, 323: 11, 324: 24, 325: 88, 326: 20, 327: 11, 328: 90, 329: 24, 330: 16, 331: 44, 332: 410, 333: 57, 334: 368, 335: 1995, 336: 477, 337: 580, 338: 43, 339: 49, 340: 114, 341: 313, 342: 113, 343: 34, 344: 332, 345: 52, 346: 45, 347: 64, 348: 32, 349: 34, 350: 126, 351: 11, 352: 56, 353: 78, 354: 14, 355: 13, 356: 230, 357: 183, 358: 12, 359: 191, 360: 251, 361: 227, 362: 678, 363: 29, 364: 33, 365: 16, 366: 18, 367: 51, 368: 28, 369: 32, 370: 14, 371: 12, 372: 17, 373: 100, 374: 21, 375: 87, 376: 39, 377: 403, 378: 255, 379: 356, 380: 22, 381: 29, 382: 22, 383: 14, 384: 66, 385: 23, 386: 90, 387: 11, 388: 70, 389: 103, 390: 172, 391: 82, 392: 164, 393: 12, 394: 18, 395: 671, 396: 205, 397: 243, 398: 196, 399: 67, 400: 18, 401: 14, 402: 27, 403: 12, 404: 199, 405: 262, 406: 16, 407: 398, 408: 43, 409: 326, 410: 32, 411: 33, 412: 81, 413: 86, 414: 62, 415: 36, 416: 39, 417: 19, 418: 12, 419: 41, 420: 11, 421: 17, 422: 15, 423: 41, 424: 14, 425: 13, 426: 124, 427: 84, 428: 92, 429: 49, 430: 12, 431: 22, 432: 138, 433: 74, 434: 27, 435: 14, 436: 23, 437: 42, 438: 77, 439: 58, 440: 37, 441: 57, 442: 20, 443: 40, 444: 31, 445: 14, 446: 12, 447: 18, 448: 13, 449: 36, 450: 17, 451: 66, 452: 16, 453: 15, 454: 110, 455: 13, 456: 34, 457: 13, 458: 25, 459: 11, 460: 36, 461: 50, 462: 62, 463: 31, 464: 17, 465: 12, 466: 35, 467: 93, 468: 25, 469: 53, 470: 38, 471: 52, 472: 32, 473: 27, 474: 70, 475: 75, 476: 93, 477: 26, 478: 93, 479: 39, 480: 13, 481: 18, 482: 58, 483: 123, 484: 96, 485: 190, 486: 304, 487: 179, 488: 237, 489: 166, 490: 141, 491: 176, 492: 92, 493: 44, 494: 27, 495: 11, 496: 21, 497: 12, 498: 11, 499: 18, 500: 22, 501: 12, 502: 15, 503: 19, 504: 16, 505: 20, 506: 13, 507: 70, 508: 84, 509: 74, 510: 34, 511: 24, 512: 35, 513: 34, 514: 17, 515: 15, 516: 16, 517: 23, 518: 15, 519: 18, 520: 13, 521: 20, 522: 23, 523: 27, 524: 13, 525: 15, 526: 17, 527: 16, 528: 51, 529: 18, 530: 11, 531: 400, 532: 40, 533: 28, 534: 15, 535: 18, 536: 18, 537: 58, 538: 12, 539: 25, 540: 15, 541: 35, 542: 19, 543: 38, 544: 16, 545: 14, 546: 49, 547: 29, 548: 22, 549: 24, 550: 23, 551: 38, 552: 65, 553: 12, 554: 15, 555: 33, 556: 35, 557: 25, 558: 13, 559: 15, 560: 11, 561: 27, 562: 12, 563: 11, 564: 19, 565: 11, 566: 46, 567: 43, 568: 11, 569: 30, 570: 21, 571: 19, 572: 18, 573: 38, 574: 38, 575: 53, 576: 20, 577: 50, 578: 26, 579: 51, 580: 40, 581: 24, 582: 33, 583: 95, 584: 16, 585: 25, 586: 16, 587: 11, 588: 296, 589: 26, 590: 18, 591: 23, 592: 32, 593: 18, 594: 80, 595: 19, 596: 38, 597: 368, 598: 64, 599: 53, 600: 29, 601: 50, 602: 24, 603: 22, 604: 11, 605: 13, 606: 28, 607: 17, 608: 11, 609: 18, 610: 15, 611: 42, 612: 125, 613: 50, 614: 270, 615: 46, 616: 13, 617: 45, 618: 49, 619: 15, 620: 692, 621: 26, 622: 37, 623: 69, 624: 14, 625: 111, 626: 150, 627: 28, 628: 13, 629: 114, 630: 32, 631: 17, 632: 193, 633: 24, 634: 62, 635: 144, 636: 12, 637: 37, 638: 18, 639: 45, 640: 25, 641: 35, 642: 53, 643: 33, 644: 17, 645: 30, 646: 14, 647: 11, 648: 14, 649: 14, 650: 20, 651: 34, 652: 12, 653: 15, 654: 22, 655: 49, 656: 42, 657: 11, 658: 35, 659: 38, 660: 34, 661: 37, 662: 24, 663: 14, 664: 15, 665: 37, 666: 16, 667: 40, 668: 23, 669: 17, 670: 75, 671: 64, 672: 42, 673: 58, 674: 70, 675: 79, 676: 28, 677: 47, 678: 62, 679: 30, 680: 69, 681: 12, 682: 60, 683: 14, 684: 22, 685: 31, 686: 19, 687: 15, 688: 38, 689: 11, 690: 15, 691: 11, 692: 69, 693: 19, 694: 44, 695: 17, 696: 37, 697: 34, 698: 17, 699: 33, 700: 74, 701: 22, 702: 19, 703: 11, 704: 11, 705: 22, 706: 12, 707: 44, 708: 61, 709: 37, 710: 28, 711: 19, 712: 33, 713: 32, 714: 12, 715: 19, 716: 19, 717: 13, 718: 12, 719: 37, 720: 266, 721: 16, 722: 22, 723: 17, 724: 30, 725: 11, 726: 27, 727: 32, 728: 12, 729: 11, 730: 13, 731: 14, 732: 14, 733: 43, 734: 21, 735: 27, 736: 18, 737: 17, 738: 43, 739: 31, 740: 14, 741: 12, 742: 22, 743: 19, 744: 59, 745: 46, 746: 14, 747: 31, 748: 78, 749: 73, 750: 1040, 751: 118, 752: 282, 753: 272, 754: 83, 755: 224, 756: 173, 757: 57, 758: 111, 759: 366, 760: 117, 761: 58, 762: 16, 763: 21, 764: 368, 765: 30, 766: 30, 767: 34, 768: 549, 769: 40, 770: 69, 771: 179, 772: 98, 773: 11, 774: 15, 775: 28, 776: 159, 777: 41, 778: 399, 779: 23, 780: 79, 781: 165, 782: 69, 783: 37, 784: 54, 785: 20, 786: 372, 787: 20, 788: 75, 789: 21, 790: 379, 791: 37, 792: 132, 793: 39, 794: 94, 795: 70, 796: 23, 797: 14, 798: 179, 799: 55, 800: 54, 801: 129, 802: 33, 803: 11, 804: 29, 805: 137, 806: 489, 807: 389, 808: 518, 809: 294, 810: 12, 811: 40, 812: 216, 813: 185, 814: 36, 815: 86, 816: 27, 817: 47, 818: 180, 819: 129, 820: 68, 821: 117, 822: 204, 823: 37, 824: 39, 825: 62, 826: 57, 827: 21, 828: 88, 829: 44, 830: 17, 831: 12, 832: 35, 833: 17, 834: 11, 835: 24, 836: 19, 837: 23, 838: 85, 839: 14, 840: 105, 841: 11, 842: 736, 843: 117, 844: 28, 845: 15, 846: 32, 847: 55, 848: 93, 849: 28, 850: 12, 851: 21, 852: 89, 853: 56, 854: 33, 855: 113, 856: 39, 857: 85, 858: 42, 859: 22, 860: 62, 861: 37, 862: 11, 863: 15, 864: 15, 865: 17, 866: 16, 867: 16, 868: 17, 869: 12, 870: 55, 871: 11, 872: 22, 873: 18, 874: 25, 875: 32, 876: 11, 877: 17, 878: 55, 879: 17, 880: 70, 881: 12, 882: 13, 883: 54, 884: 28, 885: 14, 886: 21, 887: 13, 888: 37, 889: 169, 890: 168, 891: 62, 892: 50, 893: 112, 894: 60, 895: 53, 896: 23, 897: 27, 898: 23, 899: 13, 900: 20, 901: 11, 902: 105, 903: 14, 904: 25, 905: 166, 906: 12, 907: 18, 908: 51, 909: 24, 910: 51, 911: 36, 912: 131, 913: 218, 914: 36, 915: 18, 916: 11, 917: 20, 918: 60, 919: 12, 920: 11, 921: 59, 922: 233, 923: 15, 924: 17, 925: 54, 926: 55, 927: 29, 928: 40, 929: 61, 930: 48, 931: 15, 932: 38, 933: 39, 934: 47, 935: 35, 936: 11, 937: 16, 938: 132, 939: 47, 940: 31, 941: 14, 942: 20, 943: 15, 944: 19, 945: 15, 946: 19, 947: 18, 948: 64, 949: 55, 950: 56, 951: 53, 952: 60, 953: 38, 954: 11, 955: 11, 956: 18, 957: 12, 958: 14, 959: 11, 960: 35, 961: 183, 962: 21, 963: 16, 964: 18, 965: 12, 966: 14, 967: 21, 968: 23, 969: 26, 970: 22, 971: 11, 972: 24, 973: 24, 974: 15, 975: 22, 976: 20, 977: 33, 978: 57, 979: 11, 980: 72, 981: 25, 982: 17, 983: 65, 984: 12, 985: 21, 986: 19, 987: 38, 988: 14, 989: 65, 990: 42, 991: 12, 992: 13, 993: 70, 994: 14, 995: 11, 996: 29, 997: 2099, 998: 146, 999: 20, 1000: 44, 1001: 20, 1002: 20, 1003: 12, 1004: 251, 1005: 25, 1006: 35, 1007: 14, 1008: 29, 1009: 11, 1010: 27, 1011: 85, 1012: 38, 1013: 21, 1014: 53, 1015: 15, 1016: 40, 1017: 21, 1018: 32, 1019: 84, 1020: 20, 1021: 90, 1022: 62, 1023: 31, 1024: 30, 1025: 157, 1026: 72, 1027: 15, 1028: 16, 1029: 21, 1030: 11, 1031: 29, 1032: 14, 1033: 91, 1034: 28, 1035: 24, 1036: 14, 1037: 11, 1038: 95, 1039: 18, 1040: 53, 1041: 21, 1042: 13, 1043: 24, 1044: 39, 1045: 51, 1046: 28, 1047: 17, 1048: 38, 1049: 47, 1050: 56, 1051: 11, 1052: 12, 1053: 12, 1054: 14, 1055: 12, 1056: 19, 1057: 33, 1058: 42, 1059: 84, 1060: 30, 1061: 23, 1062: 21, 1063: 188, 1064: 12, 1065: 35, 1066: 183, 1067: 24, 1068: 15, 1069: 12, 1070: 112, 1071: 30, 1072: 41, 1073: 18, 1074: 137, 1075: 60, 1076: 13, 1077: 21, 1078: 63, 1079: 52, 1080: 34, 1081: 136, 1082: 29, 1083: 153, 1084: 23, 1085: 100, 1086: 97, 1087: 123, 1088: 233, 1089: 12, 1090: 62, 1091: 118, 1092: 64, 1093: 52, 1094: 74, 1095: 109, 1096: 92, 1097: 89, 1098: 94, 1099: 28, 1100: 87, 1101: 93, 1102: 38, 1103: 54, 1104: 201, 1105: 134, 1106: 18, 1107: 35, 1108: 27, 1109: 153, 1110: 193, 1111: 96, 1112: 177, 1113: 28, 1114: 33, 1115: 49, 1116: 57, 1117: 18, 1118: 20, 1119: 33, 1120: 17, 1121: 24, 1122: 15, 1123: 36, 1124: 26, 1125: 30, 1126: 59, 1127: 32, 1128: 34, 1129: 50, 1130: 66, 1131: 25, 1132: 40, 1133: 20, 1134: 16, 1135: 48, 1136: 30, 1137: 86, 1138: 106, 1139: 22, 1140: 11, 1141: 21, 1142: 15, 1143: 28, 1144: 13, 1145: 12, 1146: 19, 1147: 40, 1148: 30, 1149: 27, 1150: 19, 1151: 45, 1152: 19, 1153: 229, 1154: 71, 1155: 133, 1156: 27, 1157: 115, 1158: 16, 1159: 113, 1160: 24, 1161: 14, 1162: 13, 1163: 24, 1164: 38, 1165: 42, 1166: 24, 1167: 45, 1168: 100, 1169: 16, 1170: 108, 1171: 69, 1172: 20, 1173: 29, 1174: 23, 1175: 19, 1176: 24, 1177: 32, 1178: 11, 1179: 16, 1180: 33, 1181: 43, 1182: 40, 1183: 11, 1184: 17, 1185: 143, 1186: 68, 1187: 65, 1188: 11, 1189: 23, 1190: 62, 1191: 24, 1192: 32, 1193: 36, 1194: 11, 1195: 14, 1196: 28, 1197: 11, 1198: 46, 1199: 22, 1200: 168, 1201: 131, 1202: 12, 1203: 15, 1204: 11, 1205: 14, 1206: 23, 1207: 15, 1208: 11, 1209: 11, 1210: 12, 1211: 63, 1212: 113, 1213: 76, 1214: 73, 1215: 14, 1216: 12, 1217: 31, 1218: 93, 1219: 35, 1220: 11, 1221: 11, 1222: 19, 1223: 12, 1224: 44, 1225: 63, 1226: 11, 1227: 25, 1228: 77, 1229: 13, 1230: 24, 1231: 41, 1232: 15, 1233: 19, 1234: 11, 1235: 12, 1236: 21, 1237: 14, 1238: 54, 1239: 20, 1240: 14, 1241: 17, 1242: 40, 1243: 20, 1244: 18, 1245: 22, 1246: 12, 1247: 11, 1248: 117, 1249: 81, 1250: 38, 1251: 27, 1252: 11, 1253: 31, 1254: 18, 1255: 54, 1256: 14, 1257: 14, 1258: 11, 1259: 76, 1260: 21, 1261: 27, 1262: 18, 1263: 16, 1264: 22, 1265: 67, 1266: 60, 1267: 67, 1268: 62, 1269: 13, 1270: 84, 1271: 22, 1272: 37, 1273: 23, 1274: 69, 1275: 43, 1276: 22, 1277: 129, 1278: 53, 1279: 30, 1280: 31, 1281: 89, 1282: 67, 1283: 18, 1284: 16, 1285: 23, 1286: 26, 1287: 48, 1288: 26, 1289: 15, 1290: 11, 1291: 13, 1292: 21, 1293: 60, 1294: 21, 1295: 28, 1296: 27, 1297: 47, 1298: 49, 1299: 15, 1300: 11, 1301: 17, 1302: 14, 1303: 19, 1304: 12, 1305: 19, 1306: 16, 1307: 31, 1308: 11, 1309: 26, 1310: 139, 1311: 18, 1312: 50, 1313: 31, 1314: 14, 1315: 12, 1316: 14, 1317: 11, 1318: 15, 1319: 12, 1320: 78, 1321: 83, 1322: 29, 1323: 12, 1324: 20, 1325: 65, 1326: 40, 1327: 39, 1328: 34, 1329: 15, 1330: 13, 1331: 75, 1332: 54, 1333: 161, 1334: 11, 1335: 21, 1336: 39, 1337: 118, 1338: 20, 1339: 11, 1340: 28, 1341: 34, 1342: 50, 1343: 19, 1344: 12, 1345: 12, 1346: 35, 1347: 11, 1348: 13, 1349: 12, 1350: 21, 1351: 19, 1352: 147, 1353: 40, 1354: 30, 1355: 16, 1356: 25, 1357: 21, 1358: 23, 1359: 18, 1360: 14, 1361: 28, 1362: 33, 1363: 29, 1364: 14, 1365: 19, 1366: 30, 1367: 239, 1368: 37, 1369: 343, 1370: 339, 1371: 27, 1372: 17, 1373: 19, 1374: 102, 1375: 29, 1376: 83, 1377: 112, 1378: 20, 1379: 21, 1380: 14, 1381: 15, 1382: 13, 1383: 25, 1384: 18, 1385: 13, 1386: 12, 1387: 49, 1388: 31, 1389: 53, 1390: 11, 1391: 14, 1392: 21, 1393: 44, 1394: 29, 1395: 48, 1396: 14, 1397: 13, 1398: 56, 1399: 22, 1400: 22, 1401: 11, 1402: 14, 1403: 26, 1404: 30, 1405: 26, 1406: 21, 1407: 71, 1408: 14, 1409: 13, 1410: 14, 1411: 28, 1412: 13, 1413: 29, 1414: 24, 1415: 254, 1416: 11, 1417: 41, 1418: 46, 1419: 18, 1420: 39, 1421: 115, 1422: 56, 1423: 16, 1424: 48, 1425: 38, 1426: 20, 1427: 31, 1428: 31, 1429: 30, 1430: 23, 1431: 54, 1432: 13, 1433: 38, 1434: 54, 1435: 28, 1436: 58, 1437: 16, 1438: 14, 1439: 190, 1440: 42, 1441: 166, 1442: 11, 1443: 12, 1444: 48, 1445: 32, 1446: 23, 1447: 14, 1448: 15, 1449: 12, 1450: 11, 1451: 14, 1452: 59, 1453: 32, 1454: 18, 1455: 21, 1456: 11, 1457: 17, 1458: 14, 1459: 18, 1460: 24, 1461: 11, 1462: 14, 1463: 14, 1464: 25, 1465: 18, 1466: 12, 1467: 20, 1468: 22, 1469: 21, 1470: 65, 1471: 44, 1472: 36, 1473: 16, 1474: 39, 1475: 41, 1476: 54, 1477: 17, 1478: 21, 1479: 12, 1480: 64, 1481: 58, 1482: 108, 1483: 34, 1484: 39, 1485: 12, 1486: 45, 1487: 16, 1488: 25, 1489: 39, 1490: 16, 1491: 22, 1492: 12, 1493: 19, 1494: 13, 1495: 13, 1496: 19, 1497: 30, 1498: 12, 1499: 76, 1500: 56, 1501: 46, 1502: 15, 1503: 74, 1504: 146, 1505: 231, 1506: 16, 1507: 29, 1508: 20, 1509: 23, 1510: 13, 1511: 12, 1512: 19, 1513: 23, 1514: 11, 1515: 11, 1516: 15, 1517: 13, 1518: 77, 1519: 12, 1520: 18, 1521: 15, 1522: 11, 1523: 18, 1524: 13, 1525: 15, 1526: 13, 1527: 12, 1528: 13, 1529: 17, 1530: 29, 1531: 35, 1532: 22, 1533: 23, 1534: 53, 1535: 19, 1536: 28, 1537: 32, 1538: 29, 1539: 57, 1540: 162, 1541: 49, 1542: 81, 1543: 16, 1544: 20, 1545: 13, 1546: 17, 1547: 20, 1548: 25, 1549: 13, 1550: 14, 1551: 23, 1552: 59, 1553: 48, 1554: 121, 1555: 31, 1556: 12, 1557: 17, 1558: 26, 1559: 20, 1560: 36, 1561: 32, 1562: 55, 1563: 26, 1564: 11, 1565: 46, 1566: 12, 1567: 14, 1568: 16, 1569: 17, 1570: 11, 1571: 12, 1572: 67, 1573: 47, 1574: 26, 1575: 15, 1576: 18, 1577: 14, 1578: 11, 1579: 14, 1580: 66, 1581: 16, 1582: 18, 1583: 20, 1584: 65, 1585: 14, 1586: 15, 1587: 54, 1588: 13, 1589: 33, 1590: 11, 1591: 12, 1592: 42, 1593: 14, 1594: 46, 1595: 14, 1596: 17, 1597: 15, 1598: 12, 1599: 20, 1600: 19, 1601: 20, 1602: 32, 1603: 14, 1604: 21, 1605: 12, 1606: 15, 1607: 19, 1608: 19, 1609: 30, 1610: 27, 1611: 15, 1612: 19, 1613: 32, 1614: 16, 1615: 31, 1616: 60, 1617: 25, 1618: 21, 1619: 28, 1620: 337, 1621: 26, 1622: 55, 1623: 24, 1624: 57, 1625: 63, 1626: 12, 1627: 82, 1628: 15, 1629: 34, 1630: 23, 1631: 58, 1632: 267, 1633: 13, 1634: 13, 1635: 19, 1636: 34, 1637: 69, 1638: 13, 1639: 27, 1640: 13, 1641: 23, 1642: 42, 1643: 27, 1644: 12, 1645: 22, 1646: 22, 1647: 64, 1648: 14, 1649: 46, 1650: 12, 1651: 99, 1652: 22, 1653: 21, 1654: 63, 1655: 223, 1656: 25, 1657: 30, 1658: 106, 1659: 62, 1660: 13, 1661: 39, 1662: 21, 1663: 14, 1664: 48, 1665: 13, 1666: 34, 1667: 11, 1668: 13, 1669: 13, 1670: 11, 1671: 12, 1672: 18, 1673: 11, 1674: 66, 1675: 66, 1676: 41, 1677: 37, 1678: 27, 1679: 18, 1680: 38, 1681: 15, 1682: 37, 1683: 12, 1684: 21, 1685: 38, 1686: 40, 1687: 18, 1688: 12, 1689: 17, 1690: 12, 1691: 20, 1692: 20, 1693: 12, 1694: 16, 1695: 14, 1696: 28, 1697: 12, 1698: 21, 1699: 12, 1700: 40, 1701: 11, 1702: 98, 1703: 17, 1704: 182, 1705: 64, 1706: 37, 1707: 38, 1708: 58, 1709: 11, 1710: 13, 1711: 24, 1712: 34, 1713: 35, 1714: 17, 1715: 36, 1716: 14, 1717: 135, 1718: 162, 1719: 20, 1720: 60, 1721: 41, 1722: 102, 1723: 79, 1724: 74, 1725: 73, 1726: 44, 1727: 11, 1728: 28, 1729: 16, 1730: 102, 1731: 68, 1732: 23, 1733: 24, 1734: 15, 1735: 80, 1736: 44, 1737: 134, 1738: 36, 1739: 17, 1740: 16, 1741: 13, 1742: 35, 1743: 17, 1744: 74, 1745: 13, 1746: 28, 1747: 81, 1748: 14, 1749: 12, 1750: 24, 1751: 12, 1752: 106, 1753: 15, 1754: 12, 1755: 15, 1756: 24, 1757: 85, 1758: 300, 1759: 81, 1760: 16, 1761: 27, 1762: 45, 1763: 155, 1764: 19, 1765: 51, 1766: 207, 1767: 16, 1768: 12, 1769: 13, 1770: 26, 1771: 43, 1772: 11, 1773: 39, 1774: 38, 1775: 136, 1776: 1153, 1777: 107, 1778: 100, 1779: 47, 1780: 400, 1781: 57, 1782: 31, 1783: 34, 1784: 36, 1785: 15, 1786: 869, 1787: 27, 1788: 97, 1789: 57, 1790: 17, 1791: 15, 1792: 79, 1793: 34, 1794: 11, 1795: 15, 1796: 11, 1797: 24, 1798: 12, 1799: 18, 1800: 27, 1801: 42, 1802: 13, 1803: 17, 1804: 51, 1805: 18, 1806: 22, 1807: 23, 1808: 53, 1809: 29, 1810: 114, 1811: 60, 1812: 72, 1813: 25, 1814: 11, 1815: 11, 1816: 31, 1817: 26, 1818: 11, 1819: 55, 1820: 134, 1821: 111, 1822: 168, 1823: 271, 1824: 11, 1825: 12, 1826: 12, 1827: 30, 1828: 11, 1829: 103, 1830: 71, 1831: 34, 1832: 27, 1833: 17, 1834: 11, 1835: 12, 1836: 24, 1837: 18, 1838: 11, 1839: 17, 1840: 32, 1841: 12, 1842: 19, 1843: 19, 1844: 13, 1845: 34, 1846: 27, 1847: 11, 1848: 46, 1849: 18, 1850: 223, 1851: 51, 1852: 28, 1853: 22, 1854: 11, 1855: 19, 1856: 98, 1857: 20, 1858: 36, 1859: 162, 1860: 458, 1861: 17, 1862: 16, 1863: 34, 1864: 15, 1865: 86, 1866: 177, 1867: 16, 1868: 105, 1869: 15, 1870: 41, 1871: 212, 1872: 134, 1873: 23, 1874: 28, 1875: 20, 1876: 20, 1877: 15, 1878: 18, 1879: 12, 1880: 15, 1881: 14, 1882: 98, 1883: 23, 1884: 14, 1885: 13, 1886: 16, 1887: 12, 1888: 23, 1889: 14, 1890: 12, 1891: 23, 1892: 42, 1893: 73, 1894: 87, 1895: 48, 1896: 56, 1897: 21, 1898: 79, 1899: 95, 1900: 63, 1901: 20, 1902: 18, 1903: 15, 1904: 11, 1905: 46, 1906: 140, 1907: 35, 1908: 35, 1909: 19, 1910: 22, 1911: 13, 1912: 13, 1913: 36, 1914: 93, 1915: 25, 1916: 23, 1917: 32, 1918: 17, 1919: 29, 1920: 31, 1921: 14, 1922: 38, 1923: 13, 1924: 22, 1925: 11, 1926: 33, 1927: 13, 1928: 48, 1929: 15, 1930: 14, 1931: 16, 1932: 42, 1933: 17, 1934: 107, 1935: 18, 1936: 146, 1937: 50, 1938: 25, 1939: 12, 1940: 21, 1941: 81, 1942: 124, 1943: 129, 1944: 25, 1945: 13, 1946: 48, 1947: 27, 1948: 30, 1949: 12, 1950: 59, 1951: 77, 1952: 23, 1953: 19, 1954: 22, 1955: 17, 1956: 22, 1957: 13, 1958: 80, 1959: 15, 1960: 99, 1961: 11, 1962: 19, 1963: 36, 1964: 29, 1965: 21, 1966: 20, 1967: 13, 1968: 93, 1969: 119, 1970: 13, 1971: 29, 1972: 20, 1973: 34, 1974: 18, 1975: 57, 1976: 18, 1977: 100, 1978: 23, 1979: 26, 1980: 15, 1981: 14, 1982: 15, 1983: 13, 1984: 15, 1985: 47, 1986: 22, 1987: 12, 1988: 23, 1989: 11, 1990: 11, 1991: 31, 1992: 22, 1993: 16, 1994: 17, 1995: 17, 1996: 20, 1997: 33, 1998: 69, 1999: 55, 2000: 28, 2001: 27, 2002: 22, 2003: 15, 2004: 29, 2005: 14, 2006: 12, 2007: 11, 2008: 78, 2009: 13, 2010: 26, 2011: 26, 2012: 16, 2013: 17, 2014: 16, 2015: 23, 2016: 25, 2017: 13, 2018: 15, 2019: 17, 2020: 15, 2021: 45, 2022: 82, 2023: 156, 2024: 12, 2025: 11, 2026: 19, 2027: 15, 2028: 11, 2029: 12, 2030: 12, 2031: 16, 2032: 22, 2033: 25, 2034: 43, 2035: 19, 2036: 11, 2037: 22, 2038: 17, 2039: 30, 2040: 24, 2041: 46, 2042: 17, 2043: 11, 2044: 53, 2045: 36, 2046: 11, 2047: 14, 2048: 11, 2049: 13, 2050: 14, 2051: 12, 2052: 14, 2053: 12, 2054: 11, 2055: 30, 2056: 21, 2057: 30, 2058: 147, 2059: 30, 2060: 12, 2061: 13, 2062: 34, 2063: 46, 2064: 113, 2065: 138, 2066: 51, 2067: 15, 2068: 12, 2069: 20, 2070: 16, 2071: 15, 2072: 11, 2073: 11, 2074: 30, 2075: 36, 2076: 46, 2077: 77, 2078: 28, 2079: 11, 2080: 28, 2081: 18, 2082: 11, 2083: 157, 2084: 15, 2085: 91, 2086: 16, 2087: 100, 2088: 105, 2089: 102, 2090: 48, 2091: 80, 2092: 27, 2093: 16, 2094: 13, 2095: 26, 2096: 18, 2097: 18, 2098: 38, 2099: 11, 2100: 15, 2101: 12, 2102: 25, 2103: 44, 2104: 17, 2105: 51, 2106: 32, 2107: 32, 2108: 27, 2109: 24, 2110: 11, 2111: 13, 2112: 26, 2113: 28, 2114: 15, 2115: 21, 2116: 12, 2117: 18, 2118: 27, 2119: 17, 2120: 19, 2121: 11, 2122: 12, 2123: 38, 2124: 14, 2125: 15, 2126: 23, 2127: 29, 2128: 15, 2129: 12, 2130: 12, 2131: 33, 2132: 13, 2133: 15, 2134: 13, 2135: 13, 2136: 11, 2137: 13, 2138: 22, 2139: 21, 2140: 27, 2141: 90, 2142: 125, 2143: 16, 2144: 48, 2145: 192, 2146: 28, 2147: 34, 2148: 16, 2149: 25, 2150: 16, 2151: 13, 2152: 13, 2153: 131, 2154: 189, 2155: 49, 2156: 263, 2157: 203, 2158: 11, 2159: 11, 2160: 121, 2161: 107, 2162: 35, 2163: 27, 2164: 46, 2165: 60, 2166: 12, 2167: 12, 2168: 68, 2169: 59, 2170: 18, 2171: 26, 2172: 24, 2173: 18, 2174: 13, 2175: 80, 2176: 30, 2177: 15, 2178: 14, 2179: 22, 2180: 15, 2181: 16, 2182: 25, 2183: 11, 2184: 40, 2185: 119, 2186: 11, 2187: 12, 2188: 35, 2189: 77, 2190: 45, 2191: 16, 2192: 19, 2193: 44, 2194: 15, 2195: 26, 2196: 17, 2197: 11, 2198: 11, 2199: 11, 2200: 23, 2201: 22, 2202: 11, 2203: 23, 2204: 13, 2205: 18, 2206: 14, 2207: 29, 2208: 19, 2209: 15, 2210: 29, 2211: 12, 2212: 67, 2213: 15, 2214: 21, 2215: 13, 2216: 11, 2217: 13, 2218: 14, 2219: 19, 2220: 11, 2221: 11, 2222: 16, 2223: 14, 2224: 11, 2225: 12, 2226: 47, 2227: 23, 2228: 14, 2229: 14, 2230: 15, 2231: 20, 2232: 27, 2233: 19, 2234: 138, 2235: 11, 2236: 11, 2237: 39, 2238: 32, 2239: 52, 2240: 11, 2241: 28, 2242: 11, 2243: 37, 2244: 12, 2245: 34, 2246: 39, 2247: 11, 2248: 18, 2249: 11, 2250: 11, 2251: 26, 2252: 19, 2253: 11, 2254: 39, 2255: 11, 2256: 30, 2257: 19, 2258: 16, 2259: 12, 2260: 17, 2261: 70, 2262: 50, 2263: 33, 2264: 15, 2265: 15, 2266: 169, 2267: 13, 2268: 14, 2269: 25, 2270: 11, 2271: 11, 2272: 11, 2273: 12, 2274: 12, 2275: 15, 2276: 24387}\n",
      "2277 unique labels are used\n",
      "0 unique labels never used\n",
      "0 unique labels used once\n",
      "0 unique labels used twice\n"
     ]
    }
   ],
   "source": [
    "print(f\"total number of steps is:\", step_count)\n",
    "print(f\"highest frequency label is {max_freq_index} and occurs {max} times\")\n",
    "print(f\"final label used is {len(label_count)-1}\")\n",
    "print(label_count)\n",
    "print(len(label_count),\"unique labels are used\")\n",
    "print(labels_never_used,\"unique labels never used\")\n",
    "print(labels_used_once, \"unique labels used once\")\n",
    "print(labels_used_twice, \"unique labels used twice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make array of label frequencies for sklearn compute_class_weight using entire dataset\n",
    "# really should be doing this for train set (otherwise, data leakage...)\n",
    "# however, train set may not include certain labels, which leads to error in compute_class_weight\n",
    "\n",
    "# make array of unique classes\n",
    "#class_num_arr = [i for i in range(len(label_count))]\n",
    "#class_num_arr = np.array(class_num_arr)\n",
    "\n",
    "# make array of all data points with labels\n",
    "#lbl_arr = np.array([])\n",
    "#for i in range(file_limit*2):\n",
    "    #for y in hidden_conc_pf_data.get(i).y:\n",
    "        #lbl_arr = np.append(lbl_arr,[y.numpy()],axis=0).astype(int)\n",
    "\n",
    "#class_weights = compute_class_weight(class_weight=\"balanced\",classes = class_num_arr, y=lbl_arr)\n",
    "#class_weights = torch.from_numpy(class_weights).float().to(device)\n",
    "#class_weights[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIN(torch.nn.Module):\n",
    "    \"\"\"GIN\"\"\"\n",
    "    def __init__(self, dim_h, norm_mode='None', norm_scale=10):\n",
    "        super(GIN, self).__init__()\n",
    "        self.conv1 = GINConv(\n",
    "            Sequential(Linear(hidden_conc_pf_data.num_node_features, dim_h),\n",
    "                       BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()))\n",
    "        \n",
    "        self.conv2 = GINConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()))\n",
    "        \n",
    "        self.conv3 = GINConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()))\n",
    "        \n",
    "        self.lin1 = Linear(dim_h*3, dim_h*3)\n",
    "        self.lin2 = Linear(dim_h*3, hidden_conc_pf_data.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \n",
    "        h1 = self.conv1(x, edge_index)\n",
    "        h2 = self.conv2(h1, edge_index)\n",
    "        h3 = self.conv3(h2, edge_index)\n",
    "\n",
    "        # Concatenate graph embeddings\n",
    "        h = torch.cat((h1, h2, h3), dim=1)\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "\n",
    "        h = self.lin1(h)\n",
    "        h = h.relu()\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "\n",
    "        h = self.lin2(h)        \n",
    "        return F.log_softmax(h, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GIN model training\n",
    "\n",
    "def train(model, loader, lr):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    # commented out code is to use class weights to account for imbalanced dataset\n",
    "    #criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "    epochs = 200\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs+1):\n",
    "        total_loss = 0\n",
    "        acc = 0\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "        #cur_graph = 0   # used to keep track of current statement to enforce preds of only PREVIOUS labels in training\n",
    "\n",
    "        # train on batches\n",
    "        for data in loader:       \n",
    "            #cur_graph += torch.max(data.batch) + 1\n",
    "            data = data.to(device, non_blocking=True)\n",
    "            data.y = data.y.to(torch.float).to(device, non_blocking=True)\n",
    "            optimizer.zero_grad()\n",
    "            length = len(loader)\n",
    "            out = model(data.x, data.edge_index.long())\n",
    "            data.y = data.y.type(torch.LongTensor).to(device, non_blocking=True)\n",
    "            out = out.type(torch.float32).to(device, non_blocking=True)\n",
    "\n",
    "            # find conclusion/final nodes in each graph to later scale\n",
    "            #conc_node_indices = []\n",
    "            #cur_graph_counter = data.batch[0].clone()\n",
    "            #for idx, graph in enumerate(data.batch.clone()):\n",
    "                #if graph > cur_graph_counter:\n",
    "                    #conc_node_indices.append(idx-1)\n",
    "                    #cur_graph_counter += 1\n",
    "            #conc_node_indices.append(len(data.batch)-1)              \n",
    "            #weighted_out = out.clone()\n",
    "            #weighted_out[conc_node_indices,:] = weighted_out[conc_node_indices,:]*.25    # scale output from final nodes\n",
    "\n",
    "            #loss = criterion(weighted_out, data.y)\n",
    "            loss = criterion(out, data.y)\n",
    "\n",
    "            total_loss += loss / length\n",
    "\n",
    "            # commented out code below is meant to enforce predictions to only come from previous theorems\n",
    "            # for dict to be properly created, you must delete and recreate test.pt by rerunning \n",
    "            # pf_data = ProofDataset(root=\"data/\",file_limit=file_limit)\n",
    "            \n",
    "            #dict = pf_data.class_corr\n",
    "            #dict_keys = [k for k in dict.keys()]            \n",
    "\n",
    "            #def return_next_lowest_idx(num):\n",
    "                #if num in dict_keys:\n",
    "                    #return dict[num]\n",
    "                #while num not in dict_keys:\n",
    "                    #try:\n",
    "                        #num -= 1\n",
    "                        #return dict[num]\n",
    "                    #except:\n",
    "                        #pass\n",
    "\n",
    "            #with torch.no_grad():\n",
    "                #cur_graph_batch.apply_(return_next_lowest_idx)\n",
    "                #masked_lbls = (torch.arange(out.size(1)) < (cur_graph_batch[..., None]+1))*(out.cpu())\n",
    "                #try:\n",
    "                    #masked_lbls = (torch.arange(out.size(1)) < (cur_graph_batch[..., None]+1))*(out.cpu())\n",
    "                    #masked_lbls = torch.where(masked_lbls==0,np.nan,masked_lbls)\n",
    "                    #masked_lbls = masked_lbls.detach().numpy()                \n",
    "                    #pred = np.nanargmax(masked_lbls,axis=1)\n",
    "                    #pred = torch.from_numpy(pred)\n",
    "                    #acc += accuracy(pred, data.y.cpu()) / length\n",
    "                #except Exception as e:\n",
    "                    #print(\"a lil error\")\n",
    "                    #out = out + .00000001\n",
    "                    #masked_lbls = (torch.arange(out.size(1)) < (cur_graph_batch[..., None]+1))*(out.cpu())\n",
    "                    #masked_lbls = torch.where(masked_lbls==0,np.nan,masked_lbls)\n",
    "                    #masked_lbls = masked_lbls.detach().numpy()\n",
    "                    #pred = np.nanargmax(masked_lbls,axis=1)\n",
    "                    #pred = torch.from_numpy(pred)\n",
    "                    #acc += accuracy(pred, data.y.cpu()) / length\n",
    "                    #out = out - .00000001\n",
    "\n",
    "            #comment out the following pred and acc lines if enforcing predictions as described above\n",
    "            pred = out.argmax(dim=1)\n",
    "            #use the following pred line instead if weighting final/conclusion node differently\n",
    "            #pred = weighted_out.argmax(dim=1)\n",
    "\n",
    "            acc += accuracy(pred, data.y) / length\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # run model on validation set\n",
    "            val_loss, val_acc, val_f1, top5_acc = test(model, val_loader)\n",
    "\n",
    "        # Print metrics every epoch\n",
    "        if(epoch % 10 == 0):\n",
    "            print(f'Epoch {epoch:>3} | Train Loss: {total_loss:.2f} | Train Acc: {acc*100:>5.2f}% | Val Loss: {val_loss:.2f} | Val Acc: {val_acc*100:.2f}% | Top5 Val Acc: {top5_acc*100:.2f}%| F Score: {val_f1:.2f}')\n",
    "            \n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, loader):\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    top5_acc = 0\n",
    "    fscore = 0    \n",
    "    \n",
    "    for data in loader:\n",
    "        data = data.to(device, non_blocking=True)\n",
    "        length = len(loader)\n",
    "        out = model(data.x, data.edge_index.long())\n",
    "        prob = torch.exp(out)\n",
    "        prob_sorted = torch.topk(prob,k=5).indices\n",
    "        data.y = data.y.type(torch.LongTensor).to(device, non_blocking=True)\n",
    "        loss += criterion(out, data.y) / length\n",
    "        pred = out.argmax(dim=1)\n",
    "        acc += accuracy(pred, data.y) / length\n",
    "        top5_acc += torch.sum(torch.sum(prob_sorted==data.y.unsqueeze(1),dim=1),dim=0) / (length*(data.y.shape[0]))\n",
    "        fscore += f1(pred.cpu(), data.y.cpu(), average='macro')    # micro looks better, but macro prob more accurate\n",
    "\n",
    "    return loss, acc, fscore, top5_acc\n",
    "\n",
    "def accuracy(pred_y, y):\n",
    "    \"\"\"Calculate accuracy.\"\"\"\n",
    "    return ((pred_y == y).sum() / len(y)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GIN(\n",
       "  (conv1): GINConv(nn=Sequential(\n",
       "    (0): Linear(in_features=512, out_features=400, bias=True)\n",
       "    (1): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=400, out_features=400, bias=True)\n",
       "    (4): ReLU()\n",
       "  ))\n",
       "  (conv2): GINConv(nn=Sequential(\n",
       "    (0): Linear(in_features=400, out_features=400, bias=True)\n",
       "    (1): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=400, out_features=400, bias=True)\n",
       "    (4): ReLU()\n",
       "  ))\n",
       "  (conv3): GINConv(nn=Sequential(\n",
       "    (0): Linear(in_features=400, out_features=400, bias=True)\n",
       "    (1): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=400, out_features=400, bias=True)\n",
       "    (4): ReLU()\n",
       "  ))\n",
       "  (lin1): Linear(in_features=1200, out_features=1200, bias=True)\n",
       "  (lin2): Linear(in_features=1200, out_features=2277, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize (and reset weights of) model\n",
    "# dim_h is hyperparameter of number of hidden layers\n",
    "\n",
    "gin_trained = None\n",
    "gin = None\n",
    "gin = GIN(dim_h=400).to(device)\n",
    "gin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | Train Loss: 7.00 | Train Acc: 16.14% | Val Loss: 5.78 | Val Acc: 17.41% | Top5 Val Acc: 30.02%| F Score: 0.00\n",
      "Epoch  10 | Train Loss: 3.46 | Train Acc: 36.80% | Val Loss: 4.02 | Val Acc: 26.29% | Top5 Val Acc: 44.22%| F Score: 0.01\n",
      "Epoch  20 | Train Loss: 2.56 | Train Acc: 44.15% | Val Loss: 3.13 | Val Acc: 31.80% | Top5 Val Acc: 55.83%| F Score: 0.09\n",
      "Epoch  30 | Train Loss: 2.17 | Train Acc: 49.55% | Val Loss: 2.65 | Val Acc: 38.58% | Top5 Val Acc: 65.62%| F Score: 0.21\n",
      "Epoch  40 | Train Loss: 1.67 | Train Acc: 57.84% | Val Loss: 2.34 | Val Acc: 44.73% | Top5 Val Acc: 71.74%| F Score: 0.39\n",
      "Epoch  50 | Train Loss: 1.42 | Train Acc: 63.07% | Val Loss: 2.32 | Val Acc: 45.10% | Top5 Val Acc: 72.93%| F Score: 0.41\n",
      "Epoch  60 | Train Loss: 1.34 | Train Acc: 65.02% | Val Loss: 2.51 | Val Acc: 43.74% | Top5 Val Acc: 70.73%| F Score: 0.40\n",
      "Epoch  70 | Train Loss: 1.14 | Train Acc: 69.26% | Val Loss: 2.27 | Val Acc: 48.69% | Top5 Val Acc: 75.89%| F Score: 0.52\n",
      "Epoch  80 | Train Loss: 0.95 | Train Acc: 74.36% | Val Loss: 3.37 | Val Acc: 38.39% | Top5 Val Acc: 62.07%| F Score: 0.33\n",
      "Epoch  90 | Train Loss: 0.72 | Train Acc: 80.15% | Val Loss: 2.43 | Val Acc: 50.23% | Top5 Val Acc: 76.71%| F Score: 0.59\n",
      "Epoch 100 | Train Loss: 0.82 | Train Acc: 78.62% | Val Loss: 2.40 | Val Acc: 53.90% | Top5 Val Acc: 78.34%| F Score: 0.67\n",
      "Epoch 110 | Train Loss: 0.46 | Train Acc: 87.56% | Val Loss: 2.91 | Val Acc: 51.53% | Top5 Val Acc: 77.62%| F Score: 0.62\n",
      "Epoch 120 | Train Loss: 0.41 | Train Acc: 89.32% | Val Loss: 2.80 | Val Acc: 54.30% | Top5 Val Acc: 78.94%| F Score: 0.68\n",
      "Epoch 130 | Train Loss: 0.36 | Train Acc: 90.34% | Val Loss: 4.55 | Val Acc: 41.10% | Top5 Val Acc: 65.90%| F Score: 0.39\n",
      "Epoch 140 | Train Loss: 0.35 | Train Acc: 91.34% | Val Loss: 4.37 | Val Acc: 41.75% | Top5 Val Acc: 65.73%| F Score: 0.44\n",
      "Epoch 150 | Train Loss: 0.52 | Train Acc: 87.57% | Val Loss: 2.98 | Val Acc: 55.11% | Top5 Val Acc: 78.63%| F Score: 0.69\n",
      "Epoch 160 | Train Loss: 0.24 | Train Acc: 94.35% | Val Loss: 4.88 | Val Acc: 42.57% | Top5 Val Acc: 68.56%| F Score: 0.47\n",
      "Epoch 170 | Train Loss: 0.54 | Train Acc: 87.79% | Val Loss: 3.28 | Val Acc: 56.06% | Top5 Val Acc: 78.35%| F Score: 0.71\n",
      "Epoch 180 | Train Loss: 0.16 | Train Acc: 96.54% | Val Loss: 3.79 | Val Acc: 55.86% | Top5 Val Acc: 78.35%| F Score: 0.71\n",
      "Epoch 190 | Train Loss: 0.15 | Train Acc: 96.79% | Val Loss: 4.03 | Val Acc: 56.14% | Top5 Val Acc: 78.24%| F Score: 0.72\n",
      "Epoch 200 | Train Loss: 0.15 | Train Acc: 96.82% | Val Loss: 3.98 | Val Acc: 55.95% | Top5 Val Acc: 78.25%| F Score: 0.72\n"
     ]
    }
   ],
   "source": [
    "# reset weights and train model\n",
    "gin_trained = None\n",
    "gin_trained = train(gin, train_loader,lr=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restricting to conclusion nodes, our val_graph has accuracy being 41.40%\n",
      "'unk' accuracy should be: 68.55%\n",
      "top5 accuracy is: 69.60%\n"
     ]
    }
   ],
   "source": [
    "# get accuracy only for conclusion node from val set\n",
    "\n",
    "total_conc_labels = len(val_dataset)\n",
    "correct_conc_pred = 0\n",
    "\n",
    "# dict of the form true_label:(predicted_labels)\n",
    "incorrect_preds = {}\n",
    "\n",
    "for graph in val_dataset:\n",
    "    get_gin_predict = gin_trained(graph.x.to(device),graph.edge_index.long().to(device))\n",
    "    get_conc_predict = np.argmax(get_gin_predict[-1].detach().cpu().numpy(),axis=0)\n",
    "    if get_conc_predict == graph.y[-1].item():\n",
    "        correct_conc_pred += 1\n",
    "    if get_conc_predict != graph.y[-1].item():\n",
    "        if graph.y[-1].item() in incorrect_preds:\n",
    "            incorrect_preds[graph.y[-1].item()] = incorrect_preds[graph.y[-1].item()]+(get_conc_predict,)\n",
    "        else:\n",
    "            incorrect_preds[graph.y[-1].item()] = (get_conc_predict,)\n",
    "\n",
    "\n",
    "def get_conc_gin_label_acc(i):\n",
    "    correct_pred_i = 0\n",
    "    total_num_i = 0\n",
    "\n",
    "    for graph in val_dataset:\n",
    "        get_gin_predict = gin_trained(graph.x.to(device),graph.edge_index.long().to(device))\n",
    "        get_conc_predict = np.argmax(get_gin_predict[-1].detach().cpu().numpy(),axis=0)\n",
    "        if graph.y[-1].item() == i:\n",
    "            total_num_i += 1\n",
    "            if get_conc_predict == i:\n",
    "                correct_pred_i += 1\n",
    "    if total_num_i == 0:\n",
    "        return 0,0,0\n",
    "    return (correct_pred_i / total_num_i), total_num_i, correct_pred_i\n",
    "\n",
    "def get_topk_acc(k):\n",
    "\n",
    "    top3_corr = 0\n",
    "\n",
    "    for graph in val_dataset:\n",
    "        get_gin_predict = gin_trained(graph.x.to(device),graph.edge_index.long().to(device))\n",
    "        prob = torch.exp(get_gin_predict[-1])\n",
    "        prob_sorted = torch.topk(prob,k=k).indices\n",
    "        graph.y = graph.y.to(device)\n",
    "        top3_corr += torch.sum(prob_sorted==graph.y[-1])\n",
    "\n",
    "    return top3_corr / len(val_dataset)\n",
    "\n",
    "k=5\n",
    "print(f\"restricting to conclusion nodes, our val_graph has accuracy being {correct_conc_pred/total_conc_labels*100:.2f}%\")\n",
    "print(f\"'unk' accuracy should be: {get_conc_gin_label_acc(2276)[0]*100:.2f}%\")\n",
    "print(f\"top{k} accuracy is: {get_topk_acc(k).item()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary to record label frequency based on val set\n",
    "# only consider conclusion/final nodes\n",
    "\n",
    "# get max label used in pf_data\n",
    "max_label = 0\n",
    "\n",
    "for i in val_indices:\n",
    "    if hidden_conc_pf_data.get(i).y[-1] > max_label:\n",
    "        max_label = hidden_conc_pf_data.get(i).y[-1].to(int).item()\n",
    "\n",
    "# initialize histogram for labels used in pf_data            \n",
    "label_count = {}\n",
    "\n",
    "for i in val_indices:\n",
    "    for j in range(max_label+1):        \n",
    "        label_count[j] = 0\n",
    "\n",
    "for i in val_indices:\n",
    "    label_count[hidden_conc_pf_data.get(i).y[-1].to(int).item()] += 1\n",
    "\n",
    "step_count = 0\n",
    "max = 0\n",
    "max_freq_index = None   # find the most frequently used index\n",
    "labels_never_used = 0\n",
    "labels_used_once = 0\n",
    "labels_used_twice = 0\n",
    "\n",
    "\n",
    "for k,v in label_count.items():\n",
    "    step_count += v\n",
    "    if v > max:     \n",
    "        max = v\n",
    "        max_freq_index = k\n",
    "\n",
    "    if v == 0:\n",
    "        labels_never_used += 1\n",
    "    if v == 1:\n",
    "        labels_used_once += 1\n",
    "    if v ==2:\n",
    "        labels_used_twice += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of steps is: 1000\n",
      "highest frequency label is 2276 and occurs 124 times\n",
      "final label used is 2276\n",
      "{0: 2, 1: 31, 2: 0, 3: 2, 4: 1, 5: 5, 6: 0, 7: 0, 8: 0, 9: 7, 10: 0, 11: 34, 12: 1, 13: 0, 14: 4, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 2, 23: 3, 24: 0, 25: 4, 26: 5, 27: 0, 28: 0, 29: 2, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 1, 37: 1, 38: 0, 39: 1, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 4, 46: 0, 47: 0, 48: 2, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 1, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 1, 70: 0, 71: 0, 72: 0, 73: 0, 74: 1, 75: 1, 76: 2, 77: 1, 78: 0, 79: 0, 80: 2, 81: 0, 82: 0, 83: 0, 84: 1, 85: 0, 86: 0, 87: 1, 88: 0, 89: 1, 90: 17, 91: 2, 92: 1, 93: 1, 94: 0, 95: 0, 96: 1, 97: 0, 98: 25, 99: 14, 100: 0, 101: 3, 102: 12, 103: 6, 104: 0, 105: 0, 106: 0, 107: 1, 108: 6, 109: 8, 110: 5, 111: 0, 112: 2, 113: 8, 114: 17, 115: 5, 116: 1, 117: 17, 118: 1, 119: 0, 120: 0, 121: 1, 122: 1, 123: 1, 124: 0, 125: 1, 126: 1, 127: 1, 128: 0, 129: 0, 130: 0, 131: 4, 132: 0, 133: 0, 134: 12, 135: 0, 136: 0, 137: 1, 138: 0, 139: 0, 140: 1, 141: 0, 142: 2, 143: 0, 144: 3, 145: 1, 146: 1, 147: 29, 148: 5, 149: 4, 150: 21, 151: 5, 152: 0, 153: 0, 154: 0, 155: 1, 156: 0, 157: 0, 158: 3, 159: 0, 160: 4, 161: 2, 162: 0, 163: 4, 164: 0, 165: 0, 166: 5, 167: 9, 168: 0, 169: 0, 170: 0, 171: 3, 172: 1, 173: 24, 174: 3, 175: 2, 176: 0, 177: 2, 178: 0, 179: 1, 180: 13, 181: 0, 182: 0, 183: 0, 184: 0, 185: 0, 186: 0, 187: 1, 188: 4, 189: 0, 190: 0, 191: 0, 192: 0, 193: 0, 194: 0, 195: 0, 196: 1, 197: 0, 198: 1, 199: 2, 200: 0, 201: 0, 202: 0, 203: 0, 204: 0, 205: 0, 206: 0, 207: 0, 208: 1, 209: 0, 210: 0, 211: 0, 212: 0, 213: 0, 214: 2, 215: 0, 216: 0, 217: 0, 218: 1, 219: 0, 220: 0, 221: 0, 222: 4, 223: 2, 224: 0, 225: 0, 226: 0, 227: 0, 228: 0, 229: 0, 230: 16, 231: 4, 232: 0, 233: 0, 234: 0, 235: 5, 236: 0, 237: 1, 238: 0, 239: 2, 240: 2, 241: 1, 242: 0, 243: 0, 244: 0, 245: 0, 246: 0, 247: 0, 248: 0, 249: 0, 250: 0, 251: 0, 252: 0, 253: 0, 254: 0, 255: 0, 256: 4, 257: 0, 258: 0, 259: 1, 260: 0, 261: 0, 262: 0, 263: 0, 264: 0, 265: 0, 266: 0, 267: 0, 268: 0, 269: 1, 270: 0, 271: 0, 272: 2, 273: 0, 274: 0, 275: 0, 276: 0, 277: 0, 278: 0, 279: 0, 280: 0, 281: 0, 282: 1, 283: 3, 284: 1, 285: 6, 286: 0, 287: 0, 288: 0, 289: 0, 290: 0, 291: 2, 292: 1, 293: 2, 294: 0, 295: 1, 296: 0, 297: 3, 298: 0, 299: 0, 300: 0, 301: 1, 302: 1, 303: 0, 304: 0, 305: 1, 306: 0, 307: 0, 308: 0, 309: 0, 310: 0, 311: 0, 312: 0, 313: 0, 314: 0, 315: 0, 316: 0, 317: 0, 318: 0, 319: 0, 320: 1, 321: 0, 322: 0, 323: 0, 324: 0, 325: 0, 326: 0, 327: 0, 328: 0, 329: 1, 330: 1, 331: 0, 332: 5, 333: 0, 334: 6, 335: 11, 336: 1, 337: 2, 338: 1, 339: 1, 340: 1, 341: 3, 342: 1, 343: 1, 344: 3, 345: 0, 346: 0, 347: 0, 348: 0, 349: 0, 350: 0, 351: 0, 352: 0, 353: 0, 354: 0, 355: 0, 356: 0, 357: 0, 358: 0, 359: 0, 360: 0, 361: 1, 362: 0, 363: 0, 364: 0, 365: 0, 366: 0, 367: 0, 368: 0, 369: 0, 370: 0, 371: 0, 372: 0, 373: 2, 374: 0, 375: 1, 376: 2, 377: 5, 378: 2, 379: 10, 380: 1, 381: 0, 382: 0, 383: 0, 384: 0, 385: 0, 386: 2, 387: 0, 388: 0, 389: 1, 390: 2, 391: 0, 392: 0, 393: 0, 394: 0, 395: 1, 396: 1, 397: 0, 398: 0, 399: 0, 400: 1, 401: 0, 402: 0, 403: 0, 404: 0, 405: 0, 406: 0, 407: 0, 408: 0, 409: 0, 410: 0, 411: 0, 412: 0, 413: 0, 414: 0, 415: 0, 416: 0, 417: 0, 418: 0, 419: 2, 420: 2, 421: 0, 422: 1, 423: 0, 424: 0, 425: 2, 426: 0, 427: 2, 428: 0, 429: 0, 430: 0, 431: 0, 432: 1, 433: 0, 434: 0, 435: 0, 436: 0, 437: 2, 438: 0, 439: 0, 440: 1, 441: 0, 442: 0, 443: 0, 444: 1, 445: 0, 446: 0, 447: 0, 448: 1, 449: 0, 450: 0, 451: 0, 452: 0, 453: 0, 454: 0, 455: 0, 456: 0, 457: 0, 458: 0, 459: 0, 460: 1, 461: 0, 462: 0, 463: 0, 464: 0, 465: 0, 466: 0, 467: 0, 468: 0, 469: 0, 470: 2, 471: 3, 472: 2, 473: 1, 474: 2, 475: 0, 476: 0, 477: 0, 478: 0, 479: 0, 480: 0, 481: 0, 482: 1, 483: 0, 484: 0, 485: 0, 486: 5, 487: 1, 488: 3, 489: 0, 490: 0, 491: 0, 492: 0, 493: 1, 494: 0, 495: 0, 496: 0, 497: 0, 498: 0, 499: 0, 500: 0, 501: 1, 502: 2, 503: 0, 504: 2, 505: 1, 506: 1, 507: 0, 508: 0, 509: 0, 510: 0, 511: 0, 512: 0, 513: 0, 514: 0, 515: 0, 516: 0, 517: 0, 518: 0, 519: 0, 520: 0, 521: 0, 522: 0, 523: 0, 524: 0, 525: 0, 526: 0, 527: 0, 528: 0, 529: 1, 530: 0, 531: 3, 532: 0, 533: 1, 534: 0, 535: 0, 536: 0, 537: 0, 538: 0, 539: 0, 540: 0, 541: 0, 542: 0, 543: 0, 544: 1, 545: 0, 546: 1, 547: 0, 548: 0, 549: 0, 550: 0, 551: 0, 552: 6, 553: 0, 554: 0, 555: 0, 556: 0, 557: 0, 558: 0, 559: 2, 560: 0, 561: 1, 562: 1, 563: 0, 564: 0, 565: 1, 566: 0, 567: 0, 568: 0, 569: 0, 570: 0, 571: 0, 572: 0, 573: 0, 574: 0, 575: 0, 576: 0, 577: 0, 578: 0, 579: 0, 580: 1, 581: 0, 582: 0, 583: 0, 584: 0, 585: 0, 586: 0, 587: 0, 588: 0, 589: 0, 590: 0, 591: 0, 592: 0, 593: 0, 594: 0, 595: 0, 596: 1, 597: 0, 598: 0, 599: 5, 600: 2, 601: 0, 602: 0, 603: 0, 604: 0, 605: 0, 606: 0, 607: 0, 608: 0, 609: 0, 610: 0, 611: 0, 612: 0, 613: 0, 614: 0, 615: 0, 616: 0, 617: 0, 618: 0, 619: 0, 620: 0, 621: 0, 622: 0, 623: 0, 624: 0, 625: 1, 626: 0, 627: 0, 628: 0, 629: 0, 630: 0, 631: 0, 632: 0, 633: 1, 634: 0, 635: 1, 636: 0, 637: 1, 638: 0, 639: 0, 640: 0, 641: 0, 642: 0, 643: 0, 644: 0, 645: 0, 646: 0, 647: 0, 648: 0, 649: 0, 650: 0, 651: 0, 652: 0, 653: 0, 654: 0, 655: 0, 656: 0, 657: 0, 658: 0, 659: 0, 660: 0, 661: 0, 662: 0, 663: 0, 664: 0, 665: 0, 666: 0, 667: 0, 668: 0, 669: 0, 670: 0, 671: 0, 672: 1, 673: 0, 674: 0, 675: 0, 676: 0, 677: 0, 678: 0, 679: 0, 680: 0, 681: 0, 682: 1, 683: 0, 684: 0, 685: 0, 686: 0, 687: 0, 688: 0, 689: 0, 690: 0, 691: 0, 692: 1, 693: 0, 694: 0, 695: 0, 696: 0, 697: 0, 698: 0, 699: 0, 700: 0, 701: 0, 702: 1, 703: 0, 704: 0, 705: 0, 706: 0, 707: 0, 708: 0, 709: 0, 710: 0, 711: 0, 712: 0, 713: 0, 714: 0, 715: 0, 716: 0, 717: 0, 718: 0, 719: 0, 720: 0, 721: 0, 722: 0, 723: 0, 724: 0, 725: 0, 726: 2, 727: 0, 728: 0, 729: 0, 730: 0, 731: 0, 732: 0, 733: 0, 734: 0, 735: 0, 736: 0, 737: 0, 738: 0, 739: 0, 740: 0, 741: 0, 742: 0, 743: 0, 744: 0, 745: 0, 746: 0, 747: 0, 748: 8, 749: 3, 750: 0, 751: 0, 752: 0, 753: 0, 754: 0, 755: 0, 756: 0, 757: 0, 758: 0, 759: 0, 760: 0, 761: 0, 762: 0, 763: 0, 764: 0, 765: 0, 766: 0, 767: 0, 768: 12, 769: 1, 770: 3, 771: 5, 772: 7, 773: 0, 774: 0, 775: 0, 776: 11, 777: 1, 778: 11, 779: 1, 780: 1, 781: 1, 782: 0, 783: 0, 784: 1, 785: 0, 786: 13, 787: 0, 788: 0, 789: 0, 790: 3, 791: 0, 792: 2, 793: 1, 794: 1, 795: 1, 796: 0, 797: 0, 798: 9, 799: 0, 800: 0, 801: 0, 802: 0, 803: 0, 804: 0, 805: 0, 806: 0, 807: 0, 808: 0, 809: 0, 810: 0, 811: 0, 812: 0, 813: 0, 814: 0, 815: 1, 816: 1, 817: 1, 818: 6, 819: 2, 820: 0, 821: 0, 822: 2, 823: 0, 824: 0, 825: 0, 826: 0, 827: 0, 828: 1, 829: 0, 830: 0, 831: 0, 832: 0, 833: 0, 834: 1, 835: 0, 836: 0, 837: 0, 838: 0, 839: 0, 840: 2, 841: 1, 842: 0, 843: 0, 844: 0, 845: 0, 846: 0, 847: 0, 848: 0, 849: 0, 850: 1, 851: 0, 852: 0, 853: 0, 854: 0, 855: 0, 856: 0, 857: 0, 858: 0, 859: 0, 860: 2, 861: 0, 862: 0, 863: 0, 864: 0, 865: 0, 866: 2, 867: 0, 868: 0, 869: 0, 870: 0, 871: 0, 872: 2, 873: 0, 874: 0, 875: 0, 876: 0, 877: 0, 878: 1, 879: 0, 880: 0, 881: 0, 882: 0, 883: 0, 884: 3, 885: 0, 886: 0, 887: 0, 888: 0, 889: 0, 890: 0, 891: 0, 892: 0, 893: 0, 894: 2, 895: 0, 896: 2, 897: 2, 898: 0, 899: 0, 900: 0, 901: 0, 902: 0, 903: 0, 904: 0, 905: 0, 906: 0, 907: 0, 908: 0, 909: 0, 910: 1, 911: 0, 912: 0, 913: 0, 914: 0, 915: 1, 916: 0, 917: 0, 918: 0, 919: 0, 920: 1, 921: 0, 922: 0, 923: 0, 924: 0, 925: 1, 926: 0, 927: 0, 928: 1, 929: 0, 930: 0, 931: 0, 932: 0, 933: 0, 934: 0, 935: 0, 936: 1, 937: 0, 938: 0, 939: 0, 940: 0, 941: 0, 942: 0, 943: 0, 944: 0, 945: 0, 946: 0, 947: 0, 948: 0, 949: 0, 950: 0, 951: 2, 952: 0, 953: 0, 954: 0, 955: 0, 956: 0, 957: 0, 958: 0, 959: 1, 960: 0, 961: 0, 962: 0, 963: 0, 964: 0, 965: 0, 966: 0, 967: 0, 968: 0, 969: 0, 970: 0, 971: 0, 972: 0, 973: 0, 974: 0, 975: 0, 976: 0, 977: 0, 978: 0, 979: 0, 980: 0, 981: 0, 982: 0, 983: 0, 984: 0, 985: 0, 986: 0, 987: 0, 988: 0, 989: 0, 990: 0, 991: 0, 992: 0, 993: 0, 994: 0, 995: 0, 996: 0, 997: 0, 998: 0, 999: 0, 1000: 0, 1001: 0, 1002: 1, 1003: 0, 1004: 0, 1005: 0, 1006: 0, 1007: 0, 1008: 0, 1009: 0, 1010: 1, 1011: 3, 1012: 0, 1013: 0, 1014: 0, 1015: 0, 1016: 0, 1017: 0, 1018: 0, 1019: 0, 1020: 0, 1021: 0, 1022: 0, 1023: 0, 1024: 0, 1025: 0, 1026: 0, 1027: 0, 1028: 0, 1029: 0, 1030: 0, 1031: 0, 1032: 1, 1033: 0, 1034: 1, 1035: 2, 1036: 0, 1037: 0, 1038: 0, 1039: 0, 1040: 0, 1041: 0, 1042: 0, 1043: 0, 1044: 0, 1045: 0, 1046: 0, 1047: 0, 1048: 0, 1049: 0, 1050: 0, 1051: 0, 1052: 0, 1053: 0, 1054: 0, 1055: 0, 1056: 0, 1057: 0, 1058: 0, 1059: 0, 1060: 0, 1061: 0, 1062: 0, 1063: 0, 1064: 0, 1065: 0, 1066: 0, 1067: 0, 1068: 0, 1069: 0, 1070: 0, 1071: 0, 1072: 0, 1073: 0, 1074: 0, 1075: 0, 1076: 0, 1077: 0, 1078: 0, 1079: 0, 1080: 0, 1081: 0, 1082: 0, 1083: 0, 1084: 0, 1085: 1, 1086: 0, 1087: 0, 1088: 0, 1089: 0, 1090: 2, 1091: 4, 1092: 0, 1093: 0, 1094: 0, 1095: 1, 1096: 0, 1097: 0, 1098: 0, 1099: 2, 1100: 3, 1101: 0, 1102: 0, 1103: 0, 1104: 0, 1105: 0, 1106: 0, 1107: 0, 1108: 0, 1109: 0, 1110: 0, 1111: 0, 1112: 2, 1113: 0, 1114: 1, 1115: 1, 1116: 1, 1117: 0, 1118: 1, 1119: 0, 1120: 2, 1121: 0, 1122: 2, 1123: 0, 1124: 0, 1125: 1, 1126: 0, 1127: 0, 1128: 0, 1129: 0, 1130: 0, 1131: 0, 1132: 0, 1133: 0, 1134: 0, 1135: 0, 1136: 0, 1137: 0, 1138: 0, 1139: 0, 1140: 0, 1141: 0, 1142: 0, 1143: 0, 1144: 0, 1145: 0, 1146: 0, 1147: 0, 1148: 0, 1149: 0, 1150: 0, 1151: 0, 1152: 0, 1153: 0, 1154: 0, 1155: 0, 1156: 0, 1157: 0, 1158: 0, 1159: 0, 1160: 0, 1161: 0, 1162: 0, 1163: 0, 1164: 0, 1165: 0, 1166: 0, 1167: 0, 1168: 0, 1169: 0, 1170: 0, 1171: 0, 1172: 0, 1173: 0, 1174: 0, 1175: 0, 1176: 0, 1177: 0, 1178: 0, 1179: 0, 1180: 0, 1181: 0, 1182: 0, 1183: 0, 1184: 0, 1185: 0, 1186: 0, 1187: 0, 1188: 0, 1189: 0, 1190: 0, 1191: 0, 1192: 0, 1193: 0, 1194: 0, 1195: 0, 1196: 0, 1197: 0, 1198: 0, 1199: 0, 1200: 0, 1201: 0, 1202: 0, 1203: 0, 1204: 0, 1205: 0, 1206: 0, 1207: 0, 1208: 0, 1209: 0, 1210: 0, 1211: 0, 1212: 0, 1213: 0, 1214: 0, 1215: 0, 1216: 0, 1217: 0, 1218: 0, 1219: 0, 1220: 0, 1221: 0, 1222: 0, 1223: 0, 1224: 0, 1225: 0, 1226: 0, 1227: 0, 1228: 0, 1229: 0, 1230: 0, 1231: 0, 1232: 0, 1233: 0, 1234: 0, 1235: 0, 1236: 0, 1237: 0, 1238: 0, 1239: 0, 1240: 0, 1241: 0, 1242: 0, 1243: 0, 1244: 0, 1245: 0, 1246: 0, 1247: 0, 1248: 0, 1249: 0, 1250: 0, 1251: 0, 1252: 0, 1253: 0, 1254: 0, 1255: 0, 1256: 0, 1257: 0, 1258: 0, 1259: 0, 1260: 0, 1261: 0, 1262: 0, 1263: 0, 1264: 0, 1265: 0, 1266: 0, 1267: 0, 1268: 0, 1269: 0, 1270: 0, 1271: 0, 1272: 0, 1273: 0, 1274: 0, 1275: 0, 1276: 0, 1277: 0, 1278: 0, 1279: 0, 1280: 0, 1281: 0, 1282: 0, 1283: 0, 1284: 0, 1285: 0, 1286: 0, 1287: 0, 1288: 0, 1289: 0, 1290: 0, 1291: 0, 1292: 0, 1293: 0, 1294: 0, 1295: 0, 1296: 0, 1297: 0, 1298: 0, 1299: 0, 1300: 0, 1301: 0, 1302: 0, 1303: 0, 1304: 0, 1305: 0, 1306: 0, 1307: 0, 1308: 0, 1309: 0, 1310: 0, 1311: 0, 1312: 0, 1313: 0, 1314: 0, 1315: 0, 1316: 0, 1317: 0, 1318: 0, 1319: 0, 1320: 0, 1321: 0, 1322: 0, 1323: 0, 1324: 0, 1325: 0, 1326: 0, 1327: 0, 1328: 0, 1329: 0, 1330: 0, 1331: 0, 1332: 0, 1333: 0, 1334: 0, 1335: 0, 1336: 0, 1337: 0, 1338: 0, 1339: 0, 1340: 0, 1341: 0, 1342: 0, 1343: 0, 1344: 0, 1345: 0, 1346: 0, 1347: 0, 1348: 0, 1349: 0, 1350: 0, 1351: 0, 1352: 0, 1353: 0, 1354: 0, 1355: 0, 1356: 0, 1357: 0, 1358: 0, 1359: 0, 1360: 0, 1361: 0, 1362: 0, 1363: 0, 1364: 0, 1365: 0, 1366: 0, 1367: 0, 1368: 0, 1369: 0, 1370: 0, 1371: 0, 1372: 0, 1373: 0, 1374: 0, 1375: 0, 1376: 0, 1377: 0, 1378: 0, 1379: 0, 1380: 0, 1381: 0, 1382: 0, 1383: 1, 1384: 1, 1385: 1, 1386: 0, 1387: 0, 1388: 0, 1389: 0, 1390: 0, 1391: 0, 1392: 0, 1393: 0, 1394: 0, 1395: 0, 1396: 0, 1397: 0, 1398: 0, 1399: 0, 1400: 0, 1401: 0, 1402: 0, 1403: 0, 1404: 0, 1405: 0, 1406: 0, 1407: 0, 1408: 0, 1409: 0, 1410: 0, 1411: 0, 1412: 0, 1413: 0, 1414: 0, 1415: 0, 1416: 0, 1417: 0, 1418: 0, 1419: 0, 1420: 1, 1421: 0, 1422: 0, 1423: 0, 1424: 0, 1425: 0, 1426: 0, 1427: 1, 1428: 0, 1429: 0, 1430: 0, 1431: 0, 1432: 0, 1433: 0, 1434: 0, 1435: 0, 1436: 0, 1437: 0, 1438: 0, 1439: 0, 1440: 0, 1441: 0, 1442: 0, 1443: 0, 1444: 0, 1445: 0, 1446: 0, 1447: 0, 1448: 0, 1449: 0, 1450: 0, 1451: 0, 1452: 0, 1453: 0, 1454: 0, 1455: 0, 1456: 0, 1457: 0, 1458: 0, 1459: 0, 1460: 0, 1461: 0, 1462: 0, 1463: 0, 1464: 0, 1465: 0, 1466: 0, 1467: 0, 1468: 0, 1469: 0, 1470: 0, 1471: 0, 1472: 0, 1473: 0, 1474: 0, 1475: 0, 1476: 0, 1477: 0, 1478: 0, 1479: 0, 1480: 0, 1481: 0, 1482: 0, 1483: 0, 1484: 0, 1485: 0, 1486: 0, 1487: 0, 1488: 0, 1489: 0, 1490: 0, 1491: 0, 1492: 0, 1493: 0, 1494: 0, 1495: 0, 1496: 0, 1497: 0, 1498: 0, 1499: 0, 1500: 0, 1501: 0, 1502: 0, 1503: 0, 1504: 0, 1505: 0, 1506: 0, 1507: 0, 1508: 0, 1509: 0, 1510: 0, 1511: 0, 1512: 0, 1513: 0, 1514: 0, 1515: 0, 1516: 1, 1517: 0, 1518: 0, 1519: 0, 1520: 0, 1521: 0, 1522: 0, 1523: 0, 1524: 0, 1525: 0, 1526: 0, 1527: 0, 1528: 0, 1529: 0, 1530: 0, 1531: 0, 1532: 0, 1533: 0, 1534: 0, 1535: 0, 1536: 0, 1537: 0, 1538: 0, 1539: 0, 1540: 0, 1541: 0, 1542: 0, 1543: 0, 1544: 0, 1545: 0, 1546: 0, 1547: 0, 1548: 0, 1549: 0, 1550: 0, 1551: 0, 1552: 0, 1553: 0, 1554: 0, 1555: 0, 1556: 0, 1557: 0, 1558: 0, 1559: 0, 1560: 0, 1561: 0, 1562: 0, 1563: 0, 1564: 0, 1565: 0, 1566: 0, 1567: 0, 1568: 0, 1569: 0, 1570: 0, 1571: 0, 1572: 0, 1573: 0, 1574: 0, 1575: 0, 1576: 0, 1577: 0, 1578: 0, 1579: 0, 1580: 0, 1581: 0, 1582: 0, 1583: 0, 1584: 0, 1585: 0, 1586: 0, 1587: 0, 1588: 0, 1589: 0, 1590: 0, 1591: 0, 1592: 0, 1593: 0, 1594: 0, 1595: 0, 1596: 0, 1597: 0, 1598: 0, 1599: 0, 1600: 0, 1601: 0, 1602: 0, 1603: 0, 1604: 0, 1605: 0, 1606: 0, 1607: 0, 1608: 0, 1609: 0, 1610: 0, 1611: 0, 1612: 0, 1613: 0, 1614: 0, 1615: 0, 1616: 0, 1617: 0, 1618: 0, 1619: 0, 1620: 0, 1621: 0, 1622: 0, 1623: 0, 1624: 0, 1625: 0, 1626: 0, 1627: 0, 1628: 0, 1629: 0, 1630: 0, 1631: 0, 1632: 0, 1633: 0, 1634: 0, 1635: 0, 1636: 0, 1637: 0, 1638: 0, 1639: 0, 1640: 0, 1641: 0, 1642: 0, 1643: 0, 1644: 0, 1645: 0, 1646: 0, 1647: 0, 1648: 0, 1649: 0, 1650: 0, 1651: 0, 1652: 0, 1653: 0, 1654: 0, 1655: 0, 1656: 0, 1657: 0, 1658: 0, 1659: 0, 1660: 0, 1661: 0, 1662: 0, 1663: 0, 1664: 0, 1665: 0, 1666: 0, 1667: 0, 1668: 0, 1669: 0, 1670: 0, 1671: 0, 1672: 0, 1673: 0, 1674: 0, 1675: 0, 1676: 0, 1677: 0, 1678: 0, 1679: 0, 1680: 0, 1681: 0, 1682: 0, 1683: 0, 1684: 0, 1685: 0, 1686: 0, 1687: 0, 1688: 0, 1689: 0, 1690: 0, 1691: 0, 1692: 0, 1693: 0, 1694: 0, 1695: 0, 1696: 0, 1697: 0, 1698: 0, 1699: 0, 1700: 0, 1701: 0, 1702: 0, 1703: 0, 1704: 0, 1705: 0, 1706: 1, 1707: 0, 1708: 0, 1709: 0, 1710: 0, 1711: 0, 1712: 0, 1713: 0, 1714: 0, 1715: 0, 1716: 0, 1717: 0, 1718: 0, 1719: 0, 1720: 0, 1721: 0, 1722: 0, 1723: 0, 1724: 0, 1725: 0, 1726: 0, 1727: 0, 1728: 0, 1729: 0, 1730: 0, 1731: 0, 1732: 0, 1733: 0, 1734: 0, 1735: 0, 1736: 0, 1737: 0, 1738: 0, 1739: 0, 1740: 0, 1741: 0, 1742: 0, 1743: 0, 1744: 0, 1745: 0, 1746: 0, 1747: 0, 1748: 0, 1749: 0, 1750: 0, 1751: 0, 1752: 0, 1753: 0, 1754: 0, 1755: 0, 1756: 0, 1757: 0, 1758: 0, 1759: 0, 1760: 0, 1761: 0, 1762: 0, 1763: 0, 1764: 0, 1765: 0, 1766: 0, 1767: 0, 1768: 0, 1769: 0, 1770: 0, 1771: 0, 1772: 0, 1773: 0, 1774: 0, 1775: 0, 1776: 0, 1777: 0, 1778: 0, 1779: 0, 1780: 0, 1781: 0, 1782: 0, 1783: 0, 1784: 0, 1785: 0, 1786: 0, 1787: 0, 1788: 0, 1789: 1, 1790: 0, 1791: 0, 1792: 0, 1793: 0, 1794: 0, 1795: 0, 1796: 0, 1797: 0, 1798: 0, 1799: 0, 1800: 0, 1801: 0, 1802: 0, 1803: 0, 1804: 0, 1805: 0, 1806: 0, 1807: 0, 1808: 0, 1809: 0, 1810: 0, 1811: 0, 1812: 0, 1813: 0, 1814: 1, 1815: 0, 1816: 0, 1817: 0, 1818: 0, 1819: 0, 1820: 0, 1821: 0, 1822: 0, 1823: 0, 1824: 0, 1825: 1, 1826: 0, 1827: 0, 1828: 0, 1829: 1, 1830: 0, 1831: 0, 1832: 0, 1833: 0, 1834: 0, 1835: 0, 1836: 0, 1837: 0, 1838: 0, 1839: 0, 1840: 0, 1841: 0, 1842: 0, 1843: 0, 1844: 0, 1845: 0, 1846: 0, 1847: 0, 1848: 0, 1849: 0, 1850: 0, 1851: 0, 1852: 0, 1853: 0, 1854: 0, 1855: 0, 1856: 0, 1857: 0, 1858: 0, 1859: 0, 1860: 0, 1861: 0, 1862: 0, 1863: 0, 1864: 0, 1865: 0, 1866: 0, 1867: 0, 1868: 0, 1869: 0, 1870: 0, 1871: 0, 1872: 0, 1873: 0, 1874: 0, 1875: 0, 1876: 0, 1877: 1, 1878: 0, 1879: 0, 1880: 0, 1881: 1, 1882: 0, 1883: 0, 1884: 0, 1885: 0, 1886: 0, 1887: 0, 1888: 0, 1889: 0, 1890: 0, 1891: 0, 1892: 0, 1893: 0, 1894: 0, 1895: 0, 1896: 0, 1897: 0, 1898: 0, 1899: 0, 1900: 0, 1901: 0, 1902: 0, 1903: 0, 1904: 0, 1905: 0, 1906: 0, 1907: 0, 1908: 0, 1909: 0, 1910: 0, 1911: 0, 1912: 0, 1913: 0, 1914: 0, 1915: 0, 1916: 0, 1917: 0, 1918: 0, 1919: 0, 1920: 0, 1921: 0, 1922: 0, 1923: 0, 1924: 0, 1925: 0, 1926: 0, 1927: 0, 1928: 0, 1929: 0, 1930: 2, 1931: 0, 1932: 0, 1933: 0, 1934: 0, 1935: 0, 1936: 0, 1937: 0, 1938: 0, 1939: 0, 1940: 0, 1941: 0, 1942: 0, 1943: 0, 1944: 1, 1945: 1, 1946: 0, 1947: 0, 1948: 0, 1949: 0, 1950: 0, 1951: 0, 1952: 0, 1953: 0, 1954: 0, 1955: 0, 1956: 0, 1957: 0, 1958: 0, 1959: 0, 1960: 0, 1961: 0, 1962: 0, 1963: 0, 1964: 0, 1965: 0, 1966: 0, 1967: 0, 1968: 0, 1969: 0, 1970: 0, 1971: 0, 1972: 0, 1973: 0, 1974: 0, 1975: 0, 1976: 0, 1977: 0, 1978: 0, 1979: 0, 1980: 0, 1981: 0, 1982: 0, 1983: 0, 1984: 0, 1985: 0, 1986: 0, 1987: 0, 1988: 0, 1989: 0, 1990: 0, 1991: 0, 1992: 0, 1993: 0, 1994: 0, 1995: 0, 1996: 0, 1997: 0, 1998: 0, 1999: 0, 2000: 0, 2001: 0, 2002: 0, 2003: 0, 2004: 0, 2005: 0, 2006: 0, 2007: 0, 2008: 0, 2009: 1, 2010: 0, 2011: 0, 2012: 0, 2013: 0, 2014: 0, 2015: 0, 2016: 0, 2017: 0, 2018: 0, 2019: 0, 2020: 0, 2021: 0, 2022: 0, 2023: 0, 2024: 0, 2025: 0, 2026: 0, 2027: 0, 2028: 0, 2029: 0, 2030: 0, 2031: 0, 2032: 0, 2033: 0, 2034: 0, 2035: 0, 2036: 0, 2037: 0, 2038: 0, 2039: 0, 2040: 0, 2041: 0, 2042: 0, 2043: 0, 2044: 0, 2045: 0, 2046: 0, 2047: 0, 2048: 0, 2049: 0, 2050: 0, 2051: 0, 2052: 0, 2053: 0, 2054: 0, 2055: 0, 2056: 0, 2057: 0, 2058: 0, 2059: 0, 2060: 0, 2061: 0, 2062: 0, 2063: 0, 2064: 0, 2065: 0, 2066: 0, 2067: 0, 2068: 0, 2069: 0, 2070: 0, 2071: 0, 2072: 0, 2073: 0, 2074: 0, 2075: 0, 2076: 0, 2077: 0, 2078: 0, 2079: 0, 2080: 0, 2081: 0, 2082: 0, 2083: 0, 2084: 0, 2085: 0, 2086: 0, 2087: 0, 2088: 0, 2089: 0, 2090: 0, 2091: 0, 2092: 0, 2093: 0, 2094: 0, 2095: 0, 2096: 0, 2097: 0, 2098: 0, 2099: 0, 2100: 0, 2101: 0, 2102: 0, 2103: 0, 2104: 0, 2105: 0, 2106: 0, 2107: 0, 2108: 0, 2109: 0, 2110: 0, 2111: 0, 2112: 0, 2113: 0, 2114: 0, 2115: 0, 2116: 0, 2117: 0, 2118: 0, 2119: 0, 2120: 0, 2121: 0, 2122: 0, 2123: 0, 2124: 0, 2125: 1, 2126: 0, 2127: 0, 2128: 0, 2129: 0, 2130: 0, 2131: 0, 2132: 0, 2133: 0, 2134: 0, 2135: 0, 2136: 0, 2137: 0, 2138: 0, 2139: 0, 2140: 0, 2141: 0, 2142: 0, 2143: 0, 2144: 0, 2145: 0, 2146: 0, 2147: 0, 2148: 0, 2149: 0, 2150: 0, 2151: 0, 2152: 0, 2153: 0, 2154: 0, 2155: 0, 2156: 0, 2157: 0, 2158: 0, 2159: 0, 2160: 0, 2161: 0, 2162: 0, 2163: 0, 2164: 0, 2165: 0, 2166: 0, 2167: 0, 2168: 0, 2169: 0, 2170: 0, 2171: 0, 2172: 0, 2173: 0, 2174: 0, 2175: 0, 2176: 0, 2177: 0, 2178: 0, 2179: 0, 2180: 0, 2181: 0, 2182: 0, 2183: 0, 2184: 0, 2185: 0, 2186: 0, 2187: 0, 2188: 0, 2189: 0, 2190: 0, 2191: 0, 2192: 0, 2193: 0, 2194: 0, 2195: 0, 2196: 0, 2197: 0, 2198: 0, 2199: 0, 2200: 0, 2201: 0, 2202: 0, 2203: 0, 2204: 0, 2205: 0, 2206: 0, 2207: 0, 2208: 0, 2209: 0, 2210: 0, 2211: 0, 2212: 0, 2213: 0, 2214: 0, 2215: 0, 2216: 0, 2217: 0, 2218: 0, 2219: 0, 2220: 0, 2221: 0, 2222: 0, 2223: 0, 2224: 0, 2225: 0, 2226: 0, 2227: 0, 2228: 0, 2229: 0, 2230: 0, 2231: 0, 2232: 0, 2233: 0, 2234: 0, 2235: 0, 2236: 0, 2237: 0, 2238: 0, 2239: 0, 2240: 0, 2241: 0, 2242: 0, 2243: 0, 2244: 0, 2245: 0, 2246: 0, 2247: 0, 2248: 2, 2249: 0, 2250: 0, 2251: 0, 2252: 0, 2253: 0, 2254: 0, 2255: 0, 2256: 0, 2257: 0, 2258: 0, 2259: 0, 2260: 0, 2261: 0, 2262: 0, 2263: 0, 2264: 0, 2265: 0, 2266: 0, 2267: 0, 2268: 0, 2269: 0, 2270: 0, 2271: 0, 2272: 0, 2273: 0, 2274: 0, 2275: 0, 2276: 124}\n",
      "2277 unique labels are used\n",
      "1994 unique labels never used\n",
      "146 unique labels used once\n",
      "59 unique labels used twice\n"
     ]
    }
   ],
   "source": [
    "print(f\"total number of steps is:\", step_count)\n",
    "print(f\"highest frequency label is {max_freq_index} and occurs {max} times\")\n",
    "print(f\"final label used is {len(label_count)-1}\")\n",
    "print(label_count)\n",
    "print(len(label_count),\"unique labels are used\")\n",
    "print(labels_never_used,\"unique labels never used\")\n",
    "print(labels_used_once, \"unique labels used once\")\n",
    "print(labels_used_twice, \"unique labels used twice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make acc dict for val conclusion nodes\n",
    "\n",
    "conc_label_acc_dict = {}\n",
    "\n",
    "def get_conc_gin_label_acc(i):\n",
    "    correct_pred_i = 0\n",
    "    total_num_i = 0\n",
    "\n",
    "    for graph in val_dataset:\n",
    "        get_gin_predict = gin_trained(graph.x.to(device),graph.edge_index.long().to(device))\n",
    "        get_conc_predict = np.argmax(get_gin_predict[-1].detach().cpu().numpy(),axis=0)\n",
    "        if graph.y[-1].item() == i:\n",
    "            total_num_i += 1\n",
    "            if get_conc_predict == i:\n",
    "                correct_pred_i += 1\n",
    "    if total_num_i == 0:\n",
    "        return 0,0,0\n",
    "    return (correct_pred_i / total_num_i), total_num_i, correct_pred_i\n",
    "\n",
    "\n",
    "total_conc_labels = 0\n",
    "total_conc_correct_preds = 0\n",
    "for i in range(hidden_conc_pf_data.num_classes):\n",
    "    if i % 200 ==0:\n",
    "        print(\"on label\",i)\n",
    "    try:\n",
    "        container = get_conc_gin_label_acc(i)\n",
    "        if container[1] != 0:\n",
    "            conc_label_acc_dict[i] = (container[0],container[1])  # pair of accuracy and label count for each label\n",
    "        total_conc_labels += container[1]\n",
    "        total_conc_correct_preds += container[2]\n",
    "    except Exception as e:\n",
    "        print(e,get_conc_gin_label_acc(i))\n",
    "\n",
    "print(conc_label_acc_dict)\n",
    "print(\"if everything here is correct, our conc node val_acc should be:\",total_conc_correct_preds/total_conc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the above dict by accuracy\n",
    "\n",
    "sorted_conc_label_acc_dict = {k: v for k, v in sorted(conc_label_acc_dict.items(), key=lambda item: item[1])}\n",
    "sorted_conc_label_acc_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
