{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from use_dataset import ProofDataset, HiddenConcProofDataset\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import random\n",
    "from sklearn.metrics import f1_score as f1\n",
    "#from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, ReLU, BatchNorm1d, Sequential\n",
    "from torch_geometric.nn import GINConv\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing graph 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jared\\Documents\\GitHub\\proof-truth\\Model\\USE GIN model\\use_dataset.py:81: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:278.)\n",
      "  x2_emb = torch.tensor([x2_emb]).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing graph 1000\n",
      "processing graph 2000\n",
      "processing graph 3000\n",
      "processing graph 4000\n",
      "processing graph 5000\n",
      "processing graph 6000\n",
      "processing graph 7000\n",
      "processing graph 8000\n",
      "processing graph 9000\n",
      "class_corr dict is of form (old_label, new_label): {0: 0, 5: 1, 6: 2, 9: 3, 10: 4, 11: 5, 12: 6, 13: 7, 14: 8, 15: 9, 16: 10, 17: 11, 18: 12, 19: 13, 20: 14, 21: 15, 22: 16, 24: 17, 25: 18, 26: 19, 29: 20, 30: 21, 31: 22, 32: 23, 33: 24, 34: 25, 35: 26, 36: 27, 37: 28, 38: 29, 39: 30, 40: 31, 41: 32, 42: 33, 43: 34, 44: 35, 45: 36, 47: 37, 50: 38, 52: 39, 53: 40, 54: 41, 55: 42, 57: 43, 59: 44, 61: 45, 62: 46, 63: 47, 65: 48, 68: 49, 73: 50, 74: 51, 76: 52, 77: 53, 78: 54, 82: 55, 83: 56, 86: 57, 87: 58, 88: 59, 89: 60, 91: 61, 92: 62, 94: 63, 95: 64, 96: 65, 114: 66, 115: 67, 119: 68, 121: 69, 123: 70, 124: 71, 127: 72, 136: 73, 138: 74, 140: 75, 141: 76, 142: 77, 143: 78, 144: 79, 147: 80, 149: 81, 150: 82, 155: 83, 157: 84, 160: 85, 162: 86, 182: 87, 183: 88, 184: 89, 185: 90, 186: 91, 189: 92, 197: 93, 198: 94, 200: 95, 201: 96, 202: 97, 212: 98, 215: 99, 218: 100, 219: 101, 220: 102, 221: 103, 222: 104, 223: 105, 225: 106, 226: 107, 227: 108, 228: 109, 229: 110, 231: 111, 232: 112, 233: 113, 234: 114, 235: 115, 236: 116, 237: 117, 238: 118, 242: 119, 243: 120, 245: 121, 246: 122, 247: 123, 248: 124, 249: 125, 250: 126, 251: 127, 252: 128, 253: 129, 254: 130, 255: 131, 256: 132, 257: 133, 260: 134, 261: 135, 262: 136, 263: 137, 264: 138, 265: 139, 267: 140, 268: 141, 270: 142, 271: 143, 274: 144, 275: 145, 276: 146, 278: 147, 279: 148, 280: 149, 281: 150, 282: 151, 283: 152, 284: 153, 285: 154, 286: 155, 287: 156, 288: 157, 290: 158, 291: 159, 292: 160, 293: 161, 294: 162, 295: 163, 297: 164, 298: 165, 299: 166, 300: 167, 301: 168, 302: 169, 303: 170, 304: 171, 305: 172, 306: 173, 307: 174, 308: 175, 312: 176, 314: 177, 315: 178, 316: 179, 317: 180, 318: 181, 319: 182, 320: 183, 321: 184, 322: 185, 323: 186, 324: 187, 326: 188, 327: 189, 328: 190, 329: 191, 330: 192, 331: 193, 332: 194, 333: 195, 334: 196, 336: 197, 337: 198, 338: 199, 339: 200, 341: 201, 342: 202, 343: 203, 344: 204, 345: 205, 346: 206, 347: 207, 348: 208, 349: 209, 353: 210, 354: 211, 358: 212, 360: 213, 361: 214, 363: 215, 364: 216, 365: 217, 366: 218, 376: 219, 380: 220, 382: 221, 383: 222, 384: 223, 392: 224, 400: 225, 403: 226, 405: 227, 406: 228, 407: 229, 410: 230, 411: 231, 412: 232, 414: 233, 415: 234, 416: 235, 417: 236, 419: 237, 420: 238, 421: 239, 422: 240, 423: 241, 424: 242, 425: 243, 426: 244, 434: 245, 435: 246, 440: 247, 454: 248, 455: 249, 456: 250, 457: 251, 458: 252, 459: 253, 460: 254, 461: 255, 462: 256, 464: 257, 465: 258, 466: 259, 469: 260, 470: 261, 471: 262, 472: 263, 473: 264, 474: 265, 475: 266, 478: 267, 480: 268, 481: 269, 482: 270, 483: 271, 484: 272, 485: 273, 486: 274, 487: 275, 488: 276, 489: 277, 492: 278, 493: 279, 494: 280, 495: 281, 498: 282, 499: 283, 500: 284, 501: 285, 502: 286, 503: 287, 504: 288, 507: 289, 508: 290, 509: 291, 510: 292, 511: 293, 512: 294, 513: 295, 514: 296, 515: 297, 516: 298, 517: 299, 518: 300, 523: 301, 524: 302, 528: 303, 529: 304, 530: 305, 531: 306, 532: 307, 533: 308, 534: 309, 535: 310, 536: 311, 539: 312, 540: 313, 552: 314, 553: 315, 554: 316, 555: 317, 559: 318, 561: 319, 563: 320, 564: 321, 566: 322, 567: 323, 568: 324, 570: 325, 572: 326, 577: 327, 578: 328, 579: 329, 580: 330, 582: 331, 583: 332, 584: 333, 586: 334, 587: 335, 589: 336, 590: 337, 591: 338, 593: 339, 594: 340, 595: 341, 596: 342, 597: 343, 598: 344, 599: 345, 600: 346, 611: 347, 613: 348, 614: 349, 615: 350, 616: 351, 617: 352, 619: 353, 621: 354, 622: 355, 625: 356, 626: 357, 627: 358, 629: 359, 631: 360, 632: 361, 633: 362, 638: 363, 644: 364, 645: 365, 649: 366, 651: 367, 655: 368, 659: 369, 675: 370, 679: 371, 683: 372, 684: 373, 685: 374, 686: 375, 687: 376, 689: 377, 690: 378, 691: 379, 694: 380, 698: 381, 702: 382, 703: 383, 708: 384, 709: 385, 710: 386, 711: 387, 712: 388, 713: 389, 714: 390, 715: 391, 716: 392, 719: 393, 720: 394, 725: 395, 726: 396, 727: 397, 728: 398, 729: 399, 730: 400, 745: 401, 746: 402, 747: 403, 766: 404, 768: 405, 769: 406, 770: 407, 771: 408, 772: 409, 774: 410, 775: 411, 776: 412, 777: 413, 778: 414, 779: 415, 780: 416, 781: 417, 782: 418, 800: 419, 801: 420, 803: 421, 811: 422, 812: 423, 815: 424, 816: 425, 835: 426, 836: 427, 837: 428, 845: 429, 848: 430, 850: 431, 854: 432, 856: 433, 857: 434, 858: 435, 859: 436, 860: 437, 861: 438, 864: 439, 865: 440, 867: 441, 868: 442, 870: 443, 871: 444, 886: 445, 902: 446, 906: 447, 908: 448, 910: 449, 911: 450, 912: 451, 913: 452, 914: 453, 916: 454, 919: 455, 934: 456, 952: 457, 955: 458, 956: 459, 962: 460, 979: 461, 981: 462, 1000: 463, 1005: 464, 1006: 465, 1085: 466, 1086: 467, 1087: 468, 1092: 469, 1107: 470, 1108: 471, 1112: 472, 1113: 473, 1114: 474, 1115: 475, 1116: 476, 1117: 477, 1118: 478, 1119: 479, 1120: 480, 1123: 481, 1125: 482, 1127: 483, 1128: 484, 1129: 485, 1130: 486, 1131: 487, 1132: 488, 1133: 489, 1134: 490, 1135: 491, 1139: 492, 1140: 493, 1141: 494, 1144: 495, 1145: 496, 1147: 497, 1152: 498, 1157: 499, 1160: 500, 1161: 501, 1162: 502, 1182: 503, 1183: 504, 1184: 505, 1185: 506, 1188: 507, 1189: 508, 1190: 509, 1191: 510, 1192: 511, 1193: 512, 1194: 513, 1195: 514, 1196: 515, 1197: 516, 1198: 517, 1199: 518, 1209: 519, 1210: 520, 1211: 521, 1217: 522, 1221: 523, 1223: 524, 1224: 525, 1225: 526, 1226: 527, 1336: 528, 1340: 529, 1345: 530, 1368: 531, 1369: 532, 1370: 533, 1424: 534, 1425: 535, 1432: 536, 1433: 537, 1435: 538, 1436: 539, 1439: 540, 1445: 541, 1446: 542, 1447: 543, 1448: 544, 1450: 545, 1458: 546, 1463: 547, 1483: 548, 1503: 549, 1522: 550, 1542: 551, 1545: 552, 1548: 553, 1552: 554, 1623: 555, 1629: 556, 1643: 557, 1657: 558, 1660: 559, 1671: 560, 1673: 561, 1677: 562, 1696: 563, 1702: 564, 1706: 565, 1715: 566, 1738: 567, 1752: 568, 1753: 569, 1755: 570, 1756: 571, 1757: 572, 1758: 573, 1782: 574, 1783: 575, 1786: 576, 1797: 577, 1798: 578, 1799: 579, 1801: 580, 1802: 581, 1806: 582, 1813: 583, 1814: 584, 1817: 585, 1818: 586, 1820: 587, 1821: 588, 1822: 589, 1825: 590, 1828: 591, 1833: 592, 1835: 593, 1836: 594, 1837: 595, 1838: 596, 1849: 597, 1850: 598, 1854: 599, 1855: 600, 1858: 601, 1859: 602, 1862: 603, 1869: 604, 1870: 605, 1871: 606, 1878: 607, 1879: 608, 1883: 609, 1887: 610, 1895: 611, 1897: 612, 1898: 613, 1900: 614, 1902: 615, 1903: 616, 1904: 617, 1911: 618, 1913: 619, 1915: 620, 1916: 621, 1917: 622, 1918: 623, 1920: 624, 1921: 625, 1922: 626, 1925: 627, 1927: 628, 1928: 629, 1929: 630, 1930: 631, 1931: 632, 1932: 633, 1933: 634, 1934: 635, 1935: 636, 1936: 637, 1937: 638, 1940: 639, 1943: 640, 1950: 641, 1954: 642, 1956: 643, 1958: 644, 1972: 645, 1986: 646, 2002: 647, 2003: 648, 2010: 649, 2011: 650, 2019: 651, 2022: 652, 2023: 653, 2024: 654, 2025: 655, 2027: 656, 2030: 657, 2032: 658, 2033: 659, 2043: 660, 2044: 661, 2070: 662, 2073: 663, 2079: 664, 2081: 665, 2085: 666, 2089: 667, 2091: 668, 2101: 669, 2119: 670, 2127: 671, 2148: 672, 2152: 673, 2153: 674, 2158: 675, 2161: 676, 2167: 677, 2179: 678, 2180: 679, 2181: 680, 2182: 681, 2183: 682, 2185: 683, 2187: 684, 2194: 685, 2195: 686, 2198: 687, 2199: 688, 2202: 689, 2206: 690, 2210: 691, 2212: 692, 2215: 693, 2216: 694, 2217: 695, 2223: 696, 2224: 697, 2236: 698, 2241: 699, 2252: 700, 2253: 701, 2270: 702, 2276: 703, 2285: 704, 2324: 705, 2333: 706, 2334: 707, 2335: 708, 2339: 709, 2340: 710, 2341: 711, 2353: 712, 2354: 713, 2387: 714, 2393: 715, 2420: 716, 2421: 717, 2442: 718, 2447: 719, 2448: 720, 2491: 721, 2524: 722, 2545: 723, 2601: 724, 2605: 725, 2606: 726, 2609: 727, 2611: 728, 2615: 729, 2619: 730, 2622: 731, 2628: 732, 2632: 733, 2637: 734, 2640: 735, 2641: 736, 2646: 737, 2648: 738, 2650: 739, 2652: 740, 2661: 741, 2684: 742, 2698: 743, 2780: 744, 2783: 745, 2788: 746, 2795: 747, 2798: 748, 2799: 749, 2801: 750, 2802: 751, 2803: 752, 2805: 753, 2806: 754, 2807: 755, 2808: 756, 2809: 757, 2810: 758, 2812: 759, 2813: 760, 2814: 761, 2815: 762, 2816: 763, 2817: 764, 2818: 765, 2821: 766, 2823: 767, 2824: 768, 2825: 769, 2826: 770, 2827: 771, 2828: 772, 2829: 773, 2830: 774, 2832: 775, 2834: 776, 2835: 777, 2836: 778, 2837: 779, 2838: 780, 2839: 781, 2840: 782, 2844: 783, 2846: 784, 2847: 785, 2848: 786, 2849: 787, 2850: 788, 2851: 789, 2852: 790, 2853: 791, 2854: 792, 2855: 793, 2856: 794, 2858: 795, 2859: 796, 2860: 797, 2861: 798, 2862: 799, 2865: 800, 2866: 801, 2869: 802, 2870: 803, 2874: 804, 2875: 805, 2877: 806, 2878: 807, 2880: 808, 2881: 809, 2882: 810, 2883: 811, 2884: 812, 2887: 813, 2888: 814, 2889: 815, 2890: 816, 2892: 817, 2893: 818, 2894: 819, 2895: 820, 2896: 821, 2897: 822, 2898: 823, 2900: 824, 2901: 825, 2902: 826, 2903: 827, 2904: 828, 2911: 829, 2920: 830, 2925: 831, 2928: 832, 2930: 833, 2931: 834, 2932: 835, 2935: 836, 2941: 837, 2946: 838, 2948: 839, 2956: 840, 2957: 841, 2958: 842, 2959: 843, 2960: 844, 2961: 845, 2964: 846, 2968: 847, 2969: 848, 2971: 849, 2972: 850, 2973: 851, 2974: 852, 2975: 853, 2976: 854, 2984: 855, 2985: 856, 2991: 857, 2994: 858, 2995: 859, 2997: 860, 3001: 861, 3003: 862, 3004: 863, 3008: 864, 3011: 865, 3015: 866, 3016: 867, 3017: 868, 3018: 869, 3019: 870, 3022: 871, 3034: 872, 3037: 873, 3042: 874, 3043: 875, 3044: 876, 3045: 877, 3049: 878, 3050: 879, 3052: 880, 3053: 881, 3054: 882, 3057: 883, 3073: 884, 3076: 885, 3077: 886, 3085: 887, 3095: 888, 3114: 889, 3115: 890, 3116: 891, 3117: 892, 3118: 893, 3119: 894, 3121: 895, 3123: 896, 3124: 897, 3127: 898, 3128: 899, 3129: 900, 3130: 901, 3131: 902, 3134: 903, 3135: 904, 3136: 905, 3137: 906, 3138: 907, 3140: 908, 3145: 909, 3147: 910, 3148: 911, 3151: 912, 3152: 913, 3153: 914, 3156: 915, 3157: 916, 3158: 917, 3159: 918, 3160: 919, 3163: 920, 3164: 921, 3165: 922, 3167: 923, 3169: 924, 3171: 925, 3173: 926, 3176: 927, 3183: 928, 3186: 929, 3192: 930, 3201: 931, 3202: 932, 3204: 933, 3205: 934, 3209: 935, 3211: 936, 3212: 937, 3213: 938, 3215: 939, 3219: 940, 3229: 941, 3232: 942, 3233: 943, 3234: 944, 3235: 945, 3236: 946, 3241: 947, 3242: 948, 3243: 949, 3244: 950, 3245: 951, 3246: 952, 3247: 953, 3249: 954, 3252: 955, 3254: 956, 3255: 957, 3256: 958, 3257: 959, 3258: 960, 3259: 961, 3262: 962, 3266: 963, 3268: 964, 3271: 965, 3277: 966, 3279: 967, 3303: 968, 3306: 969, 3310: 970, 3311: 971, 3323: 972, 3334: 973, 3337: 974, 3340: 975, 3341: 976, 3357: 977, 3359: 978, 3360: 979, 3361: 980, 3362: 981, 3365: 982, 3367: 983, 3368: 984, 3378: 985, 3379: 986, 3390: 987, 3391: 988, 3399: 989, 3400: 990, 3423: 991, 3428: 992, 3430: 993, 3434: 994, 3435: 995, 3442: 996, 3447: 997, 3449: 998, 3450: 999, 3455: 1000, 3456: 1001, 3458: 1002, 3461: 1003, 3462: 1004, 3463: 1005, 3464: 1006, 3470: 1007, 3492: 1008, 3503: 1009, 3510: 1010, 3518: 1011, 3520: 1012, 3523: 1013, 3525: 1014, 3546: 1015, 3548: 1016, 3550: 1017, 3557: 1018, 3558: 1019, 3562: 1020, 3569: 1021, 3571: 1022, 3572: 1023, 3573: 1024, 3574: 1025, 3576: 1026, 3584: 1027, 3585: 1028, 3586: 1029, 3587: 1030, 3589: 1031, 3616: 1032, 3618: 1033, 3619: 1034, 3621: 1035, 3626: 1036, 3627: 1037, 3631: 1038, 3633: 1039, 3634: 1040, 3649: 1041, 3708: 1042, 3724: 1043, 3725: 1044, 3726: 1045, 3727: 1046, 3728: 1047, 3733: 1048, 3734: 1049, 3743: 1050, 3745: 1051, 3751: 1052, 3763: 1053, 3765: 1054, 3770: 1055, 3771: 1056, 3779: 1057, 3832: 1058, 3834: 1059, 3835: 1060, 3838: 1061, 3839: 1062, 3845: 1063, 3846: 1064, 3850: 1065, 3855: 1066, 3857: 1067, 3859: 1068, 3865: 1069, 3894: 1070, 3895: 1071, 3896: 1072, 3897: 1073, 3900: 1074, 3901: 1075, 3902: 1076, 3903: 1077, 3904: 1078, 3906: 1079, 3910: 1080, 3911: 1081, 3913: 1082, 3914: 1083, 3915: 1084, 3916: 1085, 3917: 1086, 3918: 1087, 3919: 1088, 3921: 1089, 3922: 1090, 3924: 1091, 3925: 1092, 3926: 1093, 3927: 1094, 3928: 1095, 3929: 1096, 3930: 1097, 3933: 1098, 3934: 1099, 3935: 1100, 3940: 1101, 3941: 1102, 3942: 1103, 3943: 1104, 3944: 1105, 3945: 1106, 3946: 1107, 3947: 1108, 3949: 1109, 3950: 1110, 3951: 1111, 3952: 1112, 3953: 1113, 3954: 1114, 3955: 1115, 3956: 1116, 3957: 1117, 3958: 1118, 3959: 1119, 3961: 1120, 3963: 1121, 3965: 1122, 3966: 1123, 3967: 1124, 3968: 1125, 3969: 1126, 3970: 1127, 3971: 1128, 3972: 1129, 3974: 1130, 3975: 1131, 3984: 1132, 3985: 1133, 3994: 1134, 3997: 1135, 4000: 1136, 4010: 1137, 4011: 1138, 4016: 1139, 4017: 1140, 4018: 1141, 4019: 1142, 4026: 1143, 4028: 1144, 4030: 1145, 4046: 1146, 4047: 1147, 4049: 1148, 4050: 1149, 4052: 1150, 4053: 1151, 4054: 1152, 4057: 1153, 4058: 1154, 4062: 1155, 4069: 1156, 4079: 1157, 4082: 1158, 4083: 1159, 4086: 1160, 4087: 1161, 4088: 1162, 4089: 1163, 4090: 1164, 4091: 1165, 4092: 1166, 4093: 1167, 4094: 1168, 4096: 1169, 4102: 1170, 4103: 1171, 4106: 1172, 4107: 1173, 4110: 1174, 4113: 1175, 4114: 1176, 4116: 1177, 4117: 1178, 4121: 1179, 4124: 1180, 4125: 1181, 4126: 1182, 4127: 1183, 4129: 1184, 4131: 1185, 4134: 1186, 4136: 1187, 4137: 1188, 4138: 1189, 4139: 1190, 4140: 1191, 4141: 1192, 4142: 1193, 4143: 1194, 4144: 1195, 4145: 1196, 4146: 1197, 4148: 1198, 4149: 1199, 4158: 1200, 4159: 1201, 4160: 1202, 4163: 1203, 4172: 1204, 4188: 1205, 4198: 1206, 4203: 1207, 4204: 1208, 4205: 1209, 4233: 1210, 4250: 1211, 4252: 1212, 4253: 1213, 4254: 1214, 4256: 1215, 4261: 1216, 4262: 1217, 4263: 1218, 4280: 1219, 4287: 1220, 4289: 1221, 4293: 1222, 4295: 1223, 4296: 1224, 4301: 1225, 4302: 1226, 4305: 1227, 4307: 1228, 4308: 1229, 4309: 1230, 4316: 1231, 4326: 1232, 4358: 1233, 4361: 1234, 4362: 1235, 4364: 1236, 4370: 1237, 4382: 1238, 4385: 1239, 4386: 1240, 4390: 1241, 4391: 1242, 4394: 1243, 4401: 1244, 4414: 1245, 4417: 1246, 4432: 1247, 4434: 1248, 4437: 1249, 4439: 1250, 4450: 1251, 4451: 1252, 4453: 1253, 4455: 1254, 4462: 1255, 4465: 1256, 4467: 1257, 4472: 1258, 4473: 1259, 4475: 1260, 4476: 1261, 4495: 1262, 4502: 1263, 4503: 1264, 4504: 1265, 4505: 1266, 4509: 1267, 4511: 1268, 4515: 1269, 4516: 1270, 4519: 1271, 4522: 1272, 4529: 1273, 4531: 1274, 4533: 1275, 4537: 1276, 4538: 1277, 4540: 1278, 4541: 1279, 4543: 1280, 4544: 1281, 4545: 1282, 4549: 1283, 4550: 1284, 4551: 1285, 4562: 1286, 4564: 1287, 4565: 1288, 4576: 1289, 4582: 1290, 4586: 1291, 4595: 1292, 4610: 1293, 4611: 1294, 4616: 1295, 4631: 1296, 4632: 1297, 4633: 1298, 4634: 1299, 4639: 1300, 4640: 1301, 4648: 1302, 4659: 1303, 4660: 1304, 4661: 1305, 4662: 1306, 4675: 1307, 4681: 1308, 4682: 1309, 4683: 1310, 4686: 1311, 4704: 1312, 4705: 1313, 4706: 1314, 4718: 1315, 4734: 1316, 4763: 1317, 4764: 1318, 4765: 1319, 4766: 1320, 4768: 1321, 4770: 1322, 4774: 1323, 4775: 1324, 4776: 1325, 4806: 1326, 4807: 1327, 4808: 1328, 4811: 1329, 4812: 1330, 4814: 1331, 4816: 1332, 4817: 1333, 4818: 1334, 4823: 1335, 4831: 1336, 4833: 1337, 4835: 1338, 4840: 1339, 4844: 1340, 4845: 1341, 4846: 1342, 4848: 1343, 4852: 1344, 4855: 1345, 4856: 1346, 4857: 1347, 4862: 1348, 4867: 1349, 4886: 1350, 4887: 1351, 4888: 1352, 4889: 1353, 4900: 1354, 4903: 1355, 4905: 1356, 4912: 1357, 4918: 1358, 4923: 1359, 4926: 1360, 4930: 1361, 4935: 1362, 4937: 1363, 4938: 1364, 4948: 1365, 4988: 1366, 5034: 1367, 5035: 1368, 5036: 1369, 5037: 1370, 5038: 1371, 5039: 1372, 5040: 1373, 5043: 1374, 5044: 1375, 5045: 1376, 5046: 1377, 5049: 1378, 5055: 1379, 5057: 1380, 5059: 1381, 5061: 1382, 5068: 1383, 5069: 1384, 5074: 1385, 5075: 1386, 5076: 1387, 5078: 1388, 5080: 1389, 5082: 1390, 5084: 1391, 5085: 1392, 5096: 1393, 5099: 1394, 5100: 1395, 5102: 1396, 5103: 1397, 5114: 1398, 5118: 1399, 5121: 1400, 5122: 1401, 5124: 1402, 5128: 1403, 5129: 1404, 5131: 1405, 5134: 1406, 5136: 1407, 5140: 1408, 5141: 1409, 5143: 1410, 5145: 1411, 5146: 1412, 5148: 1413, 5149: 1414, 5178: 1415, 5186: 1416, 5188: 1417, 5190: 1418, 5192: 1419, 5193: 1420, 5194: 1421, 5195: 1422, 5197: 1423, 5198: 1424, 5199: 1425, 5201: 1426, 5202: 1427, 5207: 1428, 5214: 1429, 5215: 1430, 5224: 1431, 5226: 1432, 5246: 1433, 5247: 1434, 5248: 1435, 5249: 1436, 5253: 1437, 5299: 1438, 5300: 1439, 5301: 1440, 5324: 1441, 5331: 1442, 5335: 1443, 5336: 1444, 5380: 1445, 5382: 1446, 5389: 1447, 5398: 1448, 5405: 1449, 5428: 1450, 5434: 1451, 5436: 1452, 5437: 1453, 5442: 1454, 5443: 1455, 5444: 1456, 5453: 1457, 5454: 1458, 5460: 1459, 5461: 1460, 5463: 1461, 5464: 1462, 5466: 1463, 5469: 1464, 5482: 1465, 5483: 1466, 5484: 1467, 5485: 1468, 5506: 1469, 5510: 1470, 5511: 1471, 5512: 1472, 5513: 1473, 5514: 1474, 5529: 1475, 5530: 1476, 5531: 1477, 5532: 1478, 5533: 1479, 5534: 1480, 5535: 1481, 5536: 1482, 5537: 1483, 5538: 1484, 5539: 1485, 5541: 1486, 5542: 1487, 5544: 1488, 5546: 1489, 5547: 1490, 5548: 1491, 5549: 1492, 5550: 1493, 5552: 1494, 5553: 1495, 5554: 1496, 5555: 1497, 5557: 1498, 5559: 1499, 5560: 1500, 5561: 1501, 5564: 1502, 5569: 1503, 5576: 1504, 5577: 1505, 5582: 1506, 5585: 1507, 5594: 1508, 5597: 1509, 5598: 1510, 5604: 1511, 5617: 1512, 5624: 1513, 5625: 1514, 5626: 1515, 5631: 1516, 5655: 1517, 5662: 1518, 5664: 1519, 5671: 1520, 5680: 1521, 5681: 1522, 5691: 1523, 5696: 1524, 5701: 1525, 5709: 1526, 5710: 1527, 5711: 1528, 5712: 1529, 5713: 1530, 5714: 1531, 5718: 1532, 5720: 1533, 5721: 1534, 5724: 1535, 5732: 1536, 5737: 1537, 5738: 1538, 5739: 1539, 5740: 1540, 5741: 1541, 5742: 1542, 5744: 1543, 5745: 1544, 5747: 1545, 5752: 1546, 5758: 1547, 5763: 1548, 5764: 1549, 5767: 1550, 5768: 1551, 5774: 1552, 5775: 1553, 5776: 1554, 5777: 1555, 5780: 1556, 5790: 1557, 5795: 1558, 5796: 1559, 5816: 1560, 5817: 1561, 5818: 1562, 5819: 1563, 5821: 1564, 5822: 1565, 5826: 1566, 5830: 1567, 5831: 1568, 5835: 1569, 5836: 1570, 5837: 1571, 5844: 1572, 5847: 1573, 5851: 1574, 5867: 1575, 5870: 1576, 5876: 1577, 5892: 1578, 5895: 1579, 5896: 1580, 5897: 1581, 5898: 1582, 5899: 1583, 5900: 1584, 5905: 1585, 5910: 1586, 5911: 1587, 5916: 1588, 5920: 1589, 5925: 1590, 5926: 1591, 5929: 1592, 5936: 1593, 5938: 1594, 5972: 1595, 5985: 1596, 5986: 1597, 5987: 1598, 5998: 1599, 5999: 1600, 6000: 1601, 6017: 1602, 6041: 1603, 6044: 1604, 6053: 1605, 6056: 1606, 6066: 1607, 6068: 1608, 6073: 1609, 6074: 1610, 6092: 1611, 6111: 1612, 6120: 1613, 6124: 1614, 6166: 1615, 6169: 1616, 6170: 1617, 6171: 1618, 6172: 1619, 6173: 1620, 6174: 1621, 6175: 1622, 6176: 1623, 6177: 1624, 6179: 1625, 6180: 1626, 6181: 1627, 6183: 1628, 6185: 1629, 6186: 1630, 6187: 1631, 6188: 1632, 6192: 1633, 6193: 1634, 6195: 1635, 6196: 1636, 6197: 1637, 6198: 1638, 6199: 1639, 6201: 1640, 6204: 1641, 6205: 1642, 6206: 1643, 6207: 1644, 6209: 1645, 6210: 1646, 6216: 1647, 6217: 1648, 6218: 1649, 6221: 1650, 6222: 1651, 6223: 1652, 6225: 1653, 6226: 1654, 6228: 1655, 6229: 1656, 6240: 1657, 6241: 1658, 6242: 1659, 6243: 1660, 6247: 1661, 6250: 1662, 6255: 1663, 6267: 1664, 6269: 1665, 6270: 1666, 6271: 1667, 6279: 1668, 6288: 1669, 6306: 1670, 6308: 1671, 6312: 1672, 6314: 1673, 6331: 1674, 6332: 1675, 6333: 1676, 6334: 1677, 6335: 1678, 6336: 1679, 6337: 1680, 6343: 1681, 6345: 1682, 6347: 1683, 6348: 1684, 6349: 1685, 6358: 1686, 6359: 1687, 6363: 1688, 6366: 1689, 6368: 1690, 6370: 1691, 6374: 1692, 6381: 1693, 6412: 1694, 6415: 1695, 6418: 1696, 6419: 1697, 6420: 1698, 6421: 1699, 6424: 1700, 6425: 1701, 6427: 1702, 6428: 1703, 6429: 1704, 6430: 1705, 6431: 1706, 6442: 1707, 6446: 1708, 6452: 1709, 6455: 1710, 6464: 1711, 6467: 1712, 6472: 1713, 6473: 1714, 6476: 1715, 6477: 1716, 6491: 1717, 6492: 1718, 6493: 1719, 6494: 1720, 6495: 1721, 6497: 1722, 6498: 1723, 6499: 1724, 6501: 1725, 6502: 1726, 6503: 1727, 6505: 1728, 6506: 1729, 6508: 1730, 6509: 1731, 6512: 1732, 6522: 1733, 6545: 1734, 6548: 1735, 6549: 1736, 6553: 1737, 6554: 1738, 6557: 1739, 6559: 1740, 6564: 1741, 6565: 1742, 6567: 1743, 6569: 1744, 6570: 1745, 6571: 1746, 6572: 1747, 6573: 1748, 6574: 1749, 6575: 1750, 6579: 1751, 6583: 1752, 6585: 1753, 6589: 1754, 6590: 1755, 6591: 1756, 6593: 1757, 6594: 1758, 6595: 1759, 6596: 1760, 6598: 1761, 6600: 1762, 6601: 1763, 6602: 1764, 6605: 1765, 6606: 1766, 6608: 1767, 6611: 1768, 6613: 1769, 6614: 1770, 6616: 1771, 6631: 1772, 6634: 1773, 6642: 1774, 6648: 1775, 6649: 1776, 6650: 1777, 6651: 1778, 6652: 1779, 6653: 1780, 6654: 1781, 6656: 1782, 6658: 1783, 6659: 1784, 6660: 1785, 6662: 1786, 6664: 1787, 6668: 1788, 6669: 1789, 6670: 1790, 6678: 1791, 6679: 1792, 6681: 1793, 6688: 1794, 6695: 1795, 6696: 1796, 6697: 1797, 6698: 1798, 6702: 1799, 6703: 1800, 6705: 1801, 6709: 1802, 6710: 1803, 6712: 1804, 6716: 1805, 6722: 1806, 6739: 1807, 6741: 1808, 6747: 1809, 6749: 1810, 6760: 1811, 6772: 1812, 6783: 1813, 6786: 1814, 6801: 1815, 6802: 1816, 6809: 1817, 6825: 1818, 6829: 1819, 6830: 1820, 6831: 1821, 6832: 1822, 6833: 1823, 6835: 1824, 6845: 1825, 6849: 1826, 6855: 1827, 6857: 1828, 6859: 1829, 6860: 1830, 6863: 1831, 6901: 1832, 6916: 1833, 6923: 1834, 6945: 1835, 6959: 1836, 6961: 1837, 6963: 1838, 6965: 1839, 6967: 1840, 6968: 1841, 6970: 1842, 6977: 1843, 6989: 1844, 6995: 1845, 7002: 1846, 7015: 1847, 7016: 1848, 7053: 1849, 7059: 1850, 7062: 1851, 7066: 1852, 7097: 1853, 7100: 1854, 7114: 1855, 7142: 1856, 7143: 1857, 7144: 1858, 7146: 1859, 7147: 1860, 7148: 1861, 7149: 1862, 7150: 1863, 7151: 1864, 7154: 1865, 7155: 1866, 7156: 1867, 7157: 1868, 7158: 1869, 7169: 1870, 7172: 1871, 7174: 1872, 7195: 1873, 7210: 1874, 7217: 1875, 7218: 1876, 7231: 1877, 7232: 1878, 7262: 1879, 7267: 1880, 7293: 1881, 7316: 1882, 7318: 1883, 7328: 1884, 7329: 1885, 7332: 1886, 7400: 1887, 7410: 1888, 7436: 1889, 7439: 1890, 7449: 1891, 7450: 1892, 7451: 1893, 7453: 1894, 7456: 1895, 7457: 1896, 7458: 1897, 7460: 1898, 7472: 1899, 7481: 1900, 7482: 1901, 7484: 1902, 7485: 1903, 7487: 1904, 7488: 1905, 7489: 1906, 7492: 1907, 7494: 1908, 7499: 1909, 7501: 1910, 7503: 1911, 7508: 1912, 7510: 1913, 7512: 1914, 7513: 1915, 7516: 1916, 7517: 1917, 7519: 1918, 7521: 1919, 7526: 1920, 7527: 1921, 7529: 1922, 7531: 1923, 7534: 1924, 7536: 1925, 7537: 1926, 7544: 1927, 7548: 1928, 7556: 1929, 7558: 1930, 7563: 1931, 7568: 1932, 7569: 1933, 7570: 1934, 7571: 1935, 7572: 1936, 7573: 1937, 7574: 1938, 7577: 1939, 7579: 1940, 7580: 1941, 7585: 1942, 7586: 1943, 7593: 1944, 7594: 1945, 7595: 1946, 7598: 1947, 7599: 1948, 7600: 1949, 7602: 1950, 7603: 1951, 7607: 1952, 7615: 1953, 7616: 1954, 7620: 1955, 7624: 1956, 7627: 1957, 7644: 1958, 7650: 1959, 7655: 1960, 7683: 1961, 7684: 1962, 7685: 1963, 7686: 1964, 7687: 1965, 7688: 1966, 7699: 1967, 7707: 1968, 7708: 1969, 7710: 1970, 7714: 1971, 7724: 1972, 7752: 1973, 7820: 1974, 7825: 1975, 7828: 1976, 7830: 1977, 7847: 1978, 7848: 1979, 7934: 1980, 7963: 1981, 7979: 1982, 7997: 1983, 8017: 1984, 8020: 1985, 8026: 1986, 8033: 1987, 8034: 1988, 8040: 1989, 8044: 1990, 8057: 1991, 8058: 1992, 8059: 1993, 8060: 1994, 8079: 1995, 8089: 1996, 8090: 1997, 8096: 1998, 8097: 1999, 8098: 2000, 8103: 2001, 8106: 2002, 8108: 2003, 8112: 2004, 8114: 2005, 8116: 2006, 8117: 2007, 8122: 2008, 8125: 2009, 8128: 2010, 8129: 2011, 8133: 2012, 8134: 2013, 8136: 2014, 8138: 2015, 8139: 2016, 8143: 2017, 8144: 2018, 8145: 2019, 8146: 2020, 8147: 2021, 8148: 2022, 8149: 2023, 8151: 2024, 8155: 2025, 8160: 2026, 8165: 2027, 8166: 2028, 8179: 2029, 8184: 2030, 8199: 2031, 8206: 2032, 8214: 2033, 8215: 2034, 8217: 2035, 8218: 2036, 8219: 2037, 8220: 2038, 8224: 2039, 8225: 2040, 8227: 2041, 8228: 2042, 8230: 2043, 8252: 2044, 8253: 2045, 8270: 2046, 8278: 2047, 8282: 2048, 8286: 2049, 8288: 2050, 8305: 2051, 8314: 2052, 8315: 2053, 8319: 2054, 8406: 2055, 8407: 2056, 8414: 2057, 8415: 2058, 8422: 2059, 8452: 2060, 8454: 2061, 8455: 2062, 8501: 2063, 8502: 2064, 8503: 2065, 8505: 2066, 8507: 2067, 8510: 2068, 8511: 2069, 8512: 2070, 8513: 2071, 8515: 2072, 8517: 2073, 8519: 2074, 8520: 2075, 8523: 2076, 8524: 2077, 8525: 2078, 8527: 2079, 8528: 2080, 8529: 2081, 8531: 2082, 8542: 2083, 8544: 2084, 8545: 2085, 8546: 2086, 8547: 2087, 8548: 2088, 8549: 2089, 8554: 2090, 8555: 2091, 8559: 2092, 8561: 2093, 8580: 2094, 8581: 2095, 8583: 2096, 8589: 2097, 8597: 2098, 8600: 2099, 8601: 2100, 8610: 2101, 8625: 2102, 8631: 2103, 8634: 2104, 8637: 2105, 8638: 2106, 8639: 2107, 8640: 2108, 8641: 2109, 8643: 2110, 8648: 2111, 8658: 2112, 8659: 2113, 8668: 2114, 8678: 2115, 8691: 2116, 8699: 2117, 8700: 2118, 8710: 2119, 8718: 2120, 8722: 2121, 8723: 2122, 8726: 2123, 8727: 2124, 8729: 2125, 8734: 2126, 8773: 2127, 8777: 2128, 8798: 2129, 8800: 2130, 8807: 2131, 8823: 2132, 8826: 2133, 8827: 2134, 8828: 2135, 8871: 2136, 8894: 2137, 8895: 2138, 8965: 2139, 8973: 2140, 8981: 2141, 8982: 2142, 8987: 2143, 8988: 2144, 8989: 2145, 8990: 2146, 9011: 2147, 9013: 2148, 9018: 2149, 9026: 2150, 9027: 2151, 9048: 2152, 9094: 2153, 9097: 2154, 9103: 2155, 9117: 2156, 9118: 2157, 9119: 2158, 9121: 2159, 9129: 2160, 9130: 2161, 9135: 2162, 9136: 2163, 9141: 2164, 9147: 2165, 9152: 2166, 9168: 2167, 9183: 2168, 9184: 2169, 9185: 2170, 9186: 2171, 9187: 2172, 9193: 2173, 9211: 2174, 9212: 2175, 9213: 2176, 9215: 2177, 9217: 2178, 9218: 2179, 9219: 2180, 9222: 2181, 9230: 2182, 9280: 2183, 9318: 2184, 9361: 2185, 9362: 2186, 9363: 2187, 9365: 2188, 9366: 2189, 9370: 2190, 9374: 2191, 9376: 2192, 9378: 2193, 9379: 2194, 9380: 2195, 9384: 2196, 9389: 2197, 9390: 2198, 9391: 2199, 9392: 2200, 9394: 2201, 9403: 2202, 9406: 2203, 9439: 2204, 9450: 2205, 9452: 2206, 9453: 2207, 9454: 2208, 9472: 2209, 9480: 2210, 9481: 2211, 9484: 2212, 9485: 2213, 9497: 2214, 9582: 2215, 9584: 2216, 9592: 2217, 9598: 2218, 9599: 2219, 9611: 2220, 9637: 2221, 9644: 2222, 9645: 2223, 9651: 2224, 9662: 2225, 9670: 2226, 9742: 2227, 9772: 2228, 9788: 2229, 9818: 2230, 9865: 2231, 9885: 2232, 9962: 2233, 10047: 2234, 10050: 2235, 10052: 2236, 10055: 2237, 10056: 2238, 10058: 2239, 10061: 2240, 10062: 2241, 10075: 2242, 10076: 2243, 10102: 2244, 10103: 2245, 10105: 2246, 10119: 2247, 10127: 2248, 10133: 2249, 10168: 2250, 10208: 2251, 10210: 2252, 10211: 2253, 10293: 2254, 10297: 2255, 10298: 2256, 10299: 2257, 10300: 2258, 10302: 2259, 10307: 2260, 10308: 2261, 10311: 2262, 10312: 2263, 10336: 2264, 10339: 2265, 10340: 2266, 10343: 2267, 10345: 2268, 10346: 2269, 10347: 2270, 10351: 2271, 10353: 2272, 10354: 2273, 10356: 2274, 10364: 2275, 45332: 2276}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# use file_limit=10000 to only load and verify the first 10000 graphs (creates file of size ~420 MB)\n",
    "\n",
    "file_limit = 10000    # desired number of graphs to work with\n",
    "vocab_size = 1598   # number of characters in our vocabulary\n",
    "\n",
    "pf_data = ProofDataset(root=\"data/\",read_name=\"10000_relabeled_data_at_least_10.json\" , write_name=\"10000_relabeled_data_at_least_10.pt\" ,file_limit=file_limit)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make train/val/test for GIN\n",
    "\n",
    "# set seed for random # generation\n",
    "random.seed(10)\n",
    "length = file_limit\n",
    "total_indices = [i+file_limit for i in range(file_limit)]\n",
    "\n",
    "# create index vectors to filter dataset\n",
    "train_indices = [i for i in range(file_limit)]\n",
    "train_indices = train_indices + random.sample(total_indices, int(length*.8))\n",
    "train_indices.sort()\n",
    "\n",
    "val_index_options = [x for x in total_indices if x not in train_indices]\n",
    "val_indices = random.sample(val_index_options, int(length*.1))\n",
    "val_indices.sort()\n",
    "\n",
    "test_index_options = [x for x in total_indices if x not in train_indices if x not in val_indices]\n",
    "test_indices = random.sample(test_index_options, int(length*.1))\n",
    "test_indices.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# our model is judged on how well it can predict the label for the final/conclusion node as seen in the val set created above\n",
    "# to not give our model too much info, we zero out the features on the final/conclusion node for each graph in the val set\n",
    "\n",
    "data_list = []\n",
    "\n",
    "for idx, graph in enumerate(pf_data):\n",
    "    # inherit features, labels, and edges\n",
    "    x = graph.x.clone()\n",
    "    y = graph.y.clone()\n",
    "    edge_index = graph.edge_index.clone()\n",
    "    x[-1] = torch.zeros(512) # zero out ALL conclusion nodes\n",
    "\n",
    "    #if idx in val_indices:\n",
    "        #x[-1] = torch.zeros(512)    # zero out features of final/conclusion node for each graph in val set\n",
    "\n",
    "# replace features of conlusion/final nodes from val set with average of neighboring node embeddings\n",
    "    #if idx in val_indices:\n",
    "\n",
    "        #connected_nodes = []\n",
    "        #sum = torch.zeros(512)\n",
    "\n",
    "        #for i, edge in enumerate(edge_index[0]): # case of outgoing edge coming from conc node\n",
    "            #if edge == (pf_data[idx].num_nodes-1):  \n",
    "                #connected_nodes.append(int(edge_index[1][i].item()))\n",
    "        #for i, edge in enumerate(edge_index[1]): # case of outgoing edge going to conc node\n",
    "            #if edge == (pf_data[idx].num_nodes-1):\n",
    "                #connected_nodes.append(int(edge_index[0][i].item()))\n",
    "\n",
    "        #for i in connected_nodes:\n",
    "            #if not torch.equal(pf_data[idx].x[i], pf_data[idx].x[-1]):\n",
    "            #sum += pf_data[idx].x[i]\n",
    "        #if len(connected_nodes) > 0:\n",
    "            #x[-1] = sum/(len(connected_nodes))\n",
    "        #else:\n",
    "            #x[-1] = sum\n",
    "\n",
    "    data_list.append(Data(x=x,y=y,edge_index=edge_index))\n",
    "\n",
    "# read_name not used, but needed to instantiate class\n",
    "hidden_conc_pf_data = HiddenConcProofDataset(root=\"data/\",read_name=\"10000_relabeled_data_at_least_10_w_stmts.json\",write_name=\"overwritten_labels.pt\", data_list=data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set   = 18000 graphs\n",
      "Validation set = 1000 graphs\n",
      "Test set       = 1000 graphs\n"
     ]
    }
   ],
   "source": [
    "# Create training, validation, and test sets\n",
    "train_dataset = hidden_conc_pf_data[train_indices]\n",
    "val_dataset = hidden_conc_pf_data[val_indices]\n",
    "test_dataset = hidden_conc_pf_data[test_indices]\n",
    "\n",
    "print(f'Training set   = {len(train_dataset)} graphs')\n",
    "print(f'Validation set = {len(val_dataset)} graphs')\n",
    "print(f'Test set       = {len(test_dataset)} graphs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mini-batches\n",
    "# batch_size is number of graphs\n",
    "train_loader = DataLoader(train_dataset, batch_size=750, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=500, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=500, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loader:\n",
      " - Batch 0: DataBatch(x=[7562, 512], edge_index=[2, 6812], y=[7562], batch=[7562], ptr=[751])\n",
      " - Batch 1: DataBatch(x=[7012, 512], edge_index=[2, 6262], y=[7012], batch=[7012], ptr=[751])\n",
      " - Batch 2: DataBatch(x=[6329, 512], edge_index=[2, 5579], y=[6329], batch=[6329], ptr=[751])\n",
      " - Batch 3: DataBatch(x=[6330, 512], edge_index=[2, 5580], y=[6330], batch=[6330], ptr=[751])\n",
      " - Batch 4: DataBatch(x=[9286, 512], edge_index=[2, 8536], y=[9286], batch=[9286], ptr=[751])\n",
      " - Batch 5: DataBatch(x=[7190, 512], edge_index=[2, 6440], y=[7190], batch=[7190], ptr=[751])\n",
      " - Batch 6: DataBatch(x=[7955, 512], edge_index=[2, 7205], y=[7955], batch=[7955], ptr=[751])\n",
      " - Batch 7: DataBatch(x=[11743, 512], edge_index=[2, 10993], y=[11743], batch=[11743], ptr=[751])\n",
      " - Batch 8: DataBatch(x=[8107, 512], edge_index=[2, 7357], y=[8107], batch=[8107], ptr=[751])\n",
      " - Batch 9: DataBatch(x=[8510, 512], edge_index=[2, 7760], y=[8510], batch=[8510], ptr=[751])\n",
      " - Batch 10: DataBatch(x=[13047, 512], edge_index=[2, 12297], y=[13047], batch=[13047], ptr=[751])\n",
      " - Batch 11: DataBatch(x=[6218, 512], edge_index=[2, 5468], y=[6218], batch=[6218], ptr=[751])\n",
      " - Batch 12: DataBatch(x=[6026, 512], edge_index=[2, 5276], y=[6026], batch=[6026], ptr=[751])\n",
      " - Batch 13: DataBatch(x=[6202, 512], edge_index=[2, 5452], y=[6202], batch=[6202], ptr=[751])\n",
      " - Batch 14: DataBatch(x=[6634, 512], edge_index=[2, 5884], y=[6634], batch=[6634], ptr=[751])\n",
      " - Batch 15: DataBatch(x=[7894, 512], edge_index=[2, 7144], y=[7894], batch=[7894], ptr=[751])\n",
      " - Batch 16: DataBatch(x=[6291, 512], edge_index=[2, 5541], y=[6291], batch=[6291], ptr=[751])\n",
      " - Batch 17: DataBatch(x=[8416, 512], edge_index=[2, 7666], y=[8416], batch=[8416], ptr=[751])\n",
      " - Batch 18: DataBatch(x=[6023, 512], edge_index=[2, 5273], y=[6023], batch=[6023], ptr=[751])\n",
      " - Batch 19: DataBatch(x=[5311, 512], edge_index=[2, 4561], y=[5311], batch=[5311], ptr=[751])\n",
      " - Batch 20: DataBatch(x=[7482, 512], edge_index=[2, 6732], y=[7482], batch=[7482], ptr=[751])\n",
      " - Batch 21: DataBatch(x=[5145, 512], edge_index=[2, 4395], y=[5145], batch=[5145], ptr=[751])\n",
      " - Batch 22: DataBatch(x=[9313, 512], edge_index=[2, 8563], y=[9313], batch=[9313], ptr=[751])\n",
      " - Batch 23: DataBatch(x=[6667, 512], edge_index=[2, 5917], y=[6667], batch=[6667], ptr=[751])\n",
      "\n",
      "Validation loader:\n",
      " - Batch 0: DataBatch(x=[2838, 512], edge_index=[2, 2338], y=[2838], batch=[2838], ptr=[501])\n",
      " - Batch 1: DataBatch(x=[13532, 512], edge_index=[2, 13032], y=[13532], batch=[13532], ptr=[501])\n",
      "\n",
      "Test loader:\n",
      " - Batch 0: DataBatch(x=[2861, 512], edge_index=[2, 2361], y=[2861], batch=[2861], ptr=[501])\n",
      " - Batch 1: DataBatch(x=[11770, 512], edge_index=[2, 11270], y=[11770], batch=[11770], ptr=[501])\n"
     ]
    }
   ],
   "source": [
    "print('\\nTrain loader:')\n",
    "for i, batch in enumerate(train_loader):\n",
    "    print(f' - Batch {i}: {batch}')\n",
    "\n",
    "print('\\nValidation loader:')\n",
    "for i, batch in enumerate(val_loader):\n",
    "    print(f' - Batch {i}: {batch}')\n",
    "\n",
    "print('\\nTest loader:')\n",
    "for i, batch in enumerate(test_loader):\n",
    "    print(f' - Batch {i}: {batch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary to record label frequency\n",
    "\n",
    "# get max label used in hidden_conc_pf_data\n",
    "max_label = 0\n",
    "\n",
    "for i in range(file_limit*2):\n",
    "    for j in hidden_conc_pf_data.get(i).y:\n",
    "        if j > max_label:\n",
    "            max_label = j.to(int).item()\n",
    "\n",
    "# initialize histogram for labels used in hidden_conc_pf_data            \n",
    "label_count = {}\n",
    "\n",
    "for i in range(file_limit*2):\n",
    "    for j in range(max_label+1):        \n",
    "        label_count[j] = 0\n",
    "\n",
    "for i in range(file_limit*2):\n",
    "    for j in hidden_conc_pf_data.get(i).y:\n",
    "        label_count[j.to(int).item()] += 1\n",
    "\n",
    "step_count = 0\n",
    "max = 0\n",
    "max_freq_index = None   #find the most frequently used index\n",
    "labels_never_used = 0\n",
    "labels_used_once = 0\n",
    "labels_used_twice = 0\n",
    "\n",
    "for k,v in label_count.items():\n",
    "    step_count += v\n",
    "    if v > max:     \n",
    "        max = v\n",
    "        max_freq_index = k\n",
    "\n",
    "    if v == 0:\n",
    "        labels_never_used += 1\n",
    "    if v == 1:\n",
    "        labels_used_once += 1\n",
    "    if v ==2:\n",
    "        labels_used_twice += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of steps is: 211694\n",
      "highest frequency label is 0 and occurs 44150 times\n",
      "final label used is 2276\n",
      "{0: 44150, 1: 1863, 2: 66, 3: 77, 4: 111, 5: 1285, 6: 18, 7: 127, 8: 28, 9: 357, 10: 55, 11: 4640, 12: 697, 13: 72, 14: 145, 15: 81, 16: 762, 17: 47, 18: 184, 19: 15, 20: 69, 21: 27, 22: 93, 23: 323, 24: 26, 25: 243, 26: 318, 27: 30, 28: 40, 29: 100, 30: 23, 31: 37, 32: 11, 33: 51, 34: 29, 35: 13, 36: 14, 37: 144, 38: 26, 39: 42, 40: 27, 41: 14, 42: 15, 43: 22, 44: 15, 45: 26, 46: 12, 47: 37, 48: 227, 49: 92, 50: 11, 51: 11, 52: 11, 53: 24, 54: 23, 55: 28, 56: 50, 57: 152, 58: 52, 59: 47, 60: 45, 61: 39, 62: 15, 63: 25, 64: 26, 65: 11, 66: 30, 67: 39, 68: 45, 69: 60, 70: 51, 71: 14, 72: 12, 73: 50, 74: 15, 75: 21, 76: 46, 77: 66, 78: 97, 79: 13, 80: 34, 81: 26, 82: 22, 83: 62, 84: 70, 85: 14, 86: 32, 87: 33, 88: 21, 89: 25, 90: 236, 91: 12, 92: 27, 93: 13, 94: 13, 95: 52, 96: 77, 97: 46, 98: 300, 99: 192, 100: 29, 101: 221, 102: 672, 103: 888, 104: 15, 105: 28, 106: 26, 107: 110, 108: 159, 109: 86, 110: 73, 111: 115, 112: 176, 113: 305, 114: 265, 115: 750, 116: 134, 117: 1138, 118: 181, 119: 45, 120: 144, 121: 316, 122: 96, 123: 85, 124: 100, 125: 155, 126: 164, 127: 114, 128: 52, 129: 73, 130: 70, 131: 123, 132: 143, 133: 34, 134: 473, 135: 342, 136: 81, 137: 80, 138: 65, 139: 49, 140: 22, 141: 28, 142: 79, 143: 16, 144: 34, 145: 17, 146: 23, 147: 1097, 148: 92, 149: 170, 150: 350, 151: 288, 152: 13, 153: 77, 154: 80, 155: 263, 156: 13, 157: 70, 158: 323, 159: 19, 160: 120, 161: 65, 162: 17, 163: 83, 164: 24, 165: 17, 166: 105, 167: 247, 168: 28, 169: 56, 170: 14, 171: 75, 172: 12, 173: 418, 174: 76, 175: 37, 176: 16, 177: 41, 178: 12, 179: 55, 180: 240, 181: 30, 182: 13, 183: 13, 184: 308, 185: 16, 186: 85, 187: 12, 188: 34, 189: 29, 190: 26, 191: 60, 192: 74, 193: 21, 194: 29, 195: 14, 196: 29, 197: 20, 198: 36, 199: 27, 200: 78, 201: 26, 202: 16, 203: 17, 204: 321, 205: 100, 206: 44, 207: 25, 208: 617, 209: 105, 210: 94, 211: 37, 212: 36, 213: 14, 214: 38, 215: 12, 216: 23, 217: 15, 218: 13, 219: 11, 220: 14, 221: 14, 222: 41, 223: 24, 224: 16, 225: 18, 226: 49, 227: 23, 228: 14, 229: 12, 230: 679, 231: 204, 232: 11, 233: 88, 234: 13, 235: 1123, 236: 273, 237: 101, 238: 28, 239: 45, 240: 30, 241: 48, 242: 54, 243: 17, 244: 15, 245: 17, 246: 15, 247: 16, 248: 70, 249: 37, 250: 34, 251: 59, 252: 52, 253: 14, 254: 133, 255: 13, 256: 316, 257: 115, 258: 18, 259: 21, 260: 13, 261: 13, 262: 69, 263: 96, 264: 26, 265: 106, 266: 15, 267: 25, 268: 139, 269: 93, 270: 25, 271: 35, 272: 2336, 273: 1328, 274: 466, 275: 23, 276: 780, 277: 73, 278: 14, 279: 11, 280: 45, 281: 39, 282: 713, 283: 341, 284: 226, 285: 88, 286: 34, 287: 23, 288: 17, 289: 16, 290: 18, 291: 48, 292: 40, 293: 22, 294: 23, 295: 63, 296: 35, 297: 349, 298: 48, 299: 17, 300: 11, 301: 61, 302: 27, 303: 14, 304: 18, 305: 22, 306: 16, 307: 38, 308: 40, 309: 44, 310: 19, 311: 33, 312: 34, 313: 11, 314: 12, 315: 13, 316: 24, 317: 21, 318: 11, 319: 12, 320: 28, 321: 34, 322: 25, 323: 11, 324: 24, 325: 88, 326: 20, 327: 11, 328: 90, 329: 24, 330: 16, 331: 44, 332: 410, 333: 57, 334: 368, 335: 1995, 336: 477, 337: 580, 338: 43, 339: 49, 340: 114, 341: 313, 342: 113, 343: 34, 344: 332, 345: 52, 346: 45, 347: 64, 348: 32, 349: 34, 350: 126, 351: 11, 352: 56, 353: 78, 354: 14, 355: 13, 356: 230, 357: 183, 358: 12, 359: 191, 360: 251, 361: 227, 362: 678, 363: 29, 364: 33, 365: 16, 366: 18, 367: 51, 368: 28, 369: 32, 370: 14, 371: 12, 372: 17, 373: 100, 374: 21, 375: 87, 376: 39, 377: 403, 378: 255, 379: 356, 380: 22, 381: 29, 382: 22, 383: 14, 384: 66, 385: 23, 386: 90, 387: 11, 388: 70, 389: 103, 390: 172, 391: 82, 392: 164, 393: 12, 394: 18, 395: 671, 396: 205, 397: 243, 398: 196, 399: 67, 400: 18, 401: 14, 402: 27, 403: 12, 404: 199, 405: 262, 406: 16, 407: 398, 408: 43, 409: 326, 410: 32, 411: 33, 412: 81, 413: 86, 414: 62, 415: 36, 416: 39, 417: 19, 418: 12, 419: 41, 420: 11, 421: 17, 422: 15, 423: 41, 424: 14, 425: 13, 426: 124, 427: 84, 428: 92, 429: 49, 430: 12, 431: 22, 432: 138, 433: 74, 434: 27, 435: 14, 436: 23, 437: 42, 438: 77, 439: 58, 440: 37, 441: 57, 442: 20, 443: 40, 444: 31, 445: 14, 446: 12, 447: 18, 448: 13, 449: 36, 450: 17, 451: 66, 452: 16, 453: 15, 454: 110, 455: 13, 456: 34, 457: 13, 458: 25, 459: 11, 460: 36, 461: 50, 462: 62, 463: 31, 464: 17, 465: 12, 466: 35, 467: 93, 468: 25, 469: 53, 470: 38, 471: 52, 472: 32, 473: 27, 474: 70, 475: 75, 476: 93, 477: 26, 478: 93, 479: 39, 480: 13, 481: 18, 482: 58, 483: 123, 484: 96, 485: 190, 486: 304, 487: 179, 488: 237, 489: 166, 490: 141, 491: 176, 492: 92, 493: 44, 494: 27, 495: 11, 496: 21, 497: 12, 498: 11, 499: 18, 500: 22, 501: 12, 502: 15, 503: 19, 504: 16, 505: 20, 506: 13, 507: 70, 508: 84, 509: 74, 510: 34, 511: 24, 512: 35, 513: 34, 514: 17, 515: 15, 516: 16, 517: 23, 518: 15, 519: 18, 520: 13, 521: 20, 522: 23, 523: 27, 524: 13, 525: 15, 526: 17, 527: 16, 528: 51, 529: 18, 530: 11, 531: 400, 532: 40, 533: 28, 534: 15, 535: 18, 536: 18, 537: 58, 538: 12, 539: 25, 540: 15, 541: 35, 542: 19, 543: 38, 544: 16, 545: 14, 546: 49, 547: 29, 548: 22, 549: 24, 550: 23, 551: 38, 552: 65, 553: 12, 554: 15, 555: 33, 556: 35, 557: 25, 558: 13, 559: 15, 560: 11, 561: 27, 562: 12, 563: 11, 564: 19, 565: 11, 566: 46, 567: 43, 568: 11, 569: 30, 570: 21, 571: 19, 572: 18, 573: 38, 574: 38, 575: 53, 576: 20, 577: 50, 578: 26, 579: 51, 580: 40, 581: 24, 582: 33, 583: 95, 584: 16, 585: 25, 586: 16, 587: 11, 588: 296, 589: 26, 590: 18, 591: 23, 592: 32, 593: 18, 594: 80, 595: 19, 596: 38, 597: 368, 598: 64, 599: 53, 600: 29, 601: 50, 602: 24, 603: 22, 604: 11, 605: 13, 606: 28, 607: 17, 608: 11, 609: 18, 610: 15, 611: 42, 612: 125, 613: 50, 614: 270, 615: 46, 616: 13, 617: 45, 618: 49, 619: 15, 620: 692, 621: 26, 622: 37, 623: 69, 624: 14, 625: 111, 626: 150, 627: 28, 628: 13, 629: 114, 630: 32, 631: 17, 632: 193, 633: 24, 634: 62, 635: 144, 636: 12, 637: 37, 638: 18, 639: 45, 640: 25, 641: 35, 642: 53, 643: 33, 644: 17, 645: 30, 646: 14, 647: 11, 648: 14, 649: 14, 650: 20, 651: 34, 652: 12, 653: 15, 654: 22, 655: 49, 656: 42, 657: 11, 658: 35, 659: 38, 660: 34, 661: 37, 662: 24, 663: 14, 664: 15, 665: 37, 666: 16, 667: 40, 668: 23, 669: 17, 670: 75, 671: 64, 672: 42, 673: 58, 674: 70, 675: 79, 676: 28, 677: 47, 678: 62, 679: 30, 680: 69, 681: 12, 682: 60, 683: 14, 684: 22, 685: 31, 686: 19, 687: 15, 688: 38, 689: 11, 690: 15, 691: 11, 692: 69, 693: 19, 694: 44, 695: 17, 696: 37, 697: 34, 698: 17, 699: 33, 700: 74, 701: 22, 702: 19, 703: 11, 704: 11, 705: 22, 706: 12, 707: 44, 708: 61, 709: 37, 710: 28, 711: 19, 712: 33, 713: 32, 714: 12, 715: 19, 716: 19, 717: 13, 718: 12, 719: 37, 720: 266, 721: 16, 722: 22, 723: 17, 724: 30, 725: 11, 726: 27, 727: 32, 728: 12, 729: 11, 730: 13, 731: 14, 732: 14, 733: 43, 734: 21, 735: 27, 736: 18, 737: 17, 738: 43, 739: 31, 740: 14, 741: 12, 742: 22, 743: 19, 744: 59, 745: 46, 746: 14, 747: 31, 748: 78, 749: 73, 750: 1040, 751: 118, 752: 282, 753: 272, 754: 83, 755: 224, 756: 173, 757: 57, 758: 111, 759: 366, 760: 117, 761: 58, 762: 16, 763: 21, 764: 368, 765: 30, 766: 30, 767: 34, 768: 549, 769: 40, 770: 69, 771: 179, 772: 98, 773: 11, 774: 15, 775: 28, 776: 159, 777: 41, 778: 399, 779: 23, 780: 79, 781: 165, 782: 69, 783: 37, 784: 54, 785: 20, 786: 372, 787: 20, 788: 75, 789: 21, 790: 379, 791: 37, 792: 132, 793: 39, 794: 94, 795: 70, 796: 23, 797: 14, 798: 179, 799: 55, 800: 54, 801: 129, 802: 33, 803: 11, 804: 29, 805: 137, 806: 489, 807: 389, 808: 518, 809: 294, 810: 12, 811: 40, 812: 216, 813: 185, 814: 36, 815: 86, 816: 27, 817: 47, 818: 180, 819: 129, 820: 68, 821: 117, 822: 204, 823: 37, 824: 39, 825: 62, 826: 57, 827: 21, 828: 88, 829: 44, 830: 17, 831: 12, 832: 35, 833: 17, 834: 11, 835: 24, 836: 19, 837: 23, 838: 85, 839: 14, 840: 105, 841: 11, 842: 736, 843: 117, 844: 28, 845: 15, 846: 32, 847: 55, 848: 93, 849: 28, 850: 12, 851: 21, 852: 89, 853: 56, 854: 33, 855: 113, 856: 39, 857: 85, 858: 42, 859: 22, 860: 62, 861: 37, 862: 11, 863: 15, 864: 15, 865: 17, 866: 16, 867: 16, 868: 17, 869: 12, 870: 55, 871: 11, 872: 22, 873: 18, 874: 25, 875: 32, 876: 11, 877: 17, 878: 55, 879: 17, 880: 70, 881: 12, 882: 13, 883: 54, 884: 28, 885: 14, 886: 21, 887: 13, 888: 37, 889: 169, 890: 168, 891: 62, 892: 50, 893: 112, 894: 60, 895: 53, 896: 23, 897: 27, 898: 23, 899: 13, 900: 20, 901: 11, 902: 105, 903: 14, 904: 25, 905: 166, 906: 12, 907: 18, 908: 51, 909: 24, 910: 51, 911: 36, 912: 131, 913: 218, 914: 36, 915: 18, 916: 11, 917: 20, 918: 60, 919: 12, 920: 11, 921: 59, 922: 233, 923: 15, 924: 17, 925: 54, 926: 55, 927: 29, 928: 40, 929: 61, 930: 48, 931: 15, 932: 38, 933: 39, 934: 47, 935: 35, 936: 11, 937: 16, 938: 132, 939: 47, 940: 31, 941: 14, 942: 20, 943: 15, 944: 19, 945: 15, 946: 19, 947: 18, 948: 64, 949: 55, 950: 56, 951: 53, 952: 60, 953: 38, 954: 11, 955: 11, 956: 18, 957: 12, 958: 14, 959: 11, 960: 35, 961: 183, 962: 21, 963: 16, 964: 18, 965: 12, 966: 14, 967: 21, 968: 23, 969: 26, 970: 22, 971: 11, 972: 24, 973: 24, 974: 15, 975: 22, 976: 20, 977: 33, 978: 57, 979: 11, 980: 72, 981: 25, 982: 17, 983: 65, 984: 12, 985: 21, 986: 19, 987: 38, 988: 14, 989: 65, 990: 42, 991: 12, 992: 13, 993: 70, 994: 14, 995: 11, 996: 29, 997: 2099, 998: 146, 999: 20, 1000: 44, 1001: 20, 1002: 20, 1003: 12, 1004: 251, 1005: 25, 1006: 35, 1007: 14, 1008: 29, 1009: 11, 1010: 27, 1011: 85, 1012: 38, 1013: 21, 1014: 53, 1015: 15, 1016: 40, 1017: 21, 1018: 32, 1019: 84, 1020: 20, 1021: 90, 1022: 62, 1023: 31, 1024: 30, 1025: 157, 1026: 72, 1027: 15, 1028: 16, 1029: 21, 1030: 11, 1031: 29, 1032: 14, 1033: 91, 1034: 28, 1035: 24, 1036: 14, 1037: 11, 1038: 95, 1039: 18, 1040: 53, 1041: 21, 1042: 13, 1043: 24, 1044: 39, 1045: 51, 1046: 28, 1047: 17, 1048: 38, 1049: 47, 1050: 56, 1051: 11, 1052: 12, 1053: 12, 1054: 14, 1055: 12, 1056: 19, 1057: 33, 1058: 42, 1059: 84, 1060: 30, 1061: 23, 1062: 21, 1063: 188, 1064: 12, 1065: 35, 1066: 183, 1067: 24, 1068: 15, 1069: 12, 1070: 112, 1071: 30, 1072: 41, 1073: 18, 1074: 137, 1075: 60, 1076: 13, 1077: 21, 1078: 63, 1079: 52, 1080: 34, 1081: 136, 1082: 29, 1083: 153, 1084: 23, 1085: 100, 1086: 97, 1087: 123, 1088: 233, 1089: 12, 1090: 62, 1091: 118, 1092: 64, 1093: 52, 1094: 74, 1095: 109, 1096: 92, 1097: 89, 1098: 94, 1099: 28, 1100: 87, 1101: 93, 1102: 38, 1103: 54, 1104: 201, 1105: 134, 1106: 18, 1107: 35, 1108: 27, 1109: 153, 1110: 193, 1111: 96, 1112: 177, 1113: 28, 1114: 33, 1115: 49, 1116: 57, 1117: 18, 1118: 20, 1119: 33, 1120: 17, 1121: 24, 1122: 15, 1123: 36, 1124: 26, 1125: 30, 1126: 59, 1127: 32, 1128: 34, 1129: 50, 1130: 66, 1131: 25, 1132: 40, 1133: 20, 1134: 16, 1135: 48, 1136: 30, 1137: 86, 1138: 106, 1139: 22, 1140: 11, 1141: 21, 1142: 15, 1143: 28, 1144: 13, 1145: 12, 1146: 19, 1147: 40, 1148: 30, 1149: 27, 1150: 19, 1151: 45, 1152: 19, 1153: 229, 1154: 71, 1155: 133, 1156: 27, 1157: 115, 1158: 16, 1159: 113, 1160: 24, 1161: 14, 1162: 13, 1163: 24, 1164: 38, 1165: 42, 1166: 24, 1167: 45, 1168: 100, 1169: 16, 1170: 108, 1171: 69, 1172: 20, 1173: 29, 1174: 23, 1175: 19, 1176: 24, 1177: 32, 1178: 11, 1179: 16, 1180: 33, 1181: 43, 1182: 40, 1183: 11, 1184: 17, 1185: 143, 1186: 68, 1187: 65, 1188: 11, 1189: 23, 1190: 62, 1191: 24, 1192: 32, 1193: 36, 1194: 11, 1195: 14, 1196: 28, 1197: 11, 1198: 46, 1199: 22, 1200: 168, 1201: 131, 1202: 12, 1203: 15, 1204: 11, 1205: 14, 1206: 23, 1207: 15, 1208: 11, 1209: 11, 1210: 12, 1211: 63, 1212: 113, 1213: 76, 1214: 73, 1215: 14, 1216: 12, 1217: 31, 1218: 93, 1219: 35, 1220: 11, 1221: 11, 1222: 19, 1223: 12, 1224: 44, 1225: 63, 1226: 11, 1227: 25, 1228: 77, 1229: 13, 1230: 24, 1231: 41, 1232: 15, 1233: 19, 1234: 11, 1235: 12, 1236: 21, 1237: 14, 1238: 54, 1239: 20, 1240: 14, 1241: 17, 1242: 40, 1243: 20, 1244: 18, 1245: 22, 1246: 12, 1247: 11, 1248: 117, 1249: 81, 1250: 38, 1251: 27, 1252: 11, 1253: 31, 1254: 18, 1255: 54, 1256: 14, 1257: 14, 1258: 11, 1259: 76, 1260: 21, 1261: 27, 1262: 18, 1263: 16, 1264: 22, 1265: 67, 1266: 60, 1267: 67, 1268: 62, 1269: 13, 1270: 84, 1271: 22, 1272: 37, 1273: 23, 1274: 69, 1275: 43, 1276: 22, 1277: 129, 1278: 53, 1279: 30, 1280: 31, 1281: 89, 1282: 67, 1283: 18, 1284: 16, 1285: 23, 1286: 26, 1287: 48, 1288: 26, 1289: 15, 1290: 11, 1291: 13, 1292: 21, 1293: 60, 1294: 21, 1295: 28, 1296: 27, 1297: 47, 1298: 49, 1299: 15, 1300: 11, 1301: 17, 1302: 14, 1303: 19, 1304: 12, 1305: 19, 1306: 16, 1307: 31, 1308: 11, 1309: 26, 1310: 139, 1311: 18, 1312: 50, 1313: 31, 1314: 14, 1315: 12, 1316: 14, 1317: 11, 1318: 15, 1319: 12, 1320: 78, 1321: 83, 1322: 29, 1323: 12, 1324: 20, 1325: 65, 1326: 40, 1327: 39, 1328: 34, 1329: 15, 1330: 13, 1331: 75, 1332: 54, 1333: 161, 1334: 11, 1335: 21, 1336: 39, 1337: 118, 1338: 20, 1339: 11, 1340: 28, 1341: 34, 1342: 50, 1343: 19, 1344: 12, 1345: 12, 1346: 35, 1347: 11, 1348: 13, 1349: 12, 1350: 21, 1351: 19, 1352: 147, 1353: 40, 1354: 30, 1355: 16, 1356: 25, 1357: 21, 1358: 23, 1359: 18, 1360: 14, 1361: 28, 1362: 33, 1363: 29, 1364: 14, 1365: 19, 1366: 30, 1367: 239, 1368: 37, 1369: 343, 1370: 339, 1371: 27, 1372: 17, 1373: 19, 1374: 102, 1375: 29, 1376: 83, 1377: 112, 1378: 20, 1379: 21, 1380: 14, 1381: 15, 1382: 13, 1383: 25, 1384: 18, 1385: 13, 1386: 12, 1387: 49, 1388: 31, 1389: 53, 1390: 11, 1391: 14, 1392: 21, 1393: 44, 1394: 29, 1395: 48, 1396: 14, 1397: 13, 1398: 56, 1399: 22, 1400: 22, 1401: 11, 1402: 14, 1403: 26, 1404: 30, 1405: 26, 1406: 21, 1407: 71, 1408: 14, 1409: 13, 1410: 14, 1411: 28, 1412: 13, 1413: 29, 1414: 24, 1415: 254, 1416: 11, 1417: 41, 1418: 46, 1419: 18, 1420: 39, 1421: 115, 1422: 56, 1423: 16, 1424: 48, 1425: 38, 1426: 20, 1427: 31, 1428: 31, 1429: 30, 1430: 23, 1431: 54, 1432: 13, 1433: 38, 1434: 54, 1435: 28, 1436: 58, 1437: 16, 1438: 14, 1439: 190, 1440: 42, 1441: 166, 1442: 11, 1443: 12, 1444: 48, 1445: 32, 1446: 23, 1447: 14, 1448: 15, 1449: 12, 1450: 11, 1451: 14, 1452: 59, 1453: 32, 1454: 18, 1455: 21, 1456: 11, 1457: 17, 1458: 14, 1459: 18, 1460: 24, 1461: 11, 1462: 14, 1463: 14, 1464: 25, 1465: 18, 1466: 12, 1467: 20, 1468: 22, 1469: 21, 1470: 65, 1471: 44, 1472: 36, 1473: 16, 1474: 39, 1475: 41, 1476: 54, 1477: 17, 1478: 21, 1479: 12, 1480: 64, 1481: 58, 1482: 108, 1483: 34, 1484: 39, 1485: 12, 1486: 45, 1487: 16, 1488: 25, 1489: 39, 1490: 16, 1491: 22, 1492: 12, 1493: 19, 1494: 13, 1495: 13, 1496: 19, 1497: 30, 1498: 12, 1499: 76, 1500: 56, 1501: 46, 1502: 15, 1503: 74, 1504: 146, 1505: 231, 1506: 16, 1507: 29, 1508: 20, 1509: 23, 1510: 13, 1511: 12, 1512: 19, 1513: 23, 1514: 11, 1515: 11, 1516: 15, 1517: 13, 1518: 77, 1519: 12, 1520: 18, 1521: 15, 1522: 11, 1523: 18, 1524: 13, 1525: 15, 1526: 13, 1527: 12, 1528: 13, 1529: 17, 1530: 29, 1531: 35, 1532: 22, 1533: 23, 1534: 53, 1535: 19, 1536: 28, 1537: 32, 1538: 29, 1539: 57, 1540: 162, 1541: 49, 1542: 81, 1543: 16, 1544: 20, 1545: 13, 1546: 17, 1547: 20, 1548: 25, 1549: 13, 1550: 14, 1551: 23, 1552: 59, 1553: 48, 1554: 121, 1555: 31, 1556: 12, 1557: 17, 1558: 26, 1559: 20, 1560: 36, 1561: 32, 1562: 55, 1563: 26, 1564: 11, 1565: 46, 1566: 12, 1567: 14, 1568: 16, 1569: 17, 1570: 11, 1571: 12, 1572: 67, 1573: 47, 1574: 26, 1575: 15, 1576: 18, 1577: 14, 1578: 11, 1579: 14, 1580: 66, 1581: 16, 1582: 18, 1583: 20, 1584: 65, 1585: 14, 1586: 15, 1587: 54, 1588: 13, 1589: 33, 1590: 11, 1591: 12, 1592: 42, 1593: 14, 1594: 46, 1595: 14, 1596: 17, 1597: 15, 1598: 12, 1599: 20, 1600: 19, 1601: 20, 1602: 32, 1603: 14, 1604: 21, 1605: 12, 1606: 15, 1607: 19, 1608: 19, 1609: 30, 1610: 27, 1611: 15, 1612: 19, 1613: 32, 1614: 16, 1615: 31, 1616: 60, 1617: 25, 1618: 21, 1619: 28, 1620: 337, 1621: 26, 1622: 55, 1623: 24, 1624: 57, 1625: 63, 1626: 12, 1627: 82, 1628: 15, 1629: 34, 1630: 23, 1631: 58, 1632: 267, 1633: 13, 1634: 13, 1635: 19, 1636: 34, 1637: 69, 1638: 13, 1639: 27, 1640: 13, 1641: 23, 1642: 42, 1643: 27, 1644: 12, 1645: 22, 1646: 22, 1647: 64, 1648: 14, 1649: 46, 1650: 12, 1651: 99, 1652: 22, 1653: 21, 1654: 63, 1655: 223, 1656: 25, 1657: 30, 1658: 106, 1659: 62, 1660: 13, 1661: 39, 1662: 21, 1663: 14, 1664: 48, 1665: 13, 1666: 34, 1667: 11, 1668: 13, 1669: 13, 1670: 11, 1671: 12, 1672: 18, 1673: 11, 1674: 66, 1675: 66, 1676: 41, 1677: 37, 1678: 27, 1679: 18, 1680: 38, 1681: 15, 1682: 37, 1683: 12, 1684: 21, 1685: 38, 1686: 40, 1687: 18, 1688: 12, 1689: 17, 1690: 12, 1691: 20, 1692: 20, 1693: 12, 1694: 16, 1695: 14, 1696: 28, 1697: 12, 1698: 21, 1699: 12, 1700: 40, 1701: 11, 1702: 98, 1703: 17, 1704: 182, 1705: 64, 1706: 37, 1707: 38, 1708: 58, 1709: 11, 1710: 13, 1711: 24, 1712: 34, 1713: 35, 1714: 17, 1715: 36, 1716: 14, 1717: 135, 1718: 162, 1719: 20, 1720: 60, 1721: 41, 1722: 102, 1723: 79, 1724: 74, 1725: 73, 1726: 44, 1727: 11, 1728: 28, 1729: 16, 1730: 102, 1731: 68, 1732: 23, 1733: 24, 1734: 15, 1735: 80, 1736: 44, 1737: 134, 1738: 36, 1739: 17, 1740: 16, 1741: 13, 1742: 35, 1743: 17, 1744: 74, 1745: 13, 1746: 28, 1747: 81, 1748: 14, 1749: 12, 1750: 24, 1751: 12, 1752: 106, 1753: 15, 1754: 12, 1755: 15, 1756: 24, 1757: 85, 1758: 300, 1759: 81, 1760: 16, 1761: 27, 1762: 45, 1763: 155, 1764: 19, 1765: 51, 1766: 207, 1767: 16, 1768: 12, 1769: 13, 1770: 26, 1771: 43, 1772: 11, 1773: 39, 1774: 38, 1775: 136, 1776: 1153, 1777: 107, 1778: 100, 1779: 47, 1780: 400, 1781: 57, 1782: 31, 1783: 34, 1784: 36, 1785: 15, 1786: 869, 1787: 27, 1788: 97, 1789: 57, 1790: 17, 1791: 15, 1792: 79, 1793: 34, 1794: 11, 1795: 15, 1796: 11, 1797: 24, 1798: 12, 1799: 18, 1800: 27, 1801: 42, 1802: 13, 1803: 17, 1804: 51, 1805: 18, 1806: 22, 1807: 23, 1808: 53, 1809: 29, 1810: 114, 1811: 60, 1812: 72, 1813: 25, 1814: 11, 1815: 11, 1816: 31, 1817: 26, 1818: 11, 1819: 55, 1820: 134, 1821: 111, 1822: 168, 1823: 271, 1824: 11, 1825: 12, 1826: 12, 1827: 30, 1828: 11, 1829: 103, 1830: 71, 1831: 34, 1832: 27, 1833: 17, 1834: 11, 1835: 12, 1836: 24, 1837: 18, 1838: 11, 1839: 17, 1840: 32, 1841: 12, 1842: 19, 1843: 19, 1844: 13, 1845: 34, 1846: 27, 1847: 11, 1848: 46, 1849: 18, 1850: 223, 1851: 51, 1852: 28, 1853: 22, 1854: 11, 1855: 19, 1856: 98, 1857: 20, 1858: 36, 1859: 162, 1860: 458, 1861: 17, 1862: 16, 1863: 34, 1864: 15, 1865: 86, 1866: 177, 1867: 16, 1868: 105, 1869: 15, 1870: 41, 1871: 212, 1872: 134, 1873: 23, 1874: 28, 1875: 20, 1876: 20, 1877: 15, 1878: 18, 1879: 12, 1880: 15, 1881: 14, 1882: 98, 1883: 23, 1884: 14, 1885: 13, 1886: 16, 1887: 12, 1888: 23, 1889: 14, 1890: 12, 1891: 23, 1892: 42, 1893: 73, 1894: 87, 1895: 48, 1896: 56, 1897: 21, 1898: 79, 1899: 95, 1900: 63, 1901: 20, 1902: 18, 1903: 15, 1904: 11, 1905: 46, 1906: 140, 1907: 35, 1908: 35, 1909: 19, 1910: 22, 1911: 13, 1912: 13, 1913: 36, 1914: 93, 1915: 25, 1916: 23, 1917: 32, 1918: 17, 1919: 29, 1920: 31, 1921: 14, 1922: 38, 1923: 13, 1924: 22, 1925: 11, 1926: 33, 1927: 13, 1928: 48, 1929: 15, 1930: 14, 1931: 16, 1932: 42, 1933: 17, 1934: 107, 1935: 18, 1936: 146, 1937: 50, 1938: 25, 1939: 12, 1940: 21, 1941: 81, 1942: 124, 1943: 129, 1944: 25, 1945: 13, 1946: 48, 1947: 27, 1948: 30, 1949: 12, 1950: 59, 1951: 77, 1952: 23, 1953: 19, 1954: 22, 1955: 17, 1956: 22, 1957: 13, 1958: 80, 1959: 15, 1960: 99, 1961: 11, 1962: 19, 1963: 36, 1964: 29, 1965: 21, 1966: 20, 1967: 13, 1968: 93, 1969: 119, 1970: 13, 1971: 29, 1972: 20, 1973: 34, 1974: 18, 1975: 57, 1976: 18, 1977: 100, 1978: 23, 1979: 26, 1980: 15, 1981: 14, 1982: 15, 1983: 13, 1984: 15, 1985: 47, 1986: 22, 1987: 12, 1988: 23, 1989: 11, 1990: 11, 1991: 31, 1992: 22, 1993: 16, 1994: 17, 1995: 17, 1996: 20, 1997: 33, 1998: 69, 1999: 55, 2000: 28, 2001: 27, 2002: 22, 2003: 15, 2004: 29, 2005: 14, 2006: 12, 2007: 11, 2008: 78, 2009: 13, 2010: 26, 2011: 26, 2012: 16, 2013: 17, 2014: 16, 2015: 23, 2016: 25, 2017: 13, 2018: 15, 2019: 17, 2020: 15, 2021: 45, 2022: 82, 2023: 156, 2024: 12, 2025: 11, 2026: 19, 2027: 15, 2028: 11, 2029: 12, 2030: 12, 2031: 16, 2032: 22, 2033: 25, 2034: 43, 2035: 19, 2036: 11, 2037: 22, 2038: 17, 2039: 30, 2040: 24, 2041: 46, 2042: 17, 2043: 11, 2044: 53, 2045: 36, 2046: 11, 2047: 14, 2048: 11, 2049: 13, 2050: 14, 2051: 12, 2052: 14, 2053: 12, 2054: 11, 2055: 30, 2056: 21, 2057: 30, 2058: 147, 2059: 30, 2060: 12, 2061: 13, 2062: 34, 2063: 46, 2064: 113, 2065: 138, 2066: 51, 2067: 15, 2068: 12, 2069: 20, 2070: 16, 2071: 15, 2072: 11, 2073: 11, 2074: 30, 2075: 36, 2076: 46, 2077: 77, 2078: 28, 2079: 11, 2080: 28, 2081: 18, 2082: 11, 2083: 157, 2084: 15, 2085: 91, 2086: 16, 2087: 100, 2088: 105, 2089: 102, 2090: 48, 2091: 80, 2092: 27, 2093: 16, 2094: 13, 2095: 26, 2096: 18, 2097: 18, 2098: 38, 2099: 11, 2100: 15, 2101: 12, 2102: 25, 2103: 44, 2104: 17, 2105: 51, 2106: 32, 2107: 32, 2108: 27, 2109: 24, 2110: 11, 2111: 13, 2112: 26, 2113: 28, 2114: 15, 2115: 21, 2116: 12, 2117: 18, 2118: 27, 2119: 17, 2120: 19, 2121: 11, 2122: 12, 2123: 38, 2124: 14, 2125: 15, 2126: 23, 2127: 29, 2128: 15, 2129: 12, 2130: 12, 2131: 33, 2132: 13, 2133: 15, 2134: 13, 2135: 13, 2136: 11, 2137: 13, 2138: 22, 2139: 21, 2140: 27, 2141: 90, 2142: 125, 2143: 16, 2144: 48, 2145: 192, 2146: 28, 2147: 34, 2148: 16, 2149: 25, 2150: 16, 2151: 13, 2152: 13, 2153: 131, 2154: 189, 2155: 49, 2156: 263, 2157: 203, 2158: 11, 2159: 11, 2160: 121, 2161: 107, 2162: 35, 2163: 27, 2164: 46, 2165: 60, 2166: 12, 2167: 12, 2168: 68, 2169: 59, 2170: 18, 2171: 26, 2172: 24, 2173: 18, 2174: 13, 2175: 80, 2176: 30, 2177: 15, 2178: 14, 2179: 22, 2180: 15, 2181: 16, 2182: 25, 2183: 11, 2184: 40, 2185: 119, 2186: 11, 2187: 12, 2188: 35, 2189: 77, 2190: 45, 2191: 16, 2192: 19, 2193: 44, 2194: 15, 2195: 26, 2196: 17, 2197: 11, 2198: 11, 2199: 11, 2200: 23, 2201: 22, 2202: 11, 2203: 23, 2204: 13, 2205: 18, 2206: 14, 2207: 29, 2208: 19, 2209: 15, 2210: 29, 2211: 12, 2212: 67, 2213: 15, 2214: 21, 2215: 13, 2216: 11, 2217: 13, 2218: 14, 2219: 19, 2220: 11, 2221: 11, 2222: 16, 2223: 14, 2224: 11, 2225: 12, 2226: 47, 2227: 23, 2228: 14, 2229: 14, 2230: 15, 2231: 20, 2232: 27, 2233: 19, 2234: 138, 2235: 11, 2236: 11, 2237: 39, 2238: 32, 2239: 52, 2240: 11, 2241: 28, 2242: 11, 2243: 37, 2244: 12, 2245: 34, 2246: 39, 2247: 11, 2248: 18, 2249: 11, 2250: 11, 2251: 26, 2252: 19, 2253: 11, 2254: 39, 2255: 11, 2256: 30, 2257: 19, 2258: 16, 2259: 12, 2260: 17, 2261: 70, 2262: 50, 2263: 33, 2264: 15, 2265: 15, 2266: 169, 2267: 13, 2268: 14, 2269: 25, 2270: 11, 2271: 11, 2272: 11, 2273: 12, 2274: 12, 2275: 15, 2276: 24387}\n",
      "2277 unique labels are used\n",
      "0 unique labels never used\n",
      "0 unique labels used once\n",
      "0 unique labels used twice\n"
     ]
    }
   ],
   "source": [
    "print(f\"total number of steps is:\", step_count)\n",
    "print(f\"highest frequency label is {max_freq_index} and occurs {max} times\")\n",
    "print(f\"final label used is {len(label_count)-1}\")\n",
    "print(label_count)\n",
    "print(len(label_count),\"unique labels are used\")\n",
    "print(labels_never_used,\"unique labels never used\")\n",
    "print(labels_used_once, \"unique labels used once\")\n",
    "print(labels_used_twice, \"unique labels used twice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make array of label frequencies for sklearn compute_class_weight using entire dataset\n",
    "# really should be doing this for train set (otherwise, data leakage...)\n",
    "# however, train set may not include certain labels, which leads to error in compute_class_weight\n",
    "\n",
    "# make array of unique classes\n",
    "#class_num_arr = [i for i in range(len(label_count))]\n",
    "#class_num_arr = np.array(class_num_arr)\n",
    "\n",
    "# make array of all data points with labels\n",
    "#lbl_arr = np.array([])\n",
    "#for i in range(file_limit*2):\n",
    "    #for y in hidden_conc_pf_data.get(i).y:\n",
    "        #lbl_arr = np.append(lbl_arr,[y.numpy()],axis=0).astype(int)\n",
    "\n",
    "#class_weights = compute_class_weight(class_weight=\"balanced\",classes = class_num_arr, y=lbl_arr)\n",
    "#class_weights = torch.from_numpy(class_weights).float().to(device)\n",
    "#class_weights[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIN(torch.nn.Module):\n",
    "    \"\"\"GIN\"\"\"\n",
    "    def __init__(self, dim_h, norm_mode='None', norm_scale=10):\n",
    "        super(GIN, self).__init__()\n",
    "        self.conv1 = GINConv(\n",
    "            Sequential(Linear(hidden_conc_pf_data.num_node_features, dim_h),\n",
    "                       BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()))\n",
    "        \n",
    "        self.conv2 = GINConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()))\n",
    "        \n",
    "        self.conv3 = GINConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()))\n",
    "        \n",
    "        self.lin1 = Linear(dim_h*3, dim_h*3)\n",
    "        self.lin2 = Linear(dim_h*3, hidden_conc_pf_data.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \n",
    "        h1 = self.conv1(x, edge_index)\n",
    "        h2 = self.conv2(h1, edge_index)\n",
    "        h3 = self.conv3(h2, edge_index)\n",
    "\n",
    "        # Concatenate graph embeddings\n",
    "        h = torch.cat((h1, h2, h3), dim=1)\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "\n",
    "        h = self.lin1(h)\n",
    "        h = h.relu()\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "\n",
    "        h = self.lin2(h)        \n",
    "        return F.log_softmax(h, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GIN model training\n",
    "\n",
    "def train(model, loader, lr):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    # commented out code is to use class weights to account for imbalanced dataset\n",
    "    #criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "    epochs = 200\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs+1):\n",
    "        total_loss = 0\n",
    "        acc = 0\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "        #cur_graph = 0   # used to keep track of current statement to enforce preds of only PREVIOUS labels in training\n",
    "\n",
    "        # train on batches\n",
    "        for data in loader:       \n",
    "            #cur_graph += torch.max(data.batch) + 1\n",
    "            data = data.to(device, non_blocking=True)\n",
    "            data.y = data.y.to(torch.float).to(device, non_blocking=True)\n",
    "            optimizer.zero_grad()\n",
    "            length = len(loader)\n",
    "            out = model(data.x, data.edge_index.long())\n",
    "            data.y = data.y.type(torch.LongTensor).to(device, non_blocking=True)\n",
    "            out = out.type(torch.float32).to(device, non_blocking=True)\n",
    "\n",
    "            # find conclusion/final nodes in each graph to later scale\n",
    "            #conc_node_indices = []\n",
    "            #cur_graph_counter = data.batch[0].clone()\n",
    "            #for idx, graph in enumerate(data.batch.clone()):\n",
    "                #if graph > cur_graph_counter:\n",
    "                    #conc_node_indices.append(idx-1)\n",
    "                    #cur_graph_counter += 1\n",
    "            #conc_node_indices.append(len(data.batch)-1)              \n",
    "            #weighted_out = out.clone()\n",
    "            #weighted_out[conc_node_indices,:] = weighted_out[conc_node_indices,:]*.25    # scale output from final nodes\n",
    "\n",
    "            #loss = criterion(weighted_out, data.y)\n",
    "            loss = criterion(out, data.y)\n",
    "\n",
    "            total_loss += loss / length\n",
    "\n",
    "            # commented out code below is meant to enforce predictions to only come from previous theorems\n",
    "            # for dict to be properly created, you must delete and recreate test.pt by rerunning \n",
    "            # pf_data = ProofDataset(root=\"data/\",file_limit=file_limit)\n",
    "            \n",
    "            #dict = pf_data.class_corr\n",
    "            #dict_keys = [k for k in dict.keys()]            \n",
    "\n",
    "            #def return_next_lowest_idx(num):\n",
    "                #if num in dict_keys:\n",
    "                    #return dict[num]\n",
    "                #while num not in dict_keys:\n",
    "                    #try:\n",
    "                        #num -= 1\n",
    "                        #return dict[num]\n",
    "                    #except:\n",
    "                        #pass\n",
    "\n",
    "            #with torch.no_grad():\n",
    "                #cur_graph_batch.apply_(return_next_lowest_idx)\n",
    "                #masked_lbls = (torch.arange(out.size(1)) < (cur_graph_batch[..., None]+1))*(out.cpu())\n",
    "                #try:\n",
    "                    #masked_lbls = (torch.arange(out.size(1)) < (cur_graph_batch[..., None]+1))*(out.cpu())\n",
    "                    #masked_lbls = torch.where(masked_lbls==0,np.nan,masked_lbls)\n",
    "                    #masked_lbls = masked_lbls.detach().numpy()                \n",
    "                    #pred = np.nanargmax(masked_lbls,axis=1)\n",
    "                    #pred = torch.from_numpy(pred)\n",
    "                    #acc += accuracy(pred, data.y.cpu()) / length\n",
    "                #except Exception as e:\n",
    "                    #print(\"a lil error\")\n",
    "                    #out = out + .00000001\n",
    "                    #masked_lbls = (torch.arange(out.size(1)) < (cur_graph_batch[..., None]+1))*(out.cpu())\n",
    "                    #masked_lbls = torch.where(masked_lbls==0,np.nan,masked_lbls)\n",
    "                    #masked_lbls = masked_lbls.detach().numpy()\n",
    "                    #pred = np.nanargmax(masked_lbls,axis=1)\n",
    "                    #pred = torch.from_numpy(pred)\n",
    "                    #acc += accuracy(pred, data.y.cpu()) / length\n",
    "                    #out = out - .00000001\n",
    "\n",
    "            #comment out the following pred and acc lines if enforcing predictions as described above\n",
    "            pred = out.argmax(dim=1)\n",
    "            #use the following pred line instead if weighting final/conclusion node differently\n",
    "            #pred = weighted_out.argmax(dim=1)\n",
    "\n",
    "            acc += accuracy(pred, data.y) / length\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # run model on validation set\n",
    "            val_loss, val_acc, val_f1, top5_acc = test(model, val_loader)\n",
    "\n",
    "        # Print metrics every epoch\n",
    "        if(epoch % 10 == 0):\n",
    "            print(f'Epoch {epoch:>3} | Train Loss: {total_loss:.2f} | Train Acc: {acc*100:>5.2f}% | Val Loss: {val_loss:.2f} | Val Acc: {val_acc*100:.2f}% | Top5 Val Acc: {top5_acc*100:.2f}%| F Score: {val_f1:.2f}')\n",
    "            \n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, loader):\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    top5_acc = 0\n",
    "    fscore = 0    \n",
    "    \n",
    "    for data in loader:\n",
    "        data = data.to(device, non_blocking=True)\n",
    "        length = len(loader)\n",
    "        out = model(data.x, data.edge_index.long())\n",
    "        prob = torch.exp(out)\n",
    "        prob_sorted = torch.topk(prob,k=5).indices\n",
    "        data.y = data.y.type(torch.LongTensor).to(device, non_blocking=True)\n",
    "        loss += criterion(out, data.y) / length\n",
    "        pred = out.argmax(dim=1)\n",
    "        acc += accuracy(pred, data.y) / length\n",
    "        top5_acc += torch.sum(torch.sum(prob_sorted==data.y.unsqueeze(1),dim=1),dim=0) / (length*(data.y.shape[0]))\n",
    "        fscore += f1(pred.cpu(), data.y.cpu(), average='macro')    # micro looks better, but macro prob more accurate\n",
    "\n",
    "    return loss, acc, fscore, top5_acc\n",
    "\n",
    "def accuracy(pred_y, y):\n",
    "    \"\"\"Calculate accuracy.\"\"\"\n",
    "    return ((pred_y == y).sum() / len(y)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GIN(\n",
       "  (conv1): GINConv(nn=Sequential(\n",
       "    (0): Linear(in_features=512, out_features=400, bias=True)\n",
       "    (1): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=400, out_features=400, bias=True)\n",
       "    (4): ReLU()\n",
       "  ))\n",
       "  (conv2): GINConv(nn=Sequential(\n",
       "    (0): Linear(in_features=400, out_features=400, bias=True)\n",
       "    (1): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=400, out_features=400, bias=True)\n",
       "    (4): ReLU()\n",
       "  ))\n",
       "  (conv3): GINConv(nn=Sequential(\n",
       "    (0): Linear(in_features=400, out_features=400, bias=True)\n",
       "    (1): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=400, out_features=400, bias=True)\n",
       "    (4): ReLU()\n",
       "  ))\n",
       "  (lin1): Linear(in_features=1200, out_features=1200, bias=True)\n",
       "  (lin2): Linear(in_features=1200, out_features=2277, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize (and reset weights of) model\n",
    "# dim_h is hyperparameter of number of hidden layers\n",
    "\n",
    "gin_trained = None\n",
    "gin = None\n",
    "gin = GIN(dim_h=400).to(device)\n",
    "gin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | Train Loss: 7.00 | Train Acc: 16.14% | Val Loss: 5.78 | Val Acc: 17.41% | Top5 Val Acc: 30.02%| F Score: 0.00\n",
      "Epoch  10 | Train Loss: 3.46 | Train Acc: 36.80% | Val Loss: 4.02 | Val Acc: 26.29% | Top5 Val Acc: 44.22%| F Score: 0.01\n",
      "Epoch  20 | Train Loss: 2.56 | Train Acc: 44.15% | Val Loss: 3.13 | Val Acc: 31.80% | Top5 Val Acc: 55.83%| F Score: 0.09\n",
      "Epoch  30 | Train Loss: 2.17 | Train Acc: 49.55% | Val Loss: 2.65 | Val Acc: 38.58% | Top5 Val Acc: 65.62%| F Score: 0.21\n",
      "Epoch  40 | Train Loss: 1.67 | Train Acc: 57.84% | Val Loss: 2.34 | Val Acc: 44.73% | Top5 Val Acc: 71.74%| F Score: 0.39\n",
      "Epoch  50 | Train Loss: 1.42 | Train Acc: 63.07% | Val Loss: 2.32 | Val Acc: 45.10% | Top5 Val Acc: 72.93%| F Score: 0.41\n",
      "Epoch  60 | Train Loss: 1.34 | Train Acc: 65.02% | Val Loss: 2.51 | Val Acc: 43.74% | Top5 Val Acc: 70.73%| F Score: 0.40\n",
      "Epoch  70 | Train Loss: 1.14 | Train Acc: 69.26% | Val Loss: 2.27 | Val Acc: 48.69% | Top5 Val Acc: 75.89%| F Score: 0.52\n",
      "Epoch  80 | Train Loss: 0.95 | Train Acc: 74.36% | Val Loss: 3.37 | Val Acc: 38.39% | Top5 Val Acc: 62.07%| F Score: 0.33\n",
      "Epoch  90 | Train Loss: 0.72 | Train Acc: 80.15% | Val Loss: 2.43 | Val Acc: 50.23% | Top5 Val Acc: 76.71%| F Score: 0.59\n",
      "Epoch 100 | Train Loss: 0.82 | Train Acc: 78.62% | Val Loss: 2.40 | Val Acc: 53.90% | Top5 Val Acc: 78.34%| F Score: 0.67\n",
      "Epoch 110 | Train Loss: 0.46 | Train Acc: 87.56% | Val Loss: 2.91 | Val Acc: 51.53% | Top5 Val Acc: 77.62%| F Score: 0.62\n",
      "Epoch 120 | Train Loss: 0.41 | Train Acc: 89.32% | Val Loss: 2.80 | Val Acc: 54.30% | Top5 Val Acc: 78.94%| F Score: 0.68\n",
      "Epoch 130 | Train Loss: 0.36 | Train Acc: 90.34% | Val Loss: 4.55 | Val Acc: 41.10% | Top5 Val Acc: 65.90%| F Score: 0.39\n",
      "Epoch 140 | Train Loss: 0.35 | Train Acc: 91.34% | Val Loss: 4.37 | Val Acc: 41.75% | Top5 Val Acc: 65.73%| F Score: 0.44\n",
      "Epoch 150 | Train Loss: 0.52 | Train Acc: 87.57% | Val Loss: 2.98 | Val Acc: 55.11% | Top5 Val Acc: 78.63%| F Score: 0.69\n",
      "Epoch 160 | Train Loss: 0.24 | Train Acc: 94.35% | Val Loss: 4.88 | Val Acc: 42.57% | Top5 Val Acc: 68.56%| F Score: 0.47\n",
      "Epoch 170 | Train Loss: 0.54 | Train Acc: 87.79% | Val Loss: 3.28 | Val Acc: 56.06% | Top5 Val Acc: 78.35%| F Score: 0.71\n",
      "Epoch 180 | Train Loss: 0.16 | Train Acc: 96.54% | Val Loss: 3.79 | Val Acc: 55.86% | Top5 Val Acc: 78.35%| F Score: 0.71\n",
      "Epoch 190 | Train Loss: 0.15 | Train Acc: 96.79% | Val Loss: 4.03 | Val Acc: 56.14% | Top5 Val Acc: 78.24%| F Score: 0.72\n",
      "Epoch 200 | Train Loss: 0.15 | Train Acc: 96.82% | Val Loss: 3.98 | Val Acc: 55.95% | Top5 Val Acc: 78.25%| F Score: 0.72\n"
     ]
    }
   ],
   "source": [
    "# reset weights and train model\n",
    "gin_trained = None\n",
    "gin_trained = train(gin, train_loader,lr=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restricting to conclusion nodes, our val_graph has accuracy being 41.40%\n",
      "'unk' accuracy should be: 68.55%\n",
      "top5 accuracy is: 69.60%\n"
     ]
    }
   ],
   "source": [
    "# get accuracy only for conclusion node from val set\n",
    "\n",
    "total_conc_labels = len(val_dataset)\n",
    "correct_conc_pred = 0\n",
    "\n",
    "# dict of the form true_label:(predicted_labels)\n",
    "incorrect_preds = {}\n",
    "\n",
    "for graph in val_dataset:\n",
    "    get_gin_predict = gin_trained(graph.x.to(device),graph.edge_index.long().to(device))\n",
    "    get_conc_predict = np.argmax(get_gin_predict[-1].detach().cpu().numpy(),axis=0)\n",
    "    if get_conc_predict == graph.y[-1].item():\n",
    "        correct_conc_pred += 1\n",
    "    if get_conc_predict != graph.y[-1].item():\n",
    "        if graph.y[-1].item() in incorrect_preds:\n",
    "            incorrect_preds[graph.y[-1].item()] = incorrect_preds[graph.y[-1].item()]+(get_conc_predict,)\n",
    "        else:\n",
    "            incorrect_preds[graph.y[-1].item()] = (get_conc_predict,)\n",
    "\n",
    "\n",
    "def get_conc_gin_label_acc(i):\n",
    "    correct_pred_i = 0\n",
    "    total_num_i = 0\n",
    "\n",
    "    for graph in val_dataset:\n",
    "        get_gin_predict = gin_trained(graph.x.to(device),graph.edge_index.long().to(device))\n",
    "        get_conc_predict = np.argmax(get_gin_predict[-1].detach().cpu().numpy(),axis=0)\n",
    "        if graph.y[-1].item() == i:\n",
    "            total_num_i += 1\n",
    "            if get_conc_predict == i:\n",
    "                correct_pred_i += 1\n",
    "    if total_num_i == 0:\n",
    "        return 0,0,0\n",
    "    return (correct_pred_i / total_num_i), total_num_i, correct_pred_i\n",
    "\n",
    "def get_topk_acc(k):\n",
    "\n",
    "    top3_corr = 0\n",
    "\n",
    "    for graph in val_dataset:\n",
    "        get_gin_predict = gin_trained(graph.x.to(device),graph.edge_index.long().to(device))\n",
    "        prob = torch.exp(get_gin_predict[-1])\n",
    "        prob_sorted = torch.topk(prob,k=k).indices\n",
    "        graph.y = graph.y.to(device)\n",
    "        top3_corr += torch.sum(prob_sorted==graph.y[-1])\n",
    "\n",
    "    return top3_corr / len(val_dataset)\n",
    "\n",
    "k=5\n",
    "print(f\"restricting to conclusion nodes, our val_graph has accuracy being {correct_conc_pred/total_conc_labels*100:.2f}%\")\n",
    "print(f\"'unk' accuracy should be: {get_conc_gin_label_acc(2276)[0]*100:.2f}%\")\n",
    "print(f\"top{k} accuracy is: {get_topk_acc(k).item()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary to record label frequency based on val set\n",
    "# only consider conclusion/final nodes\n",
    "\n",
    "# get max label used in pf_data\n",
    "max_label = 0\n",
    "\n",
    "for i in val_indices:\n",
    "    if hidden_conc_pf_data.get(i).y[-1] > max_label:\n",
    "        max_label = hidden_conc_pf_data.get(i).y[-1].to(int).item()\n",
    "\n",
    "# initialize histogram for labels used in pf_data            \n",
    "label_count = {}\n",
    "\n",
    "for i in val_indices:\n",
    "    for j in range(max_label+1):        \n",
    "        label_count[j] = 0\n",
    "\n",
    "for i in val_indices:\n",
    "    label_count[hidden_conc_pf_data.get(i).y[-1].to(int).item()] += 1\n",
    "\n",
    "step_count = 0\n",
    "max = 0\n",
    "max_freq_index = None   # find the most frequently used index\n",
    "labels_never_used = 0\n",
    "labels_used_once = 0\n",
    "labels_used_twice = 0\n",
    "\n",
    "\n",
    "for k,v in label_count.items():\n",
    "    step_count += v\n",
    "    if v > max:     \n",
    "        max = v\n",
    "        max_freq_index = k\n",
    "\n",
    "    if v == 0:\n",
    "        labels_never_used += 1\n",
    "    if v == 1:\n",
    "        labels_used_once += 1\n",
    "    if v ==2:\n",
    "        labels_used_twice += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of steps is: 1000\n",
      "highest frequency label is 2276 and occurs 124 times\n",
      "final label used is 2276\n",
      "{0: 2, 1: 31, 2: 0, 3: 2, 4: 1, 5: 5, 6: 0, 7: 0, 8: 0, 9: 7, 10: 0, 11: 34, 12: 1, 13: 0, 14: 4, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 2, 23: 3, 24: 0, 25: 4, 26: 5, 27: 0, 28: 0, 29: 2, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 1, 37: 1, 38: 0, 39: 1, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 4, 46: 0, 47: 0, 48: 2, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 1, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 1, 70: 0, 71: 0, 72: 0, 73: 0, 74: 1, 75: 1, 76: 2, 77: 1, 78: 0, 79: 0, 80: 2, 81: 0, 82: 0, 83: 0, 84: 1, 85: 0, 86: 0, 87: 1, 88: 0, 89: 1, 90: 17, 91: 2, 92: 1, 93: 1, 94: 0, 95: 0, 96: 1, 97: 0, 98: 25, 99: 14, 100: 0, 101: 3, 102: 12, 103: 6, 104: 0, 105: 0, 106: 0, 107: 1, 108: 6, 109: 8, 110: 5, 111: 0, 112: 2, 113: 8, 114: 17, 115: 5, 116: 1, 117: 17, 118: 1, 119: 0, 120: 0, 121: 1, 122: 1, 123: 1, 124: 0, 125: 1, 126: 1, 127: 1, 128: 0, 129: 0, 130: 0, 131: 4, 132: 0, 133: 0, 134: 12, 135: 0, 136: 0, 137: 1, 138: 0, 139: 0, 140: 1, 141: 0, 142: 2, 143: 0, 144: 3, 145: 1, 146: 1, 147: 29, 148: 5, 149: 4, 150: 21, 151: 5, 152: 0, 153: 0, 154: 0, 155: 1, 156: 0, 157: 0, 158: 3, 159: 0, 160: 4, 161: 2, 162: 0, 163: 4, 164: 0, 165: 0, 166: 5, 167: 9, 168: 0, 169: 0, 170: 0, 171: 3, 172: 1, 173: 24, 174: 3, 175: 2, 176: 0, 177: 2, 178: 0, 179: 1, 180: 13, 181: 0, 182: 0, 183: 0, 184: 0, 185: 0, 186: 0, 187: 1, 188: 4, 189: 0, 190: 0, 191: 0, 192: 0, 193: 0, 194: 0, 195: 0, 196: 1, 197: 0, 198: 1, 199: 2, 200: 0, 201: 0, 202: 0, 203: 0, 204: 0, 205: 0, 206: 0, 207: 0, 208: 1, 209: 0, 210: 0, 211: 0, 212: 0, 213: 0, 214: 2, 215: 0, 216: 0, 217: 0, 218: 1, 219: 0, 220: 0, 221: 0, 222: 4, 223: 2, 224: 0, 225: 0, 226: 0, 227: 0, 228: 0, 229: 0, 230: 16, 231: 4, 232: 0, 233: 0, 234: 0, 235: 5, 236: 0, 237: 1, 238: 0, 239: 2, 240: 2, 241: 1, 242: 0, 243: 0, 244: 0, 245: 0, 246: 0, 247: 0, 248: 0, 249: 0, 250: 0, 251: 0, 252: 0, 253: 0, 254: 0, 255: 0, 256: 4, 257: 0, 258: 0, 259: 1, 260: 0, 261: 0, 262: 0, 263: 0, 264: 0, 265: 0, 266: 0, 267: 0, 268: 0, 269: 1, 270: 0, 271: 0, 272: 2, 273: 0, 274: 0, 275: 0, 276: 0, 277: 0, 278: 0, 279: 0, 280: 0, 281: 0, 282: 1, 283: 3, 284: 1, 285: 6, 286: 0, 287: 0, 288: 0, 289: 0, 290: 0, 291: 2, 292: 1, 293: 2, 294: 0, 295: 1, 296: 0, 297: 3, 298: 0, 299: 0, 300: 0, 301: 1, 302: 1, 303: 0, 304: 0, 305: 1, 306: 0, 307: 0, 308: 0, 309: 0, 310: 0, 311: 0, 312: 0, 313: 0, 314: 0, 315: 0, 316: 0, 317: 0, 318: 0, 319: 0, 320: 1, 321: 0, 322: 0, 323: 0, 324: 0, 325: 0, 326: 0, 327: 0, 328: 0, 329: 1, 330: 1, 331: 0, 332: 5, 333: 0, 334: 6, 335: 11, 336: 1, 337: 2, 338: 1, 339: 1, 340: 1, 341: 3, 342: 1, 343: 1, 344: 3, 345: 0, 346: 0, 347: 0, 348: 0, 349: 0, 350: 0, 351: 0, 352: 0, 353: 0, 354: 0, 355: 0, 356: 0, 357: 0, 358: 0, 359: 0, 360: 0, 361: 1, 362: 0, 363: 0, 364: 0, 365: 0, 366: 0, 367: 0, 368: 0, 369: 0, 370: 0, 371: 0, 372: 0, 373: 2, 374: 0, 375: 1, 376: 2, 377: 5, 378: 2, 379: 10, 380: 1, 381: 0, 382: 0, 383: 0, 384: 0, 385: 0, 386: 2, 387: 0, 388: 0, 389: 1, 390: 2, 391: 0, 392: 0, 393: 0, 394: 0, 395: 1, 396: 1, 397: 0, 398: 0, 399: 0, 400: 1, 401: 0, 402: 0, 403: 0, 404: 0, 405: 0, 406: 0, 407: 0, 408: 0, 409: 0, 410: 0, 411: 0, 412: 0, 413: 0, 414: 0, 415: 0, 416: 0, 417: 0, 418: 0, 419: 2, 420: 2, 421: 0, 422: 1, 423: 0, 424: 0, 425: 2, 426: 0, 427: 2, 428: 0, 429: 0, 430: 0, 431: 0, 432: 1, 433: 0, 434: 0, 435: 0, 436: 0, 437: 2, 438: 0, 439: 0, 440: 1, 441: 0, 442: 0, 443: 0, 444: 1, 445: 0, 446: 0, 447: 0, 448: 1, 449: 0, 450: 0, 451: 0, 452: 0, 453: 0, 454: 0, 455: 0, 456: 0, 457: 0, 458: 0, 459: 0, 460: 1, 461: 0, 462: 0, 463: 0, 464: 0, 465: 0, 466: 0, 467: 0, 468: 0, 469: 0, 470: 2, 471: 3, 472: 2, 473: 1, 474: 2, 475: 0, 476: 0, 477: 0, 478: 0, 479: 0, 480: 0, 481: 0, 482: 1, 483: 0, 484: 0, 485: 0, 486: 5, 487: 1, 488: 3, 489: 0, 490: 0, 491: 0, 492: 0, 493: 1, 494: 0, 495: 0, 496: 0, 497: 0, 498: 0, 499: 0, 500: 0, 501: 1, 502: 2, 503: 0, 504: 2, 505: 1, 506: 1, 507: 0, 508: 0, 509: 0, 510: 0, 511: 0, 512: 0, 513: 0, 514: 0, 515: 0, 516: 0, 517: 0, 518: 0, 519: 0, 520: 0, 521: 0, 522: 0, 523: 0, 524: 0, 525: 0, 526: 0, 527: 0, 528: 0, 529: 1, 530: 0, 531: 3, 532: 0, 533: 1, 534: 0, 535: 0, 536: 0, 537: 0, 538: 0, 539: 0, 540: 0, 541: 0, 542: 0, 543: 0, 544: 1, 545: 0, 546: 1, 547: 0, 548: 0, 549: 0, 550: 0, 551: 0, 552: 6, 553: 0, 554: 0, 555: 0, 556: 0, 557: 0, 558: 0, 559: 2, 560: 0, 561: 1, 562: 1, 563: 0, 564: 0, 565: 1, 566: 0, 567: 0, 568: 0, 569: 0, 570: 0, 571: 0, 572: 0, 573: 0, 574: 0, 575: 0, 576: 0, 577: 0, 578: 0, 579: 0, 580: 1, 581: 0, 582: 0, 583: 0, 584: 0, 585: 0, 586: 0, 587: 0, 588: 0, 589: 0, 590: 0, 591: 0, 592: 0, 593: 0, 594: 0, 595: 0, 596: 1, 597: 0, 598: 0, 599: 5, 600: 2, 601: 0, 602: 0, 603: 0, 604: 0, 605: 0, 606: 0, 607: 0, 608: 0, 609: 0, 610: 0, 611: 0, 612: 0, 613: 0, 614: 0, 615: 0, 616: 0, 617: 0, 618: 0, 619: 0, 620: 0, 621: 0, 622: 0, 623: 0, 624: 0, 625: 1, 626: 0, 627: 0, 628: 0, 629: 0, 630: 0, 631: 0, 632: 0, 633: 1, 634: 0, 635: 1, 636: 0, 637: 1, 638: 0, 639: 0, 640: 0, 641: 0, 642: 0, 643: 0, 644: 0, 645: 0, 646: 0, 647: 0, 648: 0, 649: 0, 650: 0, 651: 0, 652: 0, 653: 0, 654: 0, 655: 0, 656: 0, 657: 0, 658: 0, 659: 0, 660: 0, 661: 0, 662: 0, 663: 0, 664: 0, 665: 0, 666: 0, 667: 0, 668: 0, 669: 0, 670: 0, 671: 0, 672: 1, 673: 0, 674: 0, 675: 0, 676: 0, 677: 0, 678: 0, 679: 0, 680: 0, 681: 0, 682: 1, 683: 0, 684: 0, 685: 0, 686: 0, 687: 0, 688: 0, 689: 0, 690: 0, 691: 0, 692: 1, 693: 0, 694: 0, 695: 0, 696: 0, 697: 0, 698: 0, 699: 0, 700: 0, 701: 0, 702: 1, 703: 0, 704: 0, 705: 0, 706: 0, 707: 0, 708: 0, 709: 0, 710: 0, 711: 0, 712: 0, 713: 0, 714: 0, 715: 0, 716: 0, 717: 0, 718: 0, 719: 0, 720: 0, 721: 0, 722: 0, 723: 0, 724: 0, 725: 0, 726: 2, 727: 0, 728: 0, 729: 0, 730: 0, 731: 0, 732: 0, 733: 0, 734: 0, 735: 0, 736: 0, 737: 0, 738: 0, 739: 0, 740: 0, 741: 0, 742: 0, 743: 0, 744: 0, 745: 0, 746: 0, 747: 0, 748: 8, 749: 3, 750: 0, 751: 0, 752: 0, 753: 0, 754: 0, 755: 0, 756: 0, 757: 0, 758: 0, 759: 0, 760: 0, 761: 0, 762: 0, 763: 0, 764: 0, 765: 0, 766: 0, 767: 0, 768: 12, 769: 1, 770: 3, 771: 5, 772: 7, 773: 0, 774: 0, 775: 0, 776: 11, 777: 1, 778: 11, 779: 1, 780: 1, 781: 1, 782: 0, 783: 0, 784: 1, 785: 0, 786: 13, 787: 0, 788: 0, 789: 0, 790: 3, 791: 0, 792: 2, 793: 1, 794: 1, 795: 1, 796: 0, 797: 0, 798: 9, 799: 0, 800: 0, 801: 0, 802: 0, 803: 0, 804: 0, 805: 0, 806: 0, 807: 0, 808: 0, 809: 0, 810: 0, 811: 0, 812: 0, 813: 0, 814: 0, 815: 1, 816: 1, 817: 1, 818: 6, 819: 2, 820: 0, 821: 0, 822: 2, 823: 0, 824: 0, 825: 0, 826: 0, 827: 0, 828: 1, 829: 0, 830: 0, 831: 0, 832: 0, 833: 0, 834: 1, 835: 0, 836: 0, 837: 0, 838: 0, 839: 0, 840: 2, 841: 1, 842: 0, 843: 0, 844: 0, 845: 0, 846: 0, 847: 0, 848: 0, 849: 0, 850: 1, 851: 0, 852: 0, 853: 0, 854: 0, 855: 0, 856: 0, 857: 0, 858: 0, 859: 0, 860: 2, 861: 0, 862: 0, 863: 0, 864: 0, 865: 0, 866: 2, 867: 0, 868: 0, 869: 0, 870: 0, 871: 0, 872: 2, 873: 0, 874: 0, 875: 0, 876: 0, 877: 0, 878: 1, 879: 0, 880: 0, 881: 0, 882: 0, 883: 0, 884: 3, 885: 0, 886: 0, 887: 0, 888: 0, 889: 0, 890: 0, 891: 0, 892: 0, 893: 0, 894: 2, 895: 0, 896: 2, 897: 2, 898: 0, 899: 0, 900: 0, 901: 0, 902: 0, 903: 0, 904: 0, 905: 0, 906: 0, 907: 0, 908: 0, 909: 0, 910: 1, 911: 0, 912: 0, 913: 0, 914: 0, 915: 1, 916: 0, 917: 0, 918: 0, 919: 0, 920: 1, 921: 0, 922: 0, 923: 0, 924: 0, 925: 1, 926: 0, 927: 0, 928: 1, 929: 0, 930: 0, 931: 0, 932: 0, 933: 0, 934: 0, 935: 0, 936: 1, 937: 0, 938: 0, 939: 0, 940: 0, 941: 0, 942: 0, 943: 0, 944: 0, 945: 0, 946: 0, 947: 0, 948: 0, 949: 0, 950: 0, 951: 2, 952: 0, 953: 0, 954: 0, 955: 0, 956: 0, 957: 0, 958: 0, 959: 1, 960: 0, 961: 0, 962: 0, 963: 0, 964: 0, 965: 0, 966: 0, 967: 0, 968: 0, 969: 0, 970: 0, 971: 0, 972: 0, 973: 0, 974: 0, 975: 0, 976: 0, 977: 0, 978: 0, 979: 0, 980: 0, 981: 0, 982: 0, 983: 0, 984: 0, 985: 0, 986: 0, 987: 0, 988: 0, 989: 0, 990: 0, 991: 0, 992: 0, 993: 0, 994: 0, 995: 0, 996: 0, 997: 0, 998: 0, 999: 0, 1000: 0, 1001: 0, 1002: 1, 1003: 0, 1004: 0, 1005: 0, 1006: 0, 1007: 0, 1008: 0, 1009: 0, 1010: 1, 1011: 3, 1012: 0, 1013: 0, 1014: 0, 1015: 0, 1016: 0, 1017: 0, 1018: 0, 1019: 0, 1020: 0, 1021: 0, 1022: 0, 1023: 0, 1024: 0, 1025: 0, 1026: 0, 1027: 0, 1028: 0, 1029: 0, 1030: 0, 1031: 0, 1032: 1, 1033: 0, 1034: 1, 1035: 2, 1036: 0, 1037: 0, 1038: 0, 1039: 0, 1040: 0, 1041: 0, 1042: 0, 1043: 0, 1044: 0, 1045: 0, 1046: 0, 1047: 0, 1048: 0, 1049: 0, 1050: 0, 1051: 0, 1052: 0, 1053: 0, 1054: 0, 1055: 0, 1056: 0, 1057: 0, 1058: 0, 1059: 0, 1060: 0, 1061: 0, 1062: 0, 1063: 0, 1064: 0, 1065: 0, 1066: 0, 1067: 0, 1068: 0, 1069: 0, 1070: 0, 1071: 0, 1072: 0, 1073: 0, 1074: 0, 1075: 0, 1076: 0, 1077: 0, 1078: 0, 1079: 0, 1080: 0, 1081: 0, 1082: 0, 1083: 0, 1084: 0, 1085: 1, 1086: 0, 1087: 0, 1088: 0, 1089: 0, 1090: 2, 1091: 4, 1092: 0, 1093: 0, 1094: 0, 1095: 1, 1096: 0, 1097: 0, 1098: 0, 1099: 2, 1100: 3, 1101: 0, 1102: 0, 1103: 0, 1104: 0, 1105: 0, 1106: 0, 1107: 0, 1108: 0, 1109: 0, 1110: 0, 1111: 0, 1112: 2, 1113: 0, 1114: 1, 1115: 1, 1116: 1, 1117: 0, 1118: 1, 1119: 0, 1120: 2, 1121: 0, 1122: 2, 1123: 0, 1124: 0, 1125: 1, 1126: 0, 1127: 0, 1128: 0, 1129: 0, 1130: 0, 1131: 0, 1132: 0, 1133: 0, 1134: 0, 1135: 0, 1136: 0, 1137: 0, 1138: 0, 1139: 0, 1140: 0, 1141: 0, 1142: 0, 1143: 0, 1144: 0, 1145: 0, 1146: 0, 1147: 0, 1148: 0, 1149: 0, 1150: 0, 1151: 0, 1152: 0, 1153: 0, 1154: 0, 1155: 0, 1156: 0, 1157: 0, 1158: 0, 1159: 0, 1160: 0, 1161: 0, 1162: 0, 1163: 0, 1164: 0, 1165: 0, 1166: 0, 1167: 0, 1168: 0, 1169: 0, 1170: 0, 1171: 0, 1172: 0, 1173: 0, 1174: 0, 1175: 0, 1176: 0, 1177: 0, 1178: 0, 1179: 0, 1180: 0, 1181: 0, 1182: 0, 1183: 0, 1184: 0, 1185: 0, 1186: 0, 1187: 0, 1188: 0, 1189: 0, 1190: 0, 1191: 0, 1192: 0, 1193: 0, 1194: 0, 1195: 0, 1196: 0, 1197: 0, 1198: 0, 1199: 0, 1200: 0, 1201: 0, 1202: 0, 1203: 0, 1204: 0, 1205: 0, 1206: 0, 1207: 0, 1208: 0, 1209: 0, 1210: 0, 1211: 0, 1212: 0, 1213: 0, 1214: 0, 1215: 0, 1216: 0, 1217: 0, 1218: 0, 1219: 0, 1220: 0, 1221: 0, 1222: 0, 1223: 0, 1224: 0, 1225: 0, 1226: 0, 1227: 0, 1228: 0, 1229: 0, 1230: 0, 1231: 0, 1232: 0, 1233: 0, 1234: 0, 1235: 0, 1236: 0, 1237: 0, 1238: 0, 1239: 0, 1240: 0, 1241: 0, 1242: 0, 1243: 0, 1244: 0, 1245: 0, 1246: 0, 1247: 0, 1248: 0, 1249: 0, 1250: 0, 1251: 0, 1252: 0, 1253: 0, 1254: 0, 1255: 0, 1256: 0, 1257: 0, 1258: 0, 1259: 0, 1260: 0, 1261: 0, 1262: 0, 1263: 0, 1264: 0, 1265: 0, 1266: 0, 1267: 0, 1268: 0, 1269: 0, 1270: 0, 1271: 0, 1272: 0, 1273: 0, 1274: 0, 1275: 0, 1276: 0, 1277: 0, 1278: 0, 1279: 0, 1280: 0, 1281: 0, 1282: 0, 1283: 0, 1284: 0, 1285: 0, 1286: 0, 1287: 0, 1288: 0, 1289: 0, 1290: 0, 1291: 0, 1292: 0, 1293: 0, 1294: 0, 1295: 0, 1296: 0, 1297: 0, 1298: 0, 1299: 0, 1300: 0, 1301: 0, 1302: 0, 1303: 0, 1304: 0, 1305: 0, 1306: 0, 1307: 0, 1308: 0, 1309: 0, 1310: 0, 1311: 0, 1312: 0, 1313: 0, 1314: 0, 1315: 0, 1316: 0, 1317: 0, 1318: 0, 1319: 0, 1320: 0, 1321: 0, 1322: 0, 1323: 0, 1324: 0, 1325: 0, 1326: 0, 1327: 0, 1328: 0, 1329: 0, 1330: 0, 1331: 0, 1332: 0, 1333: 0, 1334: 0, 1335: 0, 1336: 0, 1337: 0, 1338: 0, 1339: 0, 1340: 0, 1341: 0, 1342: 0, 1343: 0, 1344: 0, 1345: 0, 1346: 0, 1347: 0, 1348: 0, 1349: 0, 1350: 0, 1351: 0, 1352: 0, 1353: 0, 1354: 0, 1355: 0, 1356: 0, 1357: 0, 1358: 0, 1359: 0, 1360: 0, 1361: 0, 1362: 0, 1363: 0, 1364: 0, 1365: 0, 1366: 0, 1367: 0, 1368: 0, 1369: 0, 1370: 0, 1371: 0, 1372: 0, 1373: 0, 1374: 0, 1375: 0, 1376: 0, 1377: 0, 1378: 0, 1379: 0, 1380: 0, 1381: 0, 1382: 0, 1383: 1, 1384: 1, 1385: 1, 1386: 0, 1387: 0, 1388: 0, 1389: 0, 1390: 0, 1391: 0, 1392: 0, 1393: 0, 1394: 0, 1395: 0, 1396: 0, 1397: 0, 1398: 0, 1399: 0, 1400: 0, 1401: 0, 1402: 0, 1403: 0, 1404: 0, 1405: 0, 1406: 0, 1407: 0, 1408: 0, 1409: 0, 1410: 0, 1411: 0, 1412: 0, 1413: 0, 1414: 0, 1415: 0, 1416: 0, 1417: 0, 1418: 0, 1419: 0, 1420: 1, 1421: 0, 1422: 0, 1423: 0, 1424: 0, 1425: 0, 1426: 0, 1427: 1, 1428: 0, 1429: 0, 1430: 0, 1431: 0, 1432: 0, 1433: 0, 1434: 0, 1435: 0, 1436: 0, 1437: 0, 1438: 0, 1439: 0, 1440: 0, 1441: 0, 1442: 0, 1443: 0, 1444: 0, 1445: 0, 1446: 0, 1447: 0, 1448: 0, 1449: 0, 1450: 0, 1451: 0, 1452: 0, 1453: 0, 1454: 0, 1455: 0, 1456: 0, 1457: 0, 1458: 0, 1459: 0, 1460: 0, 1461: 0, 1462: 0, 1463: 0, 1464: 0, 1465: 0, 1466: 0, 1467: 0, 1468: 0, 1469: 0, 1470: 0, 1471: 0, 1472: 0, 1473: 0, 1474: 0, 1475: 0, 1476: 0, 1477: 0, 1478: 0, 1479: 0, 1480: 0, 1481: 0, 1482: 0, 1483: 0, 1484: 0, 1485: 0, 1486: 0, 1487: 0, 1488: 0, 1489: 0, 1490: 0, 1491: 0, 1492: 0, 1493: 0, 1494: 0, 1495: 0, 1496: 0, 1497: 0, 1498: 0, 1499: 0, 1500: 0, 1501: 0, 1502: 0, 1503: 0, 1504: 0, 1505: 0, 1506: 0, 1507: 0, 1508: 0, 1509: 0, 1510: 0, 1511: 0, 1512: 0, 1513: 0, 1514: 0, 1515: 0, 1516: 1, 1517: 0, 1518: 0, 1519: 0, 1520: 0, 1521: 0, 1522: 0, 1523: 0, 1524: 0, 1525: 0, 1526: 0, 1527: 0, 1528: 0, 1529: 0, 1530: 0, 1531: 0, 1532: 0, 1533: 0, 1534: 0, 1535: 0, 1536: 0, 1537: 0, 1538: 0, 1539: 0, 1540: 0, 1541: 0, 1542: 0, 1543: 0, 1544: 0, 1545: 0, 1546: 0, 1547: 0, 1548: 0, 1549: 0, 1550: 0, 1551: 0, 1552: 0, 1553: 0, 1554: 0, 1555: 0, 1556: 0, 1557: 0, 1558: 0, 1559: 0, 1560: 0, 1561: 0, 1562: 0, 1563: 0, 1564: 0, 1565: 0, 1566: 0, 1567: 0, 1568: 0, 1569: 0, 1570: 0, 1571: 0, 1572: 0, 1573: 0, 1574: 0, 1575: 0, 1576: 0, 1577: 0, 1578: 0, 1579: 0, 1580: 0, 1581: 0, 1582: 0, 1583: 0, 1584: 0, 1585: 0, 1586: 0, 1587: 0, 1588: 0, 1589: 0, 1590: 0, 1591: 0, 1592: 0, 1593: 0, 1594: 0, 1595: 0, 1596: 0, 1597: 0, 1598: 0, 1599: 0, 1600: 0, 1601: 0, 1602: 0, 1603: 0, 1604: 0, 1605: 0, 1606: 0, 1607: 0, 1608: 0, 1609: 0, 1610: 0, 1611: 0, 1612: 0, 1613: 0, 1614: 0, 1615: 0, 1616: 0, 1617: 0, 1618: 0, 1619: 0, 1620: 0, 1621: 0, 1622: 0, 1623: 0, 1624: 0, 1625: 0, 1626: 0, 1627: 0, 1628: 0, 1629: 0, 1630: 0, 1631: 0, 1632: 0, 1633: 0, 1634: 0, 1635: 0, 1636: 0, 1637: 0, 1638: 0, 1639: 0, 1640: 0, 1641: 0, 1642: 0, 1643: 0, 1644: 0, 1645: 0, 1646: 0, 1647: 0, 1648: 0, 1649: 0, 1650: 0, 1651: 0, 1652: 0, 1653: 0, 1654: 0, 1655: 0, 1656: 0, 1657: 0, 1658: 0, 1659: 0, 1660: 0, 1661: 0, 1662: 0, 1663: 0, 1664: 0, 1665: 0, 1666: 0, 1667: 0, 1668: 0, 1669: 0, 1670: 0, 1671: 0, 1672: 0, 1673: 0, 1674: 0, 1675: 0, 1676: 0, 1677: 0, 1678: 0, 1679: 0, 1680: 0, 1681: 0, 1682: 0, 1683: 0, 1684: 0, 1685: 0, 1686: 0, 1687: 0, 1688: 0, 1689: 0, 1690: 0, 1691: 0, 1692: 0, 1693: 0, 1694: 0, 1695: 0, 1696: 0, 1697: 0, 1698: 0, 1699: 0, 1700: 0, 1701: 0, 1702: 0, 1703: 0, 1704: 0, 1705: 0, 1706: 1, 1707: 0, 1708: 0, 1709: 0, 1710: 0, 1711: 0, 1712: 0, 1713: 0, 1714: 0, 1715: 0, 1716: 0, 1717: 0, 1718: 0, 1719: 0, 1720: 0, 1721: 0, 1722: 0, 1723: 0, 1724: 0, 1725: 0, 1726: 0, 1727: 0, 1728: 0, 1729: 0, 1730: 0, 1731: 0, 1732: 0, 1733: 0, 1734: 0, 1735: 0, 1736: 0, 1737: 0, 1738: 0, 1739: 0, 1740: 0, 1741: 0, 1742: 0, 1743: 0, 1744: 0, 1745: 0, 1746: 0, 1747: 0, 1748: 0, 1749: 0, 1750: 0, 1751: 0, 1752: 0, 1753: 0, 1754: 0, 1755: 0, 1756: 0, 1757: 0, 1758: 0, 1759: 0, 1760: 0, 1761: 0, 1762: 0, 1763: 0, 1764: 0, 1765: 0, 1766: 0, 1767: 0, 1768: 0, 1769: 0, 1770: 0, 1771: 0, 1772: 0, 1773: 0, 1774: 0, 1775: 0, 1776: 0, 1777: 0, 1778: 0, 1779: 0, 1780: 0, 1781: 0, 1782: 0, 1783: 0, 1784: 0, 1785: 0, 1786: 0, 1787: 0, 1788: 0, 1789: 1, 1790: 0, 1791: 0, 1792: 0, 1793: 0, 1794: 0, 1795: 0, 1796: 0, 1797: 0, 1798: 0, 1799: 0, 1800: 0, 1801: 0, 1802: 0, 1803: 0, 1804: 0, 1805: 0, 1806: 0, 1807: 0, 1808: 0, 1809: 0, 1810: 0, 1811: 0, 1812: 0, 1813: 0, 1814: 1, 1815: 0, 1816: 0, 1817: 0, 1818: 0, 1819: 0, 1820: 0, 1821: 0, 1822: 0, 1823: 0, 1824: 0, 1825: 1, 1826: 0, 1827: 0, 1828: 0, 1829: 1, 1830: 0, 1831: 0, 1832: 0, 1833: 0, 1834: 0, 1835: 0, 1836: 0, 1837: 0, 1838: 0, 1839: 0, 1840: 0, 1841: 0, 1842: 0, 1843: 0, 1844: 0, 1845: 0, 1846: 0, 1847: 0, 1848: 0, 1849: 0, 1850: 0, 1851: 0, 1852: 0, 1853: 0, 1854: 0, 1855: 0, 1856: 0, 1857: 0, 1858: 0, 1859: 0, 1860: 0, 1861: 0, 1862: 0, 1863: 0, 1864: 0, 1865: 0, 1866: 0, 1867: 0, 1868: 0, 1869: 0, 1870: 0, 1871: 0, 1872: 0, 1873: 0, 1874: 0, 1875: 0, 1876: 0, 1877: 1, 1878: 0, 1879: 0, 1880: 0, 1881: 1, 1882: 0, 1883: 0, 1884: 0, 1885: 0, 1886: 0, 1887: 0, 1888: 0, 1889: 0, 1890: 0, 1891: 0, 1892: 0, 1893: 0, 1894: 0, 1895: 0, 1896: 0, 1897: 0, 1898: 0, 1899: 0, 1900: 0, 1901: 0, 1902: 0, 1903: 0, 1904: 0, 1905: 0, 1906: 0, 1907: 0, 1908: 0, 1909: 0, 1910: 0, 1911: 0, 1912: 0, 1913: 0, 1914: 0, 1915: 0, 1916: 0, 1917: 0, 1918: 0, 1919: 0, 1920: 0, 1921: 0, 1922: 0, 1923: 0, 1924: 0, 1925: 0, 1926: 0, 1927: 0, 1928: 0, 1929: 0, 1930: 2, 1931: 0, 1932: 0, 1933: 0, 1934: 0, 1935: 0, 1936: 0, 1937: 0, 1938: 0, 1939: 0, 1940: 0, 1941: 0, 1942: 0, 1943: 0, 1944: 1, 1945: 1, 1946: 0, 1947: 0, 1948: 0, 1949: 0, 1950: 0, 1951: 0, 1952: 0, 1953: 0, 1954: 0, 1955: 0, 1956: 0, 1957: 0, 1958: 0, 1959: 0, 1960: 0, 1961: 0, 1962: 0, 1963: 0, 1964: 0, 1965: 0, 1966: 0, 1967: 0, 1968: 0, 1969: 0, 1970: 0, 1971: 0, 1972: 0, 1973: 0, 1974: 0, 1975: 0, 1976: 0, 1977: 0, 1978: 0, 1979: 0, 1980: 0, 1981: 0, 1982: 0, 1983: 0, 1984: 0, 1985: 0, 1986: 0, 1987: 0, 1988: 0, 1989: 0, 1990: 0, 1991: 0, 1992: 0, 1993: 0, 1994: 0, 1995: 0, 1996: 0, 1997: 0, 1998: 0, 1999: 0, 2000: 0, 2001: 0, 2002: 0, 2003: 0, 2004: 0, 2005: 0, 2006: 0, 2007: 0, 2008: 0, 2009: 1, 2010: 0, 2011: 0, 2012: 0, 2013: 0, 2014: 0, 2015: 0, 2016: 0, 2017: 0, 2018: 0, 2019: 0, 2020: 0, 2021: 0, 2022: 0, 2023: 0, 2024: 0, 2025: 0, 2026: 0, 2027: 0, 2028: 0, 2029: 0, 2030: 0, 2031: 0, 2032: 0, 2033: 0, 2034: 0, 2035: 0, 2036: 0, 2037: 0, 2038: 0, 2039: 0, 2040: 0, 2041: 0, 2042: 0, 2043: 0, 2044: 0, 2045: 0, 2046: 0, 2047: 0, 2048: 0, 2049: 0, 2050: 0, 2051: 0, 2052: 0, 2053: 0, 2054: 0, 2055: 0, 2056: 0, 2057: 0, 2058: 0, 2059: 0, 2060: 0, 2061: 0, 2062: 0, 2063: 0, 2064: 0, 2065: 0, 2066: 0, 2067: 0, 2068: 0, 2069: 0, 2070: 0, 2071: 0, 2072: 0, 2073: 0, 2074: 0, 2075: 0, 2076: 0, 2077: 0, 2078: 0, 2079: 0, 2080: 0, 2081: 0, 2082: 0, 2083: 0, 2084: 0, 2085: 0, 2086: 0, 2087: 0, 2088: 0, 2089: 0, 2090: 0, 2091: 0, 2092: 0, 2093: 0, 2094: 0, 2095: 0, 2096: 0, 2097: 0, 2098: 0, 2099: 0, 2100: 0, 2101: 0, 2102: 0, 2103: 0, 2104: 0, 2105: 0, 2106: 0, 2107: 0, 2108: 0, 2109: 0, 2110: 0, 2111: 0, 2112: 0, 2113: 0, 2114: 0, 2115: 0, 2116: 0, 2117: 0, 2118: 0, 2119: 0, 2120: 0, 2121: 0, 2122: 0, 2123: 0, 2124: 0, 2125: 1, 2126: 0, 2127: 0, 2128: 0, 2129: 0, 2130: 0, 2131: 0, 2132: 0, 2133: 0, 2134: 0, 2135: 0, 2136: 0, 2137: 0, 2138: 0, 2139: 0, 2140: 0, 2141: 0, 2142: 0, 2143: 0, 2144: 0, 2145: 0, 2146: 0, 2147: 0, 2148: 0, 2149: 0, 2150: 0, 2151: 0, 2152: 0, 2153: 0, 2154: 0, 2155: 0, 2156: 0, 2157: 0, 2158: 0, 2159: 0, 2160: 0, 2161: 0, 2162: 0, 2163: 0, 2164: 0, 2165: 0, 2166: 0, 2167: 0, 2168: 0, 2169: 0, 2170: 0, 2171: 0, 2172: 0, 2173: 0, 2174: 0, 2175: 0, 2176: 0, 2177: 0, 2178: 0, 2179: 0, 2180: 0, 2181: 0, 2182: 0, 2183: 0, 2184: 0, 2185: 0, 2186: 0, 2187: 0, 2188: 0, 2189: 0, 2190: 0, 2191: 0, 2192: 0, 2193: 0, 2194: 0, 2195: 0, 2196: 0, 2197: 0, 2198: 0, 2199: 0, 2200: 0, 2201: 0, 2202: 0, 2203: 0, 2204: 0, 2205: 0, 2206: 0, 2207: 0, 2208: 0, 2209: 0, 2210: 0, 2211: 0, 2212: 0, 2213: 0, 2214: 0, 2215: 0, 2216: 0, 2217: 0, 2218: 0, 2219: 0, 2220: 0, 2221: 0, 2222: 0, 2223: 0, 2224: 0, 2225: 0, 2226: 0, 2227: 0, 2228: 0, 2229: 0, 2230: 0, 2231: 0, 2232: 0, 2233: 0, 2234: 0, 2235: 0, 2236: 0, 2237: 0, 2238: 0, 2239: 0, 2240: 0, 2241: 0, 2242: 0, 2243: 0, 2244: 0, 2245: 0, 2246: 0, 2247: 0, 2248: 2, 2249: 0, 2250: 0, 2251: 0, 2252: 0, 2253: 0, 2254: 0, 2255: 0, 2256: 0, 2257: 0, 2258: 0, 2259: 0, 2260: 0, 2261: 0, 2262: 0, 2263: 0, 2264: 0, 2265: 0, 2266: 0, 2267: 0, 2268: 0, 2269: 0, 2270: 0, 2271: 0, 2272: 0, 2273: 0, 2274: 0, 2275: 0, 2276: 124}\n",
      "2277 unique labels are used\n",
      "1994 unique labels never used\n",
      "146 unique labels used once\n",
      "59 unique labels used twice\n"
     ]
    }
   ],
   "source": [
    "print(f\"total number of steps is:\", step_count)\n",
    "print(f\"highest frequency label is {max_freq_index} and occurs {max} times\")\n",
    "print(f\"final label used is {len(label_count)-1}\")\n",
    "print(label_count)\n",
    "print(len(label_count),\"unique labels are used\")\n",
    "print(labels_never_used,\"unique labels never used\")\n",
    "print(labels_used_once, \"unique labels used once\")\n",
    "print(labels_used_twice, \"unique labels used twice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on label 0\n",
      "on label 200\n",
      "on label 400\n",
      "{0: (0.0, 2), 1: (0.7272727272727273, 22), 3: (0.0, 1), 4: (0.0, 3), 5: (0.0, 1), 6: (0.5, 2), 8: (0.5263157894736842, 19), 10: (0.5, 2), 11: (0.0, 1), 16: (0.25, 4), 17: (0.0, 1), 18: (0.5, 2), 22: (0.0, 1), 23: (0.0, 1), 25: (0.3333333333333333, 3), 26: (0.0, 1), 30: (0.0, 1), 31: (0.0, 1), 35: (0.0, 2), 40: (0.0, 1), 41: (0.2, 10), 42: (0.0, 1), 44: (0.2, 15), 45: (0.0, 7), 47: (0.0, 1), 48: (0.25, 4), 49: (0.0, 3), 52: (0.0, 2), 53: (0.0, 6), 54: (0.0, 2), 55: (0.3333333333333333, 3), 58: (0.42857142857142855, 7), 59: (0.0, 2), 62: (0.125, 8), 64: (0.0, 2), 66: (0.0, 1), 71: (0.0, 1), 72: (1.0, 1), 74: (0.0, 3), 75: (0.6666666666666666, 3), 80: (0.0, 2), 81: (1.0, 2), 83: (0.5357142857142857, 28), 84: (0.0, 2), 85: (0.0, 1), 86: (0.42857142857142855, 7), 87: (0.42857142857142855, 7), 88: (0.0, 1), 89: (0.0, 1), 90: (1.0, 1), 91: (0.0, 1), 92: (0.0, 1), 94: (0.0, 1), 97: (0.0, 6), 98: (0.0, 2), 100: (0.0, 1), 101: (0.6111111111111112, 18), 104: (0.6, 5), 106: (1.0, 1), 110: (0.0, 2), 111: (0.0, 2), 119: (0.0, 1), 123: (0.0, 1), 126: (0.0, 1), 127: (0.0, 3), 130: (1.0, 3), 134: (0.4, 5), 135: (0.0, 1), 136: (0.0, 2), 137: (0.0, 1), 140: (0.0, 1), 142: (0.0, 1), 144: (0.0, 1), 151: (0.0, 1), 157: (0.0, 2), 158: (0.0, 2), 159: (0.0, 1), 167: (1.0, 1), 169: (0.0, 2), 170: (0.0, 2), 171: (0.3333333333333333, 3), 172: (1.0, 1), 174: (1.0, 2), 184: (0.0, 1), 188: (0.0, 2), 189: (0.6, 5), 192: (0.0, 2), 193: (0.0, 4), 198: (0.3333333333333333, 3), 202: (0.0, 2), 203: (0.0, 1), 204: (0.0, 1), 208: (0.0, 1), 210: (0.0, 1), 228: (0.0, 1), 230: (0.0, 1), 231: (0.0, 5), 232: (0.3333333333333333, 3), 233: (0.0, 3), 237: (0.0, 1), 242: (0.5, 2), 250: (1.0, 1), 258: (0.0, 1), 267: (0.5, 2), 268: (0.0, 1), 273: (0.0, 1), 280: (0.0, 1), 282: (1.0, 4), 283: (0.3333333333333333, 3), 284: (0.0, 1), 328: (0.0, 1), 359: (0.0, 1), 380: (0.5, 4), 381: (0.5, 2), 387: (0.0, 2), 390: (0.0, 1), 391: (0.0, 1), 396: (0.0, 2), 397: (0.0, 5), 398: (1.0, 2), 399: (0.0, 1), 401: (0.25, 4), 402: (0.0, 2), 404: (0.0, 2), 406: (0.0, 1), 407: (0.5, 10), 424: (1.0, 3), 437: (0.0, 2), 445: (0.0, 1), 497: (1.0, 1), 554: (0.6869565217391305, 115)}\n",
      "if everything here is correct, our conc node val_acc should be: 0.402\n"
     ]
    }
   ],
   "source": [
    "# make acc dict for val conclusion nodes\n",
    "\n",
    "conc_label_acc_dict = {}\n",
    "\n",
    "def get_conc_gin_label_acc(i):\n",
    "    correct_pred_i = 0\n",
    "    total_num_i = 0\n",
    "\n",
    "    for graph in val_dataset:\n",
    "        get_gin_predict = gin_trained(graph.x.to(device),graph.edge_index.long().to(device))\n",
    "        get_conc_predict = np.argmax(get_gin_predict[-1].detach().cpu().numpy(),axis=0)\n",
    "        if graph.y[-1].item() == i:\n",
    "            total_num_i += 1\n",
    "            if get_conc_predict == i:\n",
    "                correct_pred_i += 1\n",
    "    if total_num_i == 0:\n",
    "        return 0,0,0\n",
    "    return (correct_pred_i / total_num_i), total_num_i, correct_pred_i\n",
    "\n",
    "\n",
    "total_conc_labels = 0\n",
    "total_conc_correct_preds = 0\n",
    "for i in range(hidden_conc_pf_data.num_classes):\n",
    "    if i % 200 ==0:\n",
    "        print(\"on label\",i)\n",
    "    try:\n",
    "        container = get_conc_gin_label_acc(i)\n",
    "        if container[1] != 0:\n",
    "            conc_label_acc_dict[i] = (container[0],container[1])  # pair of accuracy and label count for each label\n",
    "        total_conc_labels += container[1]\n",
    "        total_conc_correct_preds += container[2]\n",
    "    except Exception as e:\n",
    "        print(e,get_conc_gin_label_acc(i))\n",
    "\n",
    "print(conc_label_acc_dict)\n",
    "print(\"if everything here is correct, our conc node val_acc should be:\",total_conc_correct_preds/total_conc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: (0.0, 1),\n",
       " 5: (0.0, 1),\n",
       " 11: (0.0, 1),\n",
       " 17: (0.0, 1),\n",
       " 22: (0.0, 1),\n",
       " 23: (0.0, 1),\n",
       " 26: (0.0, 1),\n",
       " 30: (0.0, 1),\n",
       " 31: (0.0, 1),\n",
       " 40: (0.0, 1),\n",
       " 42: (0.0, 1),\n",
       " 47: (0.0, 1),\n",
       " 66: (0.0, 1),\n",
       " 71: (0.0, 1),\n",
       " 85: (0.0, 1),\n",
       " 88: (0.0, 1),\n",
       " 89: (0.0, 1),\n",
       " 91: (0.0, 1),\n",
       " 92: (0.0, 1),\n",
       " 94: (0.0, 1),\n",
       " 100: (0.0, 1),\n",
       " 119: (0.0, 1),\n",
       " 123: (0.0, 1),\n",
       " 126: (0.0, 1),\n",
       " 135: (0.0, 1),\n",
       " 137: (0.0, 1),\n",
       " 140: (0.0, 1),\n",
       " 142: (0.0, 1),\n",
       " 144: (0.0, 1),\n",
       " 151: (0.0, 1),\n",
       " 159: (0.0, 1),\n",
       " 184: (0.0, 1),\n",
       " 203: (0.0, 1),\n",
       " 204: (0.0, 1),\n",
       " 208: (0.0, 1),\n",
       " 210: (0.0, 1),\n",
       " 228: (0.0, 1),\n",
       " 230: (0.0, 1),\n",
       " 237: (0.0, 1),\n",
       " 258: (0.0, 1),\n",
       " 268: (0.0, 1),\n",
       " 273: (0.0, 1),\n",
       " 280: (0.0, 1),\n",
       " 284: (0.0, 1),\n",
       " 328: (0.0, 1),\n",
       " 359: (0.0, 1),\n",
       " 390: (0.0, 1),\n",
       " 391: (0.0, 1),\n",
       " 399: (0.0, 1),\n",
       " 406: (0.0, 1),\n",
       " 445: (0.0, 1),\n",
       " 0: (0.0, 2),\n",
       " 35: (0.0, 2),\n",
       " 52: (0.0, 2),\n",
       " 54: (0.0, 2),\n",
       " 59: (0.0, 2),\n",
       " 64: (0.0, 2),\n",
       " 80: (0.0, 2),\n",
       " 84: (0.0, 2),\n",
       " 98: (0.0, 2),\n",
       " 110: (0.0, 2),\n",
       " 111: (0.0, 2),\n",
       " 136: (0.0, 2),\n",
       " 157: (0.0, 2),\n",
       " 158: (0.0, 2),\n",
       " 169: (0.0, 2),\n",
       " 170: (0.0, 2),\n",
       " 188: (0.0, 2),\n",
       " 192: (0.0, 2),\n",
       " 202: (0.0, 2),\n",
       " 387: (0.0, 2),\n",
       " 396: (0.0, 2),\n",
       " 402: (0.0, 2),\n",
       " 404: (0.0, 2),\n",
       " 437: (0.0, 2),\n",
       " 4: (0.0, 3),\n",
       " 49: (0.0, 3),\n",
       " 74: (0.0, 3),\n",
       " 127: (0.0, 3),\n",
       " 233: (0.0, 3),\n",
       " 193: (0.0, 4),\n",
       " 231: (0.0, 5),\n",
       " 397: (0.0, 5),\n",
       " 53: (0.0, 6),\n",
       " 97: (0.0, 6),\n",
       " 45: (0.0, 7),\n",
       " 62: (0.125, 8),\n",
       " 41: (0.2, 10),\n",
       " 44: (0.2, 15),\n",
       " 16: (0.25, 4),\n",
       " 48: (0.25, 4),\n",
       " 401: (0.25, 4),\n",
       " 25: (0.3333333333333333, 3),\n",
       " 55: (0.3333333333333333, 3),\n",
       " 171: (0.3333333333333333, 3),\n",
       " 198: (0.3333333333333333, 3),\n",
       " 232: (0.3333333333333333, 3),\n",
       " 283: (0.3333333333333333, 3),\n",
       " 134: (0.4, 5),\n",
       " 58: (0.42857142857142855, 7),\n",
       " 86: (0.42857142857142855, 7),\n",
       " 87: (0.42857142857142855, 7),\n",
       " 6: (0.5, 2),\n",
       " 10: (0.5, 2),\n",
       " 18: (0.5, 2),\n",
       " 242: (0.5, 2),\n",
       " 267: (0.5, 2),\n",
       " 381: (0.5, 2),\n",
       " 380: (0.5, 4),\n",
       " 407: (0.5, 10),\n",
       " 8: (0.5263157894736842, 19),\n",
       " 83: (0.5357142857142857, 28),\n",
       " 104: (0.6, 5),\n",
       " 189: (0.6, 5),\n",
       " 101: (0.6111111111111112, 18),\n",
       " 75: (0.6666666666666666, 3),\n",
       " 554: (0.6869565217391305, 115),\n",
       " 1: (0.7272727272727273, 22),\n",
       " 72: (1.0, 1),\n",
       " 90: (1.0, 1),\n",
       " 106: (1.0, 1),\n",
       " 167: (1.0, 1),\n",
       " 172: (1.0, 1),\n",
       " 250: (1.0, 1),\n",
       " 497: (1.0, 1),\n",
       " 81: (1.0, 2),\n",
       " 174: (1.0, 2),\n",
       " 398: (1.0, 2),\n",
       " 130: (1.0, 3),\n",
       " 424: (1.0, 3),\n",
       " 282: (1.0, 4)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort the above dict by accuracy\n",
    "\n",
    "sorted_conc_label_acc_dict = {k: v for k, v in sorted(conc_label_acc_dict.items(), key=lambda item: item[1])}\n",
    "sorted_conc_label_acc_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
