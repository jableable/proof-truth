{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from our_dataset import ProofDataset\n",
    "from use_dataset import ProofDataset, HiddenConcProofDataset\n",
    "#from our_dataset_with_labels import ProofDatasetWithLabels\n",
    "from statement_embedding import emb_to_stmt, num_to_label\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import torch\n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dphil\\anaconda3\\envs\\erdos_may_2024\\lib\\site-packages\\torch_geometric\\data\\dataset.py:238: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  if osp.exists(f) and torch.load(f) != _repr(self.pre_transform):\n",
      "C:\\Users\\dphil\\anaconda3\\envs\\erdos_may_2024\\lib\\site-packages\\torch_geometric\\data\\dataset.py:246: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  if osp.exists(f) and torch.load(f) != _repr(self.pre_filter):\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_limit = 5000    # desired number of graphs to work with\n",
    "vocab_size = 1598   # number of characters in our vocabulary\n",
    "label_size = file_limit  # number of labels in dataset; all labels is 45332; defaults to file_limit in pf_data\n",
    "\n",
    "pf_data = pf_data = ProofDataset(root=\"data/\",read_name=\"5000_relabeled_data_at_least_5.json\" , write_name=\"10000_relabeled_data_at_least_5_w_stmts.pt\" ,file_limit=file_limit)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gets the hypotheses for a given step\n",
    "def hypos(cur_graph,step_num):\n",
    "    req_stmt_num = []\n",
    "    for idx, x in enumerate(cur_graph.edge_index[1]):\n",
    "        if step_num == x:\n",
    "            req_stmt_num.append(int(cur_graph.edge_index[0][idx]))\n",
    "    return req_stmt_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constructs a random walk up the graph of length less than 15\n",
    "def graphwalks(cur_graph):\n",
    "    walk=[cur_graph.num_nodes-1]\n",
    "    start=cur_graph.num_nodes-1\n",
    "    for i in range(15):\n",
    "        hyp=hypos(cur_graph,start)\n",
    "        if len(hyp)!=0:\n",
    "            start=np.random.choice(hyp)\n",
    "            walk.append(start)\n",
    "    return walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 0]"
      ]
     },
     "execution_count": 752,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphwalks(cur_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a \"sentence\" based on a random graph walk. In hindsight, they should have been called paragraphs.\n",
    "def sentence(cur_graph):\n",
    "    walk=graphwalks(cur_graph)\n",
    "    sent=[]\n",
    "    for step_num in reversed(walk):\n",
    "        cur_step = cur_graph.x[step_num]\n",
    "        cur_step_vec = cur_step*vocab_size\n",
    "        cur_step_vec = cur_step_vec[cur_step_vec.nonzero()] # remove trailing zeros\n",
    "        cur_step_vec = cur_step_vec.reshape(-1)\n",
    "        sent.extend(cur_step_vec.to(int).tolist())\n",
    "        #The special character -1, acts as a sentence break, telling the model that a statement has ended\n",
    "        sent.append(-1)\n",
    "    return sent         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permutes the three most common symbolic variables. Necessary to prevent the model from trivializing\n",
    "def perm(list,i):\n",
    "    if i % 3==1:\n",
    "        for j in range(len(list)):\n",
    "            if list[j] == 114:\n",
    "                list[j] = 1284\n",
    "            elif list[j] == 1284:\n",
    "                list[j] = 1181\n",
    "            elif list[j] == 1181:\n",
    "                list[j] = 114\n",
    "    if i % 3==2:\n",
    "        for j in range(len(list)):\n",
    "            if list[j] == 114:\n",
    "                list[j] = 1181\n",
    "            elif list[j] == 1284:\n",
    "                list[j] = 114\n",
    "            elif list[j] == 1181:\n",
    "                list[j] = 1284\n",
    "    return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates 6 sentences for each graph up to 1058 (The end of the section on propostional logic).\n",
    "sentences=[]\n",
    "for i in range(2,1058):\n",
    "    graph=pf_data.get(i)\n",
    "    for j in range(6):\n",
    "        if len(graph.edge_index.size()) != 1:\n",
    "            sentences.append(perm(sentence(graph),i))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constructs the dictionary of \"words.\n",
    "dict={857}\n",
    "for sent in sentences:\n",
    "    for word in sent:\n",
    "        dict.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary=list(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "length=len(dictionary)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turns a word into a vector.\n",
    "def tovec(word):\n",
    "    vec=np.zeros(length)\n",
    "    for i in range(length):\n",
    "        if word==dictionary[i]:\n",
    "            vec[i]=1\n",
    "    return vec\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The length of the skipgrams used. The model will have difficulty with statements more than half the length here.\n",
    "depth=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converts our sentences to vectors\n",
    "vecsents=[]\n",
    "for sent in sentences:\n",
    "    for i in range(3):\n",
    "        vecsent=[]\n",
    "        start=np.random.randint(1,len(sent))\n",
    "        truncsent=sent[0:start]\n",
    "        if len(truncsent)<depth:\n",
    "            for i in range(depth-len(truncsent)):\n",
    "                vecsent.append(np.zeros(len(dictionary)))\n",
    "        for word in truncsent:\n",
    "            vecsent.append(tovec(word))\n",
    "            if len(vecsent)==depth:\n",
    "                break\n",
    "    vecsents.append(np.array(vecsent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatens = torch.tensor(vecsents, dtype=torch.float32).reshape(len(vecsents), depth, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6252, 39, 22])\n",
      "torch.Size([6252, 22])\n"
     ]
    }
   ],
   "source": [
    "#Constructs the skip-grams as tensors\n",
    "dataX=datatens[:,0:depth-1,:]\n",
    "print(dataX.shape)\n",
    "datay=datatens[:,depth-1,:]\n",
    "print(datay.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines the model\n",
    "hidden_layer=250\n",
    "class predModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=length, hidden_size=hidden_layer, num_layers=1, batch_first=True)\n",
    "        #self.trans = nn.Transformer(d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=70)\n",
    "        self.dropout = nn.Dropout(0.05)\n",
    "        self.linear = nn.Linear(hidden_layer, length)\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        # take only the last output\n",
    "        x = x[:,-1]\n",
    "        # produce output\n",
    "        x = self.linear(self.dropout(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Cross-entropy: 14179.5547\n",
      "Epoch 1: Cross-entropy: 12063.4111\n",
      "Epoch 2: Cross-entropy: 8721.8623\n",
      "Epoch 3: Cross-entropy: 7737.4741\n",
      "Epoch 4: Cross-entropy: 6972.9438\n",
      "Epoch 5: Cross-entropy: 6717.7671\n",
      "Epoch 6: Cross-entropy: 6681.5698\n",
      "Epoch 7: Cross-entropy: 6267.8379\n",
      "Epoch 8: Cross-entropy: 6144.9507\n",
      "Epoch 9: Cross-entropy: 5996.0400\n",
      "Epoch 10: Cross-entropy: 5838.7266\n",
      "Epoch 11: Cross-entropy: 5647.4277\n",
      "Epoch 12: Cross-entropy: 5527.3584\n",
      "Epoch 13: Cross-entropy: 5332.2163\n",
      "Epoch 14: Cross-entropy: 5276.4937\n",
      "Epoch 15: Cross-entropy: 5108.1929\n",
      "Epoch 16: Cross-entropy: 5066.8408\n",
      "Epoch 17: Cross-entropy: 4996.2871\n",
      "Epoch 18: Cross-entropy: 5211.0293\n",
      "Epoch 19: Cross-entropy: 4638.4595\n",
      "Epoch 20: Cross-entropy: 4462.8677\n",
      "Epoch 21: Cross-entropy: 4338.2944\n",
      "Epoch 22: Cross-entropy: 4304.8604\n",
      "Epoch 23: Cross-entropy: 4193.3486\n",
      "Epoch 24: Cross-entropy: 4020.1414\n",
      "Epoch 25: Cross-entropy: 3921.9746\n",
      "Epoch 26: Cross-entropy: 3695.0845\n",
      "Epoch 27: Cross-entropy: 3687.5190\n",
      "Epoch 28: Cross-entropy: 3704.0789\n",
      "Epoch 29: Cross-entropy: 3571.3091\n",
      "Epoch 30: Cross-entropy: 3304.6787\n",
      "Epoch 31: Cross-entropy: 3206.0452\n",
      "Epoch 32: Cross-entropy: 3016.2520\n",
      "Epoch 33: Cross-entropy: 2898.6450\n",
      "Epoch 34: Cross-entropy: 2887.7764\n",
      "Epoch 35: Cross-entropy: 2708.4414\n",
      "Epoch 36: Cross-entropy: 2666.2791\n",
      "Epoch 37: Cross-entropy: 2547.3010\n",
      "Epoch 38: Cross-entropy: 2466.9639\n",
      "Epoch 39: Cross-entropy: 2466.9578\n"
     ]
    }
   ],
   "source": [
    "#Trains the model\n",
    "n_epochs = 40\n",
    "batch_size = 100\n",
    "model = predModel()\n",
    " \n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "loader = data.DataLoader(data.TensorDataset(dataX, datay), shuffle=True, batch_size=batch_size)\n",
    " \n",
    "best_model = None\n",
    "best_loss = np.inf\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in loader:\n",
    "        y_pred = model(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            y_pred = model(X_batch)\n",
    "            loss += loss_fn(y_pred, y_batch)\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_model = model.state_dict()\n",
    "        print(\"Epoch %d: Cross-entropy: %.4f\" % (epoch, loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates an example for predicting the n-1 step of a graph. Most of this code is redoing everything we did to create the training data.\n",
    "def createex(graph_num):\n",
    "    sentences=[]\n",
    "    cur_graph = pf_data.get(graph_num)\n",
    "    for i in range(10):\n",
    "        walk=[]\n",
    "        sent=[]\n",
    "        step=cur_graph.num_nodes-2\n",
    "        for i in range(15):\n",
    "            hyp=hypos(cur_graph,step)\n",
    "            if len(hyp)!=0:\n",
    "                step=np.random.choice(hyp)\n",
    "                walk.append(step)\n",
    "        for step_num in reversed(walk):\n",
    "            cur_step = cur_graph.x[step_num]\n",
    "            cur_step_vec = cur_step*vocab_size\n",
    "            cur_step_vec = cur_step_vec[cur_step_vec.nonzero()] # remove trailing zeros\n",
    "            cur_step_vec = cur_step_vec.reshape(-1)\n",
    "            sent.extend(cur_step_vec.to(int).tolist())\n",
    "            sent.append(-1)\n",
    "        sentences.append(sent)\n",
    "    vecsents=[]\n",
    "    for sent in sentences:\n",
    "        vecsent=[]\n",
    "        if len(sent)<depth-1:\n",
    "            for i in range(depth-1-len(sent)):\n",
    "                vecsent.append(np.zeros(len(dictionary)))\n",
    "        for word in sent[-(depth-1):]:\n",
    "            vecsent.append(tovec(word))\n",
    "        vecsents.append(vecsent)\n",
    "    return vecsents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicts the second to last step based on the example given\n",
    "def predstat(example):\n",
    "    predsentence=[]\n",
    "    predletter=0\n",
    "    i=0\n",
    "    while predletter!=-1:\n",
    "        exarray=np.array(example)\n",
    "        extensor=torch.tensor(exarray[:,-(depth-1):,:], dtype=torch.float32).reshape(10, (depth-1),length)\n",
    "        predletterraw = torch.sum(model(extensor), dim=0)\n",
    "        predletterraw=nn.functional.normalize(predletterraw, dim=0)\n",
    "        #print(predletterraw)\n",
    "        predletter=dictionary[torch.argmax(predletterraw)]\n",
    "        predsentence.append(predletter)\n",
    "        for sent in example:\n",
    "            sent.append(tovec(predletter));\n",
    "    return predsentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The actual statement is: ph -> ( ps -> ( ch -> th ) ) )\n",
      "The predicted statement is: ( ( ph /\\ ps ) /\\ ch ) -> th ) -> ( ph -> ps ) )\n"
     ]
    }
   ],
   "source": [
    "exnum=450\n",
    "cur_graph = pf_data.get(exnum)\n",
    "cur_step_vec = cur_graph.x[cur_graph.num_nodes-2]*vocab_size\n",
    "cur_step_vec = cur_step_vec[cur_step_vec.nonzero()] # remove trailing zeros\n",
    "cur_step_vec = cur_step_vec.reshape(-1)\n",
    "cur_step_vec = cur_step_vec.to(int).tolist()\n",
    "print(\"The actual statement is:\",emb_to_stmt(cur_step_vec) ) \n",
    "print(\"The predicted statement is:\",emb_to_stmt(predstat(createex(exnum))[:-1]) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is difficult to quantify exactly how accurate this model is, since it typically cannot distinguish between the variables ph, ps, ch, etc.\n",
    "The results have been typically \"close\" to the true answer, but, characteristic of deep learning models, it tends to switch similar symbols. Typically for shorter statements, the outputs have been genuine logical statements. However, for very long statements, the limited skip-gram length will lead to errors. (For technical reasons, the above output does not display opening parentheses.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The actual statement is: ph -> ( ch <-> ps ) )\n",
      "The predicted statement is: ph -> ( ps -> ch ) )\n"
     ]
    }
   ],
   "source": [
    "exnum=220\n",
    "cur_graph = pf_data.get(exnum)\n",
    "cur_step_vec = cur_graph.x[cur_graph.num_nodes-2]*vocab_size\n",
    "cur_step_vec = cur_step_vec[cur_step_vec.nonzero()] # remove trailing zeros\n",
    "cur_step_vec = cur_step_vec.reshape(-1)\n",
    "cur_step_vec = cur_step_vec.to(int).tolist()\n",
    "print(\"The actual statement is:\",emb_to_stmt(cur_step_vec) ) \n",
    "print(\"The predicted statement is:\",emb_to_stmt(predstat(createex(exnum))[:-1]) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The actual statement is: ( ( th /\\ ph ) /\\ ps ) -> ch )\n",
      "The predicted statement is: ( ph /\\ ( ps /\\ ch ) ) -> th )\n"
     ]
    }
   ],
   "source": [
    "exnum=750\n",
    "cur_graph = pf_data.get(exnum)\n",
    "cur_step_vec = cur_graph.x[cur_graph.num_nodes-2]*vocab_size\n",
    "cur_step_vec = cur_step_vec[cur_step_vec.nonzero()] # remove trailing zeros\n",
    "cur_step_vec = cur_step_vec.reshape(-1)\n",
    "cur_step_vec = cur_step_vec.to(int).tolist()\n",
    "print(\"The actual statement is:\",emb_to_stmt(cur_step_vec) ) \n",
    "print(\"The predicted statement is:\",emb_to_stmt(predstat(createex(exnum))[:-1]) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The actual statement is: th <-> ( ps /\\ ch ) )\n",
      "The predicted statement is: ( ph -> ps ) -> ( ph -> ( ph \\/ ps ) ) )\n"
     ]
    }
   ],
   "source": [
    "#An example that behaves especially badly.\n",
    "exnum=582\n",
    "cur_graph = pf_data.get(exnum)\n",
    "cur_step_vec = cur_graph.x[cur_graph.num_nodes-2]*vocab_size\n",
    "cur_step_vec = cur_step_vec[cur_step_vec.nonzero()] # remove trailing zeros\n",
    "cur_step_vec = cur_step_vec.reshape(-1)\n",
    "cur_step_vec = cur_step_vec.to(int).tolist()\n",
    "print(\"The actual statement is:\",emb_to_stmt(cur_step_vec) ) \n",
    "print(\"The predicted statement is:\",emb_to_stmt(predstat(createex(exnum))[:-1]) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
