{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from our_dataset import ProofDataset\n",
    "from our_dataset_with_labels import ProofDatasetWithLabels\n",
    "from statement_embedding import emb_to_stmt, num_to_label\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import torch\n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dphil\\anaconda3\\envs\\erdos_may_2024\\lib\\site-packages\\torch_geometric\\data\\dataset.py:238: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  if osp.exists(f) and torch.load(f) != _repr(self.pre_transform):\n",
      "C:\\Users\\dphil\\anaconda3\\envs\\erdos_may_2024\\lib\\site-packages\\torch_geometric\\data\\dataset.py:246: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  if osp.exists(f) and torch.load(f) != _repr(self.pre_filter):\n",
      "Processing...\n",
      "C:\\Users\\dphil\\OneDrive\\Documents\\proof-truth\\proof-truth\\Model\\our_dataset_with_labels.py:59: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  x_emb = torch.Tensor([emb[1:]])\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# WARNING!!!!\n",
    "# running ProofDataset(root=\"data/\", file_limit=None) currently has a high initial cost\n",
    "# it requires ~63 GB of space, and it took my computer about 2 hours to run\n",
    "# however, after an initial run, the dataset doesn't need to be created again\n",
    "# secondary runs merely verify that all files are in place; this takes my computer about 15s\n",
    "# WARNING!!!!\n",
    "\n",
    "# to avoid previous warning, use file_limit=8000 to only load and verify the first 8000 graphs (<4GB)\n",
    "# if entire graph dataset is desired, use file_limit=None\n",
    "file_limit = 20000    # desired number of graphs to work with\n",
    "vocab_size = 1598   # number of characters in our vocabulary\n",
    "label_size = file_limit  # number of labels in dataset; all labels is 45332; defaults to file_limit in pf_data\n",
    "\n",
    "pf_data = ProofDatasetWithLabels(root=\"data/\",file_limit=file_limit, label_size=file_limit )  # normalize with transform = NormalizeFeatures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is used to get the label of a statement from tag_proof.csv\n",
    "\n",
    "def get_thm_lbl(num):\n",
    "    with open(\"../Assets/tag_proof.csv\",\"r\") as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for idx, row in enumerate(reader):\n",
    "            if num == idx:\n",
    "                return row['tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dphil\\OneDrive\\Documents\\proof-truth\\proof-truth\\Model\\our_dataset_with_labels.py:94: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(osp.join(self.processed_dir, f'data_{idx}.pt'))\n"
     ]
    }
   ],
   "source": [
    "# let's explore graph 2, i.e. mp2, as our current graph at proof step 3\n",
    "graph_num = 2\n",
    "step_num = 3\n",
    "graph_lbl = get_thm_lbl(graph_num)\n",
    "cur_graph = pf_data.get(graph_num)\n",
    "cur_step = cur_graph.x[step_num]\n",
    "\n",
    "# let's get the normalized statement feature vector at step 3 in our proof\n",
    "cur_step_vec_norm = cur_step\n",
    "cur_step_vec_norm = cur_step_vec_norm[cur_step_vec_norm.nonzero()]  # remove trailing zeros\n",
    "cur_step_vec_norm = cur_step_vec_norm.reshape(-1)\n",
    "cur_step_vec_norm = cur_step_vec_norm.tolist()\n",
    "\n",
    "# unnormalized version of above\n",
    "cur_step_vec = cur_step*vocab_size\n",
    "cur_step_vec = cur_step_vec[cur_step_vec.nonzero()] # remove trailing zeros\n",
    "cur_step_vec = cur_step_vec.reshape(-1)\n",
    "cur_step_vec = cur_step_vec.to(int).tolist()\n",
    "\n",
    "# convert feature vector back to statement\n",
    "#stmt_list = cur_step_vec.copy()\n",
    "#stmt_list.insert(0,0)\n",
    "\n",
    "# let's get the label of our current step\n",
    "cur_label = cur_graph.y[step_num]*label_size\n",
    "cur_label = cur_label.to(int).item()\n",
    "\n",
    "# get statement dependencies of current step \n",
    "req_stmt_num = []\n",
    "req_stmt = []\n",
    "for idx, x in enumerate(cur_graph.edge_index[1]):\n",
    "    if step_num == x:\n",
    "        req_stmt_num.append(cur_graph.edge_index[0][idx].to(int))\n",
    "for num in req_stmt_num:\n",
    "    dependency = cur_graph.x[num]*vocab_size\n",
    "    dependency = dependency.to(int).tolist()\n",
    "    dependency.insert(0,0)\n",
    "    dependency = emb_to_stmt(dependency)\n",
    "    req_stmt.append(dependency)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gets the hypothesis for a given step\n",
    "def hypos(cur_graph,step_num):\n",
    "    req_stmt_num = []\n",
    "    for idx, x in enumerate(cur_graph.edge_index[1]):\n",
    "        if step_num == x:\n",
    "            req_stmt_num.append(int(cur_graph.edge_index[0][idx]))\n",
    "    return req_stmt_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constructs a random walk up the graph of length less than 15\n",
    "def graphwalks(cur_graph):\n",
    "    walk=[cur_graph.num_nodes-1]\n",
    "    start=cur_graph.num_nodes-1\n",
    "    for i in range(15):\n",
    "        hyp=hypos(cur_graph,start)\n",
    "        if len(hyp)!=0:\n",
    "            start=np.random.choice(hyp)\n",
    "            walk.append(start)\n",
    "    return walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 3, 1]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphwalks(cur_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a paragraph based on a random graph walk\n",
    "def sentence(cur_graph):\n",
    "    walk=graphwalks(cur_graph)\n",
    "    sent=[]\n",
    "    for step_num in reversed(walk):\n",
    "        cur_step = cur_graph.x[step_num]\n",
    "        cur_step_vec = cur_step*vocab_size\n",
    "        cur_step_vec = cur_step_vec[cur_step_vec.nonzero()] # remove trailing zeros\n",
    "        cur_step_vec = cur_step_vec.reshape(-1)\n",
    "        sent.extend(cur_step_vec.to(int).tolist())\n",
    "        #The special character -1, acts as a sentence break, telling the model that a statement has ended\n",
    "        sent.append(-1)\n",
    "    return sent         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dphil\\OneDrive\\Documents\\proof-truth\\proof-truth\\Model\\our_dataset_with_labels.py:94: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(osp.join(self.processed_dir, f'data_{idx}.pt'))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[301], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m):\n\u001b[0;32m      3\u001b[0m     graph\u001b[38;5;241m=\u001b[39mpf_data\u001b[38;5;241m.\u001b[39mget(graph_num)\n\u001b[1;32m----> 4\u001b[0m     sentences\u001b[38;5;241m.\u001b[39mappend(\u001b[43msentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[300], line 3\u001b[0m, in \u001b[0;36msentence\u001b[1;34m(cur_graph)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentence\u001b[39m(cur_graph):\n\u001b[1;32m----> 3\u001b[0m     walk\u001b[38;5;241m=\u001b[39m\u001b[43mgraphwalks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcur_graph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     sent\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(walk):\n",
      "Cell \u001b[1;32mIn[298], line 6\u001b[0m, in \u001b[0;36mgraphwalks\u001b[1;34m(cur_graph)\u001b[0m\n\u001b[0;32m      4\u001b[0m start\u001b[38;5;241m=\u001b[39mcur_graph\u001b[38;5;241m.\u001b[39mnum_nodes\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m15\u001b[39m):\n\u001b[1;32m----> 6\u001b[0m     hyp\u001b[38;5;241m=\u001b[39m\u001b[43mhypos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcur_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(hyp)\u001b[38;5;241m!=\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m      8\u001b[0m         start\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(hyp)\n",
      "Cell \u001b[1;32mIn[296], line 3\u001b[0m, in \u001b[0;36mhypos\u001b[1;34m(cur_graph, step_num)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhypos\u001b[39m(cur_graph,step_num):\n\u001b[0;32m      2\u001b[0m     req_stmt_num \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcur_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m step_num \u001b[38;5;241m==\u001b[39m x:\n\u001b[0;32m      5\u001b[0m             req_stmt_num\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mint\u001b[39m(cur_graph\u001b[38;5;241m.\u001b[39medge_index[\u001b[38;5;241m0\u001b[39m][idx]))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\erdos_may_2024\\lib\\site-packages\\torch\\_tensor.py:1053\u001b[0m, in \u001b[0;36mTensor.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state():\n\u001b[0;32m   1045\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1046\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterating over a tensor might cause the trace to be incorrect. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1047\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing a tensor of different shape won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt change the number of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1051\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   1052\u001b[0m     )\n\u001b[1;32m-> 1053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munbind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Creates 4 sentences for each graph\n",
    "for i in range(1,2139):\n",
    "    for j in range(4):\n",
    "        graph=pf_data.get(graph_num)\n",
    "        sentences.append(sentence(graph))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constructs the dictionary of words.\n",
    "dict={857}\n",
    "for sent in sentences:\n",
    "    for word in sent:\n",
    "        dict.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary=list(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turns a word into a vector\n",
    "def tovec(word):\n",
    "    vec=np.zeros(7)\n",
    "    for i in range(7):\n",
    "        if word==dictionary[i]:\n",
    "            vec[i]=1\n",
    "    return vec\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converts our sentences to vectors\n",
    "vecsents=[]\n",
    "for sent in sentences:\n",
    "    for i in range(3):\n",
    "        vecsent=[]\n",
    "        start=np.random.randint(1,len(sent))\n",
    "        truncsent=sent[0:start]\n",
    "        if len(truncsent)<10:\n",
    "            for i in range(10-len(truncsent)):\n",
    "                vecsent.append(np.zeros(len(dictionary)))\n",
    "        for word in truncsent:\n",
    "            vecsent.append(tovec(word))\n",
    "            if len(vecsent)==10:\n",
    "                break\n",
    "    vecsents.append(np.array(vecsent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 7)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecsents[5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8552"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vecsents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatens = torch.tensor(vecsents, dtype=torch.float32).reshape(8552, 10, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8552, 9, 7])\n",
      "torch.Size([8552, 7])\n"
     ]
    }
   ],
   "source": [
    "#Constructs the skip-grams\n",
    "dataX=datatens[:,0:9,:]\n",
    "print(dataX.shape)\n",
    "datay=datatens[:,9,:]\n",
    "print(datay.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines the model\n",
    "class CharModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=7, hidden_size=100, num_layers=1, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.linear = nn.Linear(100, 7)\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        # take only the last output\n",
    "        x = x[:, -1, :]\n",
    "        # produce output\n",
    "        x = self.linear(self.dropout(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Cross-entropy: 12243.2637\n",
      "Epoch 1: Cross-entropy: 5845.8550\n",
      "Epoch 2: Cross-entropy: 3552.3167\n",
      "Epoch 3: Cross-entropy: 2684.5513\n",
      "Epoch 4: Cross-entropy: 2106.2007\n",
      "Epoch 5: Cross-entropy: 1620.4266\n",
      "Epoch 6: Cross-entropy: 1457.0802\n",
      "Epoch 7: Cross-entropy: 1376.5580\n",
      "Epoch 8: Cross-entropy: 1339.0958\n",
      "Epoch 9: Cross-entropy: 1259.7552\n",
      "Epoch 10: Cross-entropy: 1217.3639\n",
      "Epoch 11: Cross-entropy: 1193.7926\n",
      "Epoch 12: Cross-entropy: 1197.3599\n",
      "Epoch 13: Cross-entropy: 1213.9332\n",
      "Epoch 14: Cross-entropy: 1165.1077\n",
      "Epoch 15: Cross-entropy: 1150.5404\n",
      "Epoch 16: Cross-entropy: 1149.1710\n",
      "Epoch 17: Cross-entropy: 1147.0646\n",
      "Epoch 18: Cross-entropy: 1143.2030\n",
      "Epoch 19: Cross-entropy: 1137.3346\n"
     ]
    }
   ],
   "source": [
    "#Trains the model\n",
    "n_epochs = 20\n",
    "batch_size = 120\n",
    "model = CharModel()\n",
    " \n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "loader = data.DataLoader(data.TensorDataset(dataX, datay), shuffle=True, batch_size=batch_size)\n",
    " \n",
    "best_model = None\n",
    "best_loss = np.inf\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in loader:\n",
    "        y_pred = model(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            y_pred = model(X_batch)\n",
    "            loss += loss_fn(y_pred, y_batch)\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_model = model.state_dict()\n",
    "        print(\"Epoch %d: Cross-entropy: %.4f\" % (epoch, loss))\n",
    " \n",
    "#torch.save([best_model, char_to_int], \"single-char.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "#An example, which will probably work differently because of the randomization.\n",
    "example=np.array([tovec(word) for word in sentences[24]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "extensor=torch.tensor(example[3:12], dtype=torch.float32).reshape(1, 9, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 1., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 1., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 1., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[705, 289, 1284, 114, 857, 1181, -1]"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model(extensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.1864, -4.3440, -2.6406, -1.1165, -3.6526, -3.1844,  6.5438]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The prediction, with my seed gives a high chance to 705 and -1. 705 (the slightly lower chance) turns out to be correct.\n",
    "y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
